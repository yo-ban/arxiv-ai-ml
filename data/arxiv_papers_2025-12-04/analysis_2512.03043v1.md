# OneThinker: All-in-one Reasoning Model for Image and Video

## 基本情報
- arXiv ID は 2512.03043v1 (https://arxiv.org/abs/2512.03043)
- 著者 Kaituo Feng, Manyuan Zhang, Hongyu Li ほか14名
- 所属 MMLab CUHK, Meituan LongCat Team
- 投稿日 2025年12月4日
- カテゴリ cs.AI, cs.CV

## 簡単に説明すると
既存のマルチモーダル大規模言語モデル（MLLM）における推論能力を統合した革新的な研究です。
従来の手法では画像理解と動画理解が別々のモデルで扱われ、さらに各タスク（質問応答、物体検出、セグメンテーションなど）も独立して開発されていました。
OneThinkerは、画像と動画の両方を対象とした統合推論モデルです。
質問応答、キャプション生成、空間・時間的グラウンディング、トラッキング、セグメンテーションなど
10種類の基本的な視覚理解タスクを単一モデルで処理できる「オールインワン」システムを提案しています。
強化学習のGroup Relative Policy Optimization（GRPO）を改良したEMA-GRPOアルゴリズムを導入。
異なる特性を持つ複数タスクの報酬を適切にバランスさせ、31のベンチマークで優れた性能を実証しています。
関連リンク：GitHub (https://github.com/tulerfeng/OneThinker), HuggingFace (https://huggingface.co/OneThink)

## 1. 研究概要
### 1.1 背景と動機
従来のマルチモーダル大規模言語モデル（MLLM）は、画像理解と動画理解を別々のモデルで処理。
さらに個別のタスクごとに特化した訓練をしていました。
この分離された設計は、実用性の制限と、タスク間および異なるモダリティ間での知識共有の機会を逸することにつながっていました。
強化学習（RL）を用いたDeepSeek-R1の成功に触発され、Vision-R1やVideo-R1といった専門的な推論モデルが登場しましたが、これらも単一タスクや単一モダリティに制限されていました。
現実世界では静的画像と動的映像を含む多様な視覚的推論タスクへの統合的対応が求められており、この統合的アプローチの実現が重要な課題となっていました。

### 1.2 主要な貢献
本研究では、マルチモーダル推論の統合化において3つの重要な貢献を提示しています。
第1の貢献として、画像と動画の両方を対象とした多様なタスクを単一モデルで処理可能な
OneThinkerアーキテクチャを提案しました。
第2に、600kサンプルからなるOneThinker-600kデータセットを構築。
Chain-of-Thought（CoT）アノテーション付きのOneThinker-SFT-340kサブセットにより
大規模な多タスク学習環境を整備しました。
第3の貢献として、異なる特性を持つ複数の視覚タスクにおける報酬の不均衡問題を解決する
EMA-GRPOアルゴリズムを開発し、タスク間での適応的な正規化を実現しました。

- 画像・動画統合型の統一的マルチモーダル推論モデルの開発
- 多様な視覚タスクを包含する大規模データセット（OneThinker-600k）の構築
- タスク間の報酬不均衡を解決するEMA-GRPOアルゴリズムの提案
- 31のベンチマークにおける一貫した性能向上の実証

## 2. 提案手法
### 2.1 手法の概要
OneThinkerは、Qwen-3-VL-Instruct-8Bをベースモデルとして、多様な視覚理解タスクを統一的なテキストインターフェースで処理します。
モデルは`<think>...</think>`タグ内で内部推論をし、`<answer>...</answer>`タグ内でタスク固有の結果を出力する2段階構造を採用しています。
認識指向のタスクでは、時間範囲や境界ボックス、スパースポイントなどの構造化表現を事前定義されたスキーマに従って出力し、自動解析と検証を可能にしています。
訓練プロセスは、CoT アノテーション付きデータセットでのSFT（Supervised Fine-Tuning）冷開始と、その後の強化学習による最適化の2段階で構成されています。

### 2.2 技術的詳細
各タスクの報酬は、精度報酬R_accと形式報酬R_formatの合計として定義されています（R = R_acc + R_format）。
ルールベースQAでは多肢選択、数値計算、数学、OCRタスクを含み、回帰タスクには平均相対精度（MRA）メトリックを採用しています。
空間的グラウンディングでは空間的IoU、時間的グラウンディングでは時間的IoUを用い、
空間時間的グラウンディングではこれらを組み合わせた評価をします。
セグメンテーションタスクでは、境界ボックス予測と正負ポイント予測を組み合わせ。
ガウシアンカーネル（σ=50 for spatial, σ=1 for temporal）を用いた距離正規化により
精密な評価を実現しています。

### 2.3 新規性
既存手法との最も重要な違いは、画像と動画の両方における多様な基本的視覚タスクを単一モデルで処理する統合的アプローチです。
EMA-GRPOアルゴリズムは、標準GRPOの2つの重要な問題を解決しています。
タスク内不均衡（低分散ロールアウト過重視）とタスク間不均衡（スパース報酬タスクによる密な報酬タスク圧倒）です。
タスクごとの指数移動平均を用いた適応的正規化により、各タスクが独立した報酬スケールを維持しながら、全体的にバランスの取れた最適化を実現しています。
この設計により、従来の単一タスク・単一モダリティの制限を打破し、実用的な汎用視覚推論システムへの道筋を開いています。

## 3. 実験結果
### 3.1 実験設定
実験は32台のNVIDIA H800 GPUを用いて実施。
SFTステージではOneThinker-SFT-340kデータセットで学習し、
その後OneThinker-600kコーパスで強化学習しました。
バッチサイズはSFTで32、RLで128に設定し、学習率はSFTで1×10^-5、RLで2×10^-6、最適化にはAdamWを使用しました。
動画フレーム数は効率化のため128フレームに制限し、EMA-GRPOのグループサイズは8、減衰因子βは0.99、KL正規化係数は0.01に設定されました。
評価は greedy decoding を用いて行い、画像QA、動画QA、キャプション、空間・時間的グラウンディング、トラッキング、セグメンテーションの各カテゴリで31のベンチマークを対象としました。

### 3.2 主要な結果
画像QAにおいて、OneThinker-8BはMMMUで70.6%、MathVerseで64.3%を達成。
既存の専門モデルと比較して明確な性能向上を示しました。
動画QAではVideoMMMUで66.2%、LongVideo-Reasonで79.2%、VideoMathQAで35.0%を記録し、従来のビデオ専門モデルより一貫して高い性能を示しました。
認識指向タスクでは、GOT-10kトラッキングで84.4 R@0.5、ReasonVOSセグメンテーションで54.9 J&Fという強力な結果を達成しています。
特筆すべきは、Qwen3-VL-Instruct-8Bベースラインと比較して全てのタスクで明確な改善を示し、統合学習の有効性を実証していることです。

### 3.3 既存手法との比較
既存の専門化されたR1シリーズモデルと比較して、
OneThinkerは同等以上の性能を単一モデルで実現しています。
商用モデルとの比較では、GPT-4oを多くのベンチマークで上回り、特にMathVerseでは64.3% vs 41.2%と顕著な差を示しました。
Gemini 2.5 ProやSeed1.5-VLといった最先端モデルとも競争力のある結果を達成し、オープンソースモデルとしての価値を証明しています。
タスク間の知識転移効果も観察され、特定のタスクペア間での相互改善と、未見シナリオでの予備的ゼロショット汎化能力を示しています。

## 4. 実用性評価
### 4.1 実装の容易性
OneThinkerの実装は比較的容易で、既存のQwen-3-VL-Instruct-8Bアーキテクチャを基盤として構築されています。
統一されたテキストインターフェースにより、異なるタスクを一貫した方法で処理でき、新しいタスクの追加も既存のスキーマに従うことで簡単に実現できます。
GitHub (https://github.com/tulerfeng/OneThinker) とHugging Face (https://huggingface.co/OneThink) で
コードとモデルを公開。研究コミュニティでの利用と拡張を容易にしています。
ただし、完全な訓練には32台のH800 GPUで約10日間を要するため、大規模な計算資源が必要となります。

### 4.2 計算効率
推論時の計算効率は、単一モデルで複数タスクを処理できるため、個別の専門モデルを使用する場合と比較してメモリ使用量とデプロイメント複雑性が削減されます。
動画処理では最大128フレームに制限することで、計算コストを管理しながら実用的な性能を維持しています。
訓練時のEMA-GRPOアルゴリズムは標準GRPOと同程度の計算コストで、より効果的なマルチタスク最適化を実現しています。
ただし、多様なタスクを同時に処理するため、単一タスク専門モデルと比較すると
推論時のレイテンシ増加の可能性があります。

### 4.3 応用可能性
OneThinkerの統合的アプローチは、自動運転、ロボティクス、医療画像解析、コンテンツ制作など、複数の視覚理解タスクが必要な実世界アプリケーションに直接適用できます。
マルチモーダル対話システム、視覚QAシステム、動画コンテンツ分析プラットフォームなどの開発において、
単一のモデルで多様な機能を提供できる利点があります。
教育分野では、視覚的教材の自動分析や学習支援システムでの活用が期待され、
特に数学や科学分野での視覚推論支援に有効です。
エンターテイメント産業では、動画編集支援、コンテンツ自動タグ付け、インタラクティブメディア制作などの応用が考えられます。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、マルチモーダルAI分野において重要な方向転換を示しています。
従来の「1つのモデル、1つのタスク」アプローチから脱却し、
現実世界により近い統合的な視覚理解システムの実現可能性を実証しました。
EMA-GRPOアルゴリズムの提案は、異なる特性を持つ複数タスクを同時に最適化する際の根本的な問題を解決する技術的貢献として評価できます。
31のベンチマークで一貫した性能向上を達成し、理論と実践の両面でマルチタスク学習の有効性を示した点で、学術的価値が高いと考えられます。

### 5.2 今後の展望
OneThinkerは汎用視覚推論システムへの重要な第一歩ですが、さらなる改善の余地があります。
より多様なタスクの統合、3D理解の追加、言語生成との深い統合などが将来的な発展方向として期待されます。
計算効率の最適化、より軽量なモデルバリアントの開発、
エッジデバイスでの実行可能性の向上も重要な課題です。
実世界での長期運用における安定性、ドメイン適応能力の向上、継続学習メカニズムの統合なども、実用化に向けた重要な研究方向と考えられます。
