# OpenCUA: Open Foundations for Computer-Use Agents

## 基本情報
- **arXiv ID**: 2508.09123v1 (https://arxiv.org/abs/2508.09123)
- **著者**: Xinyuan Wang、Bowen Wang、Dunjie Lu、Junlin Yang、Tianbao Xie、Junli Wang等 39名 
- **所属**: XLANG Lab University of Hong Kong、Moonshot AI、Stanford University、University of Waterloo、Carnegie Mellon University
- **投稿日**: 2025年08月12日
- **カテゴリ**: cs.AI, cs.LG

## 簡単に説明すると
この論文は、コンピュータを自動的に操作できるAIエージェント（Computer-Use Agent、CUA）を構築するための包括的なオープンソースフレームワーク「OpenCUA」を提案しています。

人間がコンピュータを使う際のマウスクリックやキーボード入力などの操作を記録し、それをAIモデルが学習できる形式に変換することで、AIがWindows、macOS、Ubuntuなど様々なOSで200以上のアプリケーションやウェブサイトを自動操作できるようになります。

特筆すべきは、OpenCUA-32Bモデルが業界標準ベンチマークOSWorld-Verifiedで34.8%の成功率を達成し、オープンソースモデルとして最高性能を記録し、OpenAIのCUA（GPT-4oベース）をも上回ったことです。

プロジェクトのホームページ（https://opencua.xlang.ai）では、アノテーションツール、データセット、コード、モデルが公開されている。

## 1. 研究概要
### 1.1 背景と動機
ビジョン言語モデル（VLM）を活用したコンピュータ使用エージェント（CUA）は、日常的なコンピュータタスクを自動化する大きな可能性を秘めています。しかし、最先端のCUAシステム（Anthropic Claude Computer Use、OpenAI Operator等）の訓練データ、アーキテクチャ、開発プロセスなどの重要な詳細は非公開であり、プロプライエタリな状態にあります。

これらのエージェントが私たちのデジタルインタラクションを仲介し、重要な意思決定を代行する機会が増えるにつれ、研究コミュニティはCUAの能力、限界、リスクを研究するためのオープンなフレームワークへのアクセスを必要としています。透明性の欠如は技術的進歩を制限し、安全性に関する懸念を引き起こしています。

現在のオープンソースCUAの試みは重大な課題に直面しています。第一に、多様で大規模なコンピュータ使用データを収集するためのスケーラブルなインフラストラクチャが存在しません。第二に、既存のGUIデータセットは特定のドメイン（グラウンディング、モバイル、ウェブ）に焦点を当てているか、一般的なコンピュータ使用アプリケーションに必要な多様性が不足しています。第三に、多くのCUA研究は、モデリング戦略や訓練レシピについて不十分な詳細しか提供しておらず、データへのアクセスがあっても再現が困難です。

### 1.2 主要な貢献
OpenCUAフレームワークは、CUAデータと基盤モデルをスケールアップするための完全にオープンソースのソリューションを提供します。主要な貢献は以下の通りです：

- **AgentNet Tool**: ユーザーフレンドリーでクロスOSに対応したタスクアノテーションアプリケーション。個人のコンピュータにインストールして、自然な人間のデモンストレーションと対応するコンピュータの状態をシームレスに記録できる。
- **AgentNetデータセット**: Windows、macOS、Ubuntu全体で100以上のアプリケーションと200以上のウェブサイトにまたがる22,625のオープンドメインコンピュータタスクトラジェクトリを含む、初の大規模デスクトップエージェントタスクデータセット。
- **反射的長CoT（Reflective Long CoT）合成パイプライン**: 生のデモンストレーションを、計画、記憶、反省を明示的に含む自然言語の「内部独白」で拡張された状態-アクションペアに変換する新しい手法。
- **スケーラブルな訓練レシピ**: マルチ画像履歴エンコーディング、CoTレベルの混合、一般テキストデータとの組み合わせなど、エージェント性能を向上させる重要なモデリング詳細を含む。
- **高性能の実現**: OpenCUA-32Bモデルは、OSWorld-Verifiedで34.8%の成功率を達成し、オープンソースモデルの中で新たな最高記録を樹立し、OpenAI CUA（GPT-4oベース）を上回った。

## 2. 提案手法
### 2.1 手法の概要
OpenCUAのアプローチは、人間のコンピュータ操作デモンストレーションを、ビジョン言語モデルが学習可能な形式に変換することに焦点を当てています。システムは主に以下のコンポーネントで構成されています。

まず、アクションの離散化パイプラインが、高頻度のマウス移動やキーボード入力を含む生のデモンストレーションを、訓練可能な状態-アクションペアに変換します。マウス移動イベントはクリックやドラッグの前提条件として扱われ、連続するキー入力はテキスト入力文字列にマージされます。

次に、状態-アクションマッチングが、各アクションに対応する代表的なスクリーンショットを抽出します。マウスクリックの場合、将来の情報の漏洩を防ぐために、マウスの事前移動フェーズの開始時点まで遡り、最後の視覚的に異なるフレームを探します。

### 2.2 技術的詳細
反射的長CoT（Reflective Long CoT）の合成が、OpenCUAの核心技術です。このフレームワークは、AguvisとActReのパイプラインを拡張し、「Thought」により包括的なエージェントコンポーネントを装備しています。

CoT合成パイプラインは3つのコンポーネントで構成されています。

1. **Reflector**：各ステップの正確性と冗長性を検査し、アクション前後のスクリーンショットを比較します。ステップが不正確または冗長な場合、Reflectorは理由を詳細に説明し、そのステップは訓練中に無視されます。

2. **Generator**：完全なエージェントコンテキスト（以前の反省、アクション履歴、タスク目標、スクリーンショット、アクションコード）を条件として、構造化されたCoTを生成します。

3. **Summarizer**：曖昧なユーザー記述の目標をより正確で整合されたタスク目的に洗練し、各トラジェクトリを整合性、効率性、難易度でスコアリングします。

また、モデルが座標関連のアクションをより正確にグラウンディングできるように、視覚的手がかり（マウスアクション座標の赤いマーカーとズームインされた画像パッチ）を組み込んでいます。

### 2.3 新規性
OpenCUAの新規性は以下の点にあります。

第一に、反射的長CoT（Reflective Long CoT）の導入です。従来の手法が単純なアクション予測に焦点を当てていたのに対し、OpenCUAはL3（文脈的観察）、L2（反射的推論）、L1（実行可能アクション）の3層構造を採用し、エラー認識と修正能力を大幅に向上させています。

第二に、人間のデモンストレーションのエラーを学習リソースとして活用するアプローチです。従来の手法が「完璧な」トラジェクトリを要求していたのに対し、OpenCUAは人間のエラーを識別し、それを通じてモデルのエラー検出・回復能力を向上させます。

第三に、スケーラブルなクロスOSデータ収集インフラストラクチャの構築です。AgentNet Toolは、Windows、macOS、Ubuntuの3つの主要OSをサポートし、ユーザーの自然なコンピュータ使用を妨げることなくデータを収集できます。

## 3. 実験結果
### 3.1 実験設定
実験は複数のオープンソースビジョン言語モデルを基盤として実施されました。Kimi-VL-A3B、Qwen2-VL-7B-Instruct、Qwen2.5-VL-7B-Instruct、Qwen2.5-VL-32B-Instructを使用し、それぞれOpenCUA-A3B、OpenCUA-Qwen2-7B、OpenCUA-7B、OpenCUA-32Bとしてファインチューニングしました。

評価は以下の3つのカテゴリで実施されました。

1. **オンラインエージェント評価**：OSWorld-Verified（369の人間が作成したタスクを含む業界標準ベンチマーク）とWindowsAgentArena（154のWindows中心のタスク）で評価。

2. **オフラインエージェント評価**：AgentNetBench（WindowsとmacOSでの100の代表的タスク）で評価。

3. **GUIグラウンディング評価**：OSWorld-G、Screenspot-V2、Screenspot-Proの3つのベンチマークで、自然言語指示をGUI内の特定アクションにマッピングする能力を評価。

評価設定では、システム解像度を1920x1080に設定し、L2 CoTフォーマット（Thought + Action）を使用。温度パラメータを0に設定して決定論的デコーディングを実施しました。

### 3.2 主要な結果
OSWorld-Verifiedにおける結果は、OpenCUAの強力な性能を示しています。

**主要な成果：**
- OpenCUA-32Bは100ステップで**34.8%**の成功率を達成し、オープンソースモデルの中で新たな最高記録を樹立。
- OpenAI CUA（GPT-4oベース、31.4%）を上回り、Claude 4 Sonnet（43.9%）やClaude 3.7 Sonnet（35.9%）との差を縮めています。
- モデルサイズに応じた性能スケーリングが確認され、OpenCUA-32BはOpenCUA-7Bをすべてのステップ予算で一貫して上回っています。

**ステップ数の影響：**
- 15から50ステップへの増加で性能が大幅に向上（OpenCUA-32Bでは29.7%から34.1%への+4.4%）。
- 50から100ステップへの増加では改善が限定的（+0.7%）。これは多くのタスクが50ステップ未満で完了可能であること、および現在のモデルがエラー認識と回復にまだ課題があることを示しています。

**Pass@nスコア：**
OpenCUA-32BのPass@3成功率はPass@1の34.2%から**45.6%**に大幅に向上。この大きなマージンは、将来のポストトレーニング、リランキング、またはマルチエージェント手法の大きな余地を示唆しています。

### 3.3 既存手法との比較
既存手法との比較では、OpenCUAの優位性が明確に示されています。

**オープンソースモデルとの比較：**
- Aguvis-7B（以前の最高）：12.5%→ OpenCUA-7B：27.3%（118%向上）
- Qwen2.5-VL-32B（ベースモデル）：8.1%→ OpenCUA-32B：34.8%（330%向上）
- SeeClick-7Bや7.7%、ShowUI-7Bや6.5%と比較しても大幅な性能向上を実現。

**ドメイン汎化能力：**
クロスプラットフォーム訓練の効果が実証されました。Ubuntuデータで訓練されたモデルはOSWorldで優れた性能を示し、Windows/macOSデータで訓練されたモデルはWindowsAgentArenaとAgentNetBenchで優れた性能を示しました。

しかし、アプリケーションレベルの知識はオペレーティングシステム間で部分的に転移可能であることが示され、インターフェーススタイルが異なっても一定の汎化が可能であることがわかりました。

## 4. 実用性評価
### 4.1 実装の容易性
OpenCUAの実装は非常に容易です。フレームワークの完全なオープンソース化により、以下の利点があります。

- **AgentNet Tool**は個人のPCに簡単にインストール可能で、特別な技術知識がなくてもデータ収集が可能。
- 公開されたモデルはHugging Faceの標準的なインターフェースで利用可能。
- 詳細なドキュメントとコード例が提供され、カスタマイズも簡単。
- 既存のビジョン言語モデルに対してSFTを実施するだけでCUA能力を付与可能。

### 4.2 計算効率
計算効率の観点から、OpenCUAは優れたバランスを実現しています。

- **アクション離散化**により、生のデモンストレーションの数千の低レベルアクションを意味のある少数のアクションに圧縮。
- **視覚履歴の最適化**：3枚のスクリーンショットが性能と効率の最良のバランスを提供（5枚に増やしても性能向上は限定的で、3Kトークンの追加コストが発生）。
- **CoTミキシング戦略**により、訓練時に異なるレベルのCoTを混合して使用することで、推論時にL2 CoTのみを使用しても高い性能を維持。
- モデルサイズに応じた柔軟な訓練戦略（Stage-2のみ、Stage-1+2、ジョイント訓練）により、リソースに応じた最適化が可能。

### 4.3 応用可能性
OpenCUAの応用可能性は非常に幅広いです。

**直接的な応用：**
- オフィス業務の自動化（ドキュメント作成、データ入力、メール管理等）
- ソフトウェアテストの自動化
- カスタマーサポートの自動化
- アクセシビリティ支援ツール

**研究・開発への応用：**
- CUAのエラー分析と改善手法の研究
- マルチモーダル推論の研究
- 人間とAIのインタラクション研究
- セキュリティと安全性の研究

**将来的な可能性：**
- 複数のCUAを協調させるマルチエージェントシステム
- リアルタイム学習と適応能力の向上
- ユーザーの好みやスタイルを学習するパーソナライズCUA

## 5. まとめと所感
### 5.1 論文の意義
OpenCUAは、コンピュータ使用エージェント研究における重要なマイルストーンです。本研究の意義は以下の点に集約されます。

第一に、完全なオープンソース化による透明性の提供です。これまでプロプライエタリなブラックボックスだったCUAシステムの内部メカニズムが明らかになり、研究コミュニティがその能力、限界、リスクを系統的に研究できるようになりました。

第二に、スケールの実証です。データ量とモデルサイズの両方において性能がスケールすることを示し、さらに大規模なCUAシステムの開発への道筋を示しました。

第三に、反射的長CoTの導入によるエラー回復能力の向上です。人間のエラーを学習リソースとして活用するアプローチは、よりロバストなCUAの開発に寄与します。

ただし、Pass@nの結果から明らかなように、モデルのロバスト性にはまだ改善の余地があります。環境の小さな変化がタスクの成功率に大きく影響するという問題は、実用化に向けた重要な課題です。

### 5.2 今後の展望
OpenCUAの成果を踏まえ、今後の研究方向として以下が考えられます。

**技術的改善点：**
- リアルタイム学習能力の向上による環境変化への適応
- マルチエージェントシステムによる複雑なタスクの分担と協調
- 長期記憶メカニズムの導入によるコンテキスト理解の向上
- エラー検出と自己修正能力のさらなる強化

**応用面の拡大：**
- モバイルデバイスやIoT機器への対応
- マルチモーダル入力（音声、ジェスチャー等）の統合
- エンタープライズ環境でのセキュリティとプライバシーの強化
- ユーザーの個人化されたワークフローの学習と最適化

**社会的課題：**
- CUAの安全性と信頼性の保証方法の確立
- 人間とAIの役割分担に関するガイドラインの策定
- デジタルデバイドを考慮したアクセシビリティの向上
- CUAの意思決定プロセスの透明性と説明可能性の確保

OpenCUAは、CUA研究の基盤を提供することで、これらの課題に取り組む研究コミュニティを支援する重要な一歩です。
