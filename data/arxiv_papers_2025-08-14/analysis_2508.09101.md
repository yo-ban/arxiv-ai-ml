# AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators

## 基本情報
- arXiv ID: 2508.09101v1 (https://arxiv.org/abs/2508.09101)
- 著者: Jason Chou、Ao Liu、Yuchi Deng、Zhiying Zeng、Tao Zhang、Wiggin Zhou、Fengzong Lian 等 16名
- 所属: Hunyuan Team、Tencent
- 投稿日: 2025年08月11日
- カテゴリ: cs.AI, cs.LG

## 簡単に説明すると
この論文は、大規模言語モデル（LLM）を使って、高品質なコード生成ベンチマークを自動的に作成する新しい手法を提案している。

従来のコード生成ベンチマーク（HumanEvalやMBPPなど）は、人間が手作業でプログラミング問題を作成する必要があった。時間とコストがかかる上に、Pythonに偏っていたり、難易度が低いという問題があった。

本研究では、AutoCodeGenという自動ワークフローを開発した。このワークフローはLLMとサンドボックス環境を組み合わせたアプローチを採用している。LLMがテスト入力を生成し、サンドボックスで実行してテスト出力を取得することで、正確性と完全性を保証している。

この手法を使って作成されたAutoCodeBenchは、２０のプログラミング言語に均等に分布した3,920問の大規模ベンチマークです。高難度、実用性、多様性を兼ね備えています。

プロジェクトホームページ：https://autocodebench.github.io/

## 1. 研究概要
### 1.1 背景と動機
近年、大規模言語モデル（LLM）の急速な発展に伴い、コード生成能力はモデルの知能と実用性を示す重要な指標となっている。Claude 4やDeepSeek-V3などの強力なLLMは、すでにAI支援コーディングのシナリオで幅広く採用されている。

LLMのコード生成能力を測定・改善するために、数多くのベンチマークが提案されてきた。初期のHumanEvalやMBPPは、短いアルゴリズム中心のPython問題でLLMを評価する基礎を築いた。しかし、LLMの急速な発展に伴い、これらの単純で狭い範囲のタスクはすぐに時代遅れとなった。

最近のベンチマークは、専門家による手作業のアノテーションを利用して、より挑戦的な競技レベルのPython問題に焦点を移している。一方で、FullStackBenchなどの他のベンチマークは、より実用的な多言語プログラミングシナリオを強調している。

しかし、これらのプログラミング問題とテストケースを手作業で作成するのは時間と労力がかかる。高難度と多言語カバレッジを同時に達成することも困難です。そのため、コミュニティは現在、高難度、実用的多様性、バランスの取れた多言語分布を組み合わせたベンチマークを必要としています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです。

第一に、AutoCodeGenというLLM-Sandboxインタラクションに基づく自動ワークフローを提案しました。LLMがテスト入力を生成し、サンドボックスでテスト出力を取得することで、高品質なコード生成ベンチマークを作成しました。このワークフローは高品質な訓練データの合成にも応用可能です。

第二に、AutoCodeBenchという3,920問の大規模コード生成ベンチマークを作成した。これは２０のプログラミング言語に均等に分布し、高難度、実用性、多様性を特徴としている。簡易版のAutoCodeBench-Liteと、ベースモデル向けのAutoCodeBench-Completeも提供している。

第三に、多言語サンドボックスをオープンソース化した。これは20以上のプログラミング言語をサポートし、高並行性とリクエストベースの呼び出しが可能で、多言語評価と大規模訓練に適している。

第四に、実験結果として30以上のオープンソース・プロプライエタリモデルを評価しました。現在最も高性能なLLMでも複雑で多様な多言語プログラミングタスク、特にマルチロジカルシナリオで苦戦することが判明しました。

第五に、AutoCodeGenとAutoCodeBenchの包括的な分析を実施しました。生成データの品質、難敡度、多様性、および生成プロセスにおけるモデルバイアスの可能性に焦点を当てた分析を提供しています。

## 2. 提案手法
### 2.1 手法の概要
AutoCodeGenは、大規模言語モデルとサンドボックス環境を組み合わせた自動ワークフローです。このシステムは、手動アノテーションなしで高品質なコード生成ベンチマークを作成することを目的としています。

ワークフローは大きく4つのステージで構成されています。

1. コード解答生成：Stack-Eduから抽出した実際のコードスニペットを種として使用し、LLMが自己完結的で検証可能なコード解答に進化させる

2. テスト関数生成：LLMがテスト入力を生成し、サンドボックスでテスト出力を取得、入力と出力を組み合わせて完全なテスト関数を構築する

3. プログラミング問題生成：コード解答とテスト関数を基に、詳細な仕様を満たす高品質なプログラミング問題を生成する

4. データフィルタリング：複数段階のフィルタリングにより、高難度、高品質、多様性を確保する

### 2.2 技術的詳細
**LLM-Sandboxインタラクション**が本手法の核心です。従来の手法がLLMのみに依存していたのに対し、AutoCodeGenはLLMとサンドボックスの協調により正確性を保証します。

**テスト関数生成の3段階プロセス**
1. **テスト入力生成**：公開テスト入力（3個以下の基本ケース）と非公開テスト入力（7個以上、エッジケース含む）を生成。
2. **テスト出力生成**：コード解答とテスト入力を連結してサンドボックスで実行し、対応する出力を取得。
3. **入出力統合**：テスト入力と出力結果をLLMに提示し、一貫性のある検証可能なテスト関数を生成。

**プログラミング問題の仕様要件**
- 言語仕様：使用すべきプログラミング言語の明示
- 問題説明：曖昧さのない正確なタスク記述
- 関数/クラス名：テスト関数で使用される全ての識別子の明確化
- 入出力形式：型と値の範囲の明示的定義
- 使用例：公開テスト関数を埋め込んだサンプル
- 解答ヒントの排除：問題文に解法のヒントを含めない

**多言語サポート戦略**
Python、C++、Shell、Java、JavaScript、Goは直接ワークフローを適用。その他14言語は、リソース制限のため言語間翻訳アプローチを採用。翻訳ペアは慎重に選定され（例：Python→R、Java→Scala、JavaScript→TypeScript）、翻訳後も同じフィルタリングプロセスを適用。

### 2.3 新規性
AutoCodeGenの新規性は以下の点にあります。

**逆順問題生成アプローチ**：従来の手法が問題→解答→テストの順序で生成していたのに対し、AutoCodeGenは解答→テスト→問題の逆順で生成。これにより、テストケースの正確性と完全性が保証されます。

**LLM-Sandboxインタラクション**：単にLLMに依存するのではなく、実行環境との対話を通じて正確性を検証。これにより、従来手法で問題となっていたテスト出力の不正確さを解決。

品質管理のしくみとして、以下の3つのアプローチを採用しました。
- 難易度制御：DeepSeek-Coder-V2-Liteによる10回サンプリングで簡単すぎる問題を除外
- 品質制御：DeepSeek-R1による7つの観点からの批評
- 多様性サンプリング：カテゴリベースのサイクリックサンプリング

**スケーラブルな多言語対応**：20言語に均等に分布した大規模ベンチマークの実現。言語間翻訳アプローチにより、低リソース言語でも高品質なデータを確保。

## 3. 実験結果
### 3.1 実験設定
**評価ベンチマーク**
- AutoCodeBench（ACB）：3,920問の完全版
- AutoCodeBench-Lite（ACB-Lite）：1,586問の簡易版
- AutoCodeBench-Complete（ACB-Complete）：ベースモデル向け1,000問

**評価モデル**
30以上のオープンソース・プロプライエタリモデルを評価。主要モデルには以下が含まれます。
- Claude Opus 4（推論・非推論モード）
- GPT 4.1、Gemini 2.5 Pro
- DeepSeekシリーズ（V3、R1、Coder）
- Qwenシリーズ（2.5、3）
- その他オープンソースモデル

**評価指標**
- Pass@1：1回の生成で正解する確率
- Pass@n：n回の生成で少なくとも1回正解する確率
- Upper Bound：全モデルの結果を組み合わせた理論的上限

**マルチターン改善評価**
サンドボックスのエラーフィードバックを活用した反復的改善の効果を測定。最大3ターンまでの改善を評価。

### 3.2 主要な結果
**ACBの高難度性**
- 評価された全モデルの平均スコアが53未満
- 最高性能のClaude Opus 4（推論モード）でも52.5
- 現在のLLMにとって挑戦的なベンチマークであることを実証

**ACB-Liteによるモデル差別化**
- 多数のモデルが解ける問題を除外することで、モデル間の性能差を増幅
- より効果的なモデル比較が可能

**Claude Opus 4の優れた性能**
- ACBとACB-Liteの両方で一貫して首位
- 推論モード：ACBで52.5、ACB-Liteで64.5
- 非推論モード：ACBで51.6、ACB-Liteで63.3
- 多言語・複雑タスクでの優位性を実証

**推論モードの効果**
- 推論ベースモデルが非推論モデルを一貫して上回る
- 複雑な多論理問題の解決に推論と思考が有効

**Upper Boundの示唆**
- ACBでのUpper Boundは74.8%
- 最高性能モデルとの差は22.3ポイント
- モデル間の相補的な強みと改善余地を示唆

### 3.3 既存手法との比較
**人気言語vs低リソース言語**
- 人気言語（Python、C++、Java、C#）：モデル間の差は小さい（50.4〜53.8）
- 低リソース言語（Racket、Shell、Elixir、TS）：差が拡大（45.3〜62.0）
- Claude Opus 4は両カテゴリで優位性を維持

**多論理問題での性能低下**
- 1,622の多論理問題で全モデルが大幅な性能低下
- Claude Opus 4：相対的に小さい低下
- DeepSeek、Gemini、GPT：より大きな性能低下
- 実世界のコードエージェントシナリオでの改善必要性を示唆

**スケーリング法則の分析**
- パラメータスケーリング：Qwen3（Think）シリーズが最も急峻な改善曲線
- テスト時サンプリングスケーリング：モデルサイズに関わらず一貫した改善
- 推論能力はモデルサイズと共により積極的にスケール

**マルチターン改善の効果**
- DeepSeek-V3：48.1%→59.7%（+11.6ポイント）
- Qwen2.5-Coder-32B：35.8%→47.4%（+11.6ポイント）
- Qwen3-8B：23.3%→30.2%（+6.9ポイント）
- 最初のターンで最大の改善、その後は逓減

**ベースモデル評価（ACB-Complete）**
- 30B+モデル：DeepSeek-Coder-V2-Baseが39.0%で首位
- 8Bモデル：Seed-Coder-8B-Baseが31.6%で最高性能
- 既存ベンチマークとの一貫性を保ちつつ、より実用的な評価を提供

## 4. 実用性評価
### 4.1 実装の容易性
AutoCodeGenワークフローとAutoCodeBenchの実装は非常に容易です。

**オープンソースの利点**
- 完全なコードとドキュメントがGitHubで公開
- 20+言語対応のマルチ言語サンドボックスが利用可能
- リクエストベースのAPI設計により、既存システムへの統合が簡単

**カスタマイズ性**
- 新しいプログラミング言語の追加が容易
- 難易度やカテゴリに基づくフィルタリングのカスタマイズ可能
- 独自のベンチマーク作成への応用が可能

**評価の簡便性**
- 標準的な評価スクリプトを提供
- Pass@1、Pass@nの自動計算
- 詳細なエラー分析ツールを含む

### 4.2 計算効率
AutoCodeGenはバッチ処理と並列実行の設計により、大規模なベンチマーク生成を実現しています。

**データ生成の効率**
- LLM呼び出しの最適化：バッチ処理による効率化
- サンドボックス実行の並列化：高並行性をサポート
- フィルタリングの段階的実施：早期に低品質データを除外

**評価の効率**
- 軽量なACB-Liteによる高速評価オプション
- 言語別・カテゴリ別の部分評価が可能
- キャッシュメカニズムによる再評価の高速化

**リソース要件**
- 標準的なGPUサーバーで実行可能
- クラウドAPIを使用する場合、ローカルGPUは不要
- サンドボックスは軽量でスケーラブル

### 4.3 応用可能性
AutoCodeBenchとAutoCodeGenワークフローは幅広い応用が可能です。

**研究への応用**
- LLMのコード生成能力の系統的評価
- 多言語プログラミング能力の分析
- 新しいコード生成手法の開発とベンチマーキング
- エラー分析と改善戦略の研究

**産業への応用**
- AIコーディングアシスタントの性能評価
- 社内コード生成モデルの品質保証
- プログラミング教育ツールの評価
- コードレビュー自動化システムの検証

**教育への応用**
- プログラミング問題の自動生成
- 多言語プログラミング教材の作成
- 学習者の進捗評価システム
- 適応的学習システムの構築

**将来的な拡張**
- より複雑なソフトウェア開発タスクへの拡張
- ドメイン特化型ベンチマークの作成
- リアルタイムコード生成評価システム
- 人間とAIの協調コーディング評価

## 5. まとめと所感
### 5.1 論文の意義
AutoCodeBenchは、コード生成ベンチマークの自動化という重要な課題に取り組んだ画期的な研究です。本研究の意義は以下の点に集約されます。

**手動アノテーションからの脱却**
従来のベンチマーク作成における最大のボトルネックであった手動アノテーションを、LLM-Sandboxインタラクションによって解決。これにより、大規模かつ高品質なベンチマークの迅速な作成が可能になりました。

**真の多言語評価の実現**
20言語に均等に分布した3,920問という規模は、既存ベンチマークを大きく上回ります。特に低リソース言語における評価が可能になったことで、LLMの多言語能力をより包括的に評価できるようになりました。

**実用性と難易度の両立**
実際のGitHubコードを基にした問題生成により、アルゴリズム中心の既存ベンチマークとは異なり、実務で必要とされるプログラミング能力を評価できます。同時に、最先端モデルでも53%未満という難易度は、今後のLLM改善の余地を明確に示しています。

**多論理問題の導入**
複数の機能を同時に実装する必要がある多論理問題は、実世界のプログラミングタスクをより忠実に反映しています。これは、SWE-Benchのようなコードエージェントシナリオでの性能予測に特に有用です。

### 5.2 今後の展望
AutoCodeBenchの成果を踏まえ、以下の発展が期待されます。

**技術的改善の方向性**
- より複雑なプロジェクトレベルのタスクへの拡張
- 依存関係やライブラリ使用を含むより現実的な問題の生成
- 実行時間やメモリ使用量などの非機能要件の評価
- セキュリティやコード品質の観点を含む多面的評価

**ベンチマークの進化**
- 継続的な更新メカニズムの確立（LiveCodeBenchのように）
- ドメイン特化型ベンチマークの自動生成
- 難易度の動的調整機能
- ユーザーフィードバックの統合

**コミュニティへの影響**
- オープンソース化により、研究者が独自のベンチマークを作成可能
- 産業界での標準評価ツールとしての採用
- 教育分野での活用による次世代プログラマーの育成
- 多言語プログラミング研究の活性化

**潜在的な課題と対策**
- データ汚染の防止：定期的な更新と新規問題の追加
- 評価の公平性：言語間のバランスの継続的な調整
- 計算リソース：複数モデルの並列評価やキャッシュ機能を活用した評価手法の開発
- 実用性の検証：実際の開発タスクとの相関分析

AutoCodeBenchは、LLMのコード生成能力評価に新たな標準を確立し、より実用的で包括的な評価への道を開きました。今後、このフレームワークを基盤として、さらに高度なコード生成システムの開発が期待されます。
