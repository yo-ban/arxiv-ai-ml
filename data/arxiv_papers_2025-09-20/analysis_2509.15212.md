# RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation

## 基本情報
- arXiv ID: 2509.15212v1 (https://arxiv.org/abs/2509.15212)
- 著者: Yuming Jiang, Siteng Huang, Shengke Xue他（DAMO Academy, Alibaba Group）
- 所属: DAMO Academy, Alibaba Group / Hupan Lab
- 投稿日: 2025年09月24日
- カテゴリ: cs.RO, cs.AI

## 簡単に説明すると
Vision-Language-Action（VLA）モデルを大規模な動画生成の事前学習により改善する研究です。
人間のデモンストレーション動画から学習した操作スキルをロボット操作に転移させる新しいアプローチを提案しています。
3段階の事前学習手法（エゴセントリック動画生成→人間軌道予測→ロボット操作）により、
既存の最先端手法に対して20ポイント以上の性能向上を達成しています。
コードとモデルをGitHubで公開しています：https://github.com/alibaba-damo-academy/RynnVLA-001

## 1. 研究概要
### 1.1 背景と動機
Vision-Language-Action（VLA）モデルの進歩は、大規模ロボット操作データの不足により制約されています。
大規模言語モデル（LLM）や視覚言語モデル（VLM）が豊富なWeb データから恩恵を受けているのとは対照的に、
ロボット操作データの収集は物理ロボットでの人間テレオペレーションに依存しており、労働集約的で高コストです。

従来のアプローチは2つの方向性に分かれています。
1つは大規模ロボット操作データセットの構築ですが、データサイズはLLMやVLMで使用されるものと比べて依然として小さいままです。
もう一つは事前学習済み生成モデルやVLMの大規模事前知識を活用してデータ不足問題を軽減する手法です。

この研究では、人間のエゴセントリック操作動画から操作スキルを暗黙的に転移させるという新しい洞察に基づいています。
12M のエゴセントリック人間操作動画を使用して、視覚レベルでの操作予測から低レベルのロボット制御へと段階的に学習を進める手法を提案しています。

### 1.2 主要な貢献
この研究の主な技術的貢献は以下の3点です。

新しい3段階事前学習手法の提案です。
第1段階のエゴセントリック動画生成の事前学習では、単一画像と言語指示から後続フレームを予測するImage-to-Video（I2V）モデルを訓練します。
第2段階の人間中心軌道認識モデリングでは、視覚フレーム予測に加えて人間キーポイント軌道を共同予測することで、
視覚動態と行動予測の間のギャップを効果的に橋渡しします。

ActionVAEという新しい変分オートエンコーダーの提案です。
行動シーケンスをコンパクトな潜在埋め込みに圧縮することで、VLA出力空間の複雑性を削減し、
時間的一貫性のある滑らかな行動予測を可能にします。

包括的な実験による有効性実証です。
同一の下流ロボットデータセットでファインチューニングした結果、
RynnVLA-001は最先端ベースライン（GR00T N1.5、Pi0）に対して20ポイント以上の性能向上を達成し、
提案する事前学習戦略がVLAモデルにより効果的な初期化を提供することを実証しました。

## 2. 提案手法
### 2.1 手法の概要
RynnVLA-001は大規模動画生成の事前学習に基づくVLAモデルです。
3段階の段階的な学習プロセスを通じて、人間のデモンストレーションからロボット操作への知識転移を実現します。

第1段階では、エゴセントリック動画生成の事前学習により、
12Mのエゴセントリック人間操作動画でI2Vモデルを訓練し、未来フレーム予測能力を獲得します。

第2段階では、人間中心軌道認識動画モデリングにより、
EgoDexデータセットの人間キーポイントアノテーションを使用して、
視覚フレーム予測と軌道予測を共同で学習します。

第3段階では、ロボット中心Vision-Language-Actionモデリングにより、
前段階の重みを継承してロボットデータでファインチューニングし、
ActionVAEでエンコードされた行動埋め込みと未来視覚観測を予測します。

### 2.2 技術的詳細
**アーキテクチャ設計**

ベースアーキテクチャには自己回帰（AR）Transformerを採用しています。
AR ベースの動画生成フレームワークの限られた利用可能性のため、
強力なAR画像生成モデルであるChameleonを拡張してI2Vタスクを実行します。

入力シーケンス構造は各段階で進化します。
第1段階では `[言語トークン, 視覚トークン_t, 言語トークン, 視覚トークン_t+1, ...]` の形式で、
言語トークンと視覚トークンを交互配置してVLA推論との整合性を確保します。

第2段階では状態埋め込み（青ブロック）を追加し、
`[言語, 視覚トークン_t, 状態埋め込み_t, <ACTION_PLACEHOLDER>, ...]` の構造になります。
これにより現在の人間手首キーポイント位置を表現する固有受容情報を提供します。

**ActionVAE設計**

行動表現の向上のため、ActionVAEという変分オートエンコーダーを提案します。
単一ステップ行動ではなく行動チャンクを予測することで、
反復的予測を避け効率を向上させます。

ActionVAEはエンコーダー・デコーダーアーキテクチャで、
行動チャンクをコンパクトで連続的な埋め込みに圧縮します。
人間軌道（第2段階）とロボット行動（第3段階）でドメイン固有のActionVAEを使用します。

**データキュレーションパイプライン**

多段階フィルタリングプロセスにより高品質なエゴセントリック動画を収集します。
キーポイント検出により人間ポーズランドマークを抽出し、
顔キーポイントを含む動画を除去して三人称視点を排除します。
手キーポイントが見える動画のみを保持し、Qwen2-VL-7Bにより簡潔な操作記述を生成します。

### 2.3 新規性
既存のVLA関連研究と比較して、RynnVLA-001の主な新規性は以下の通りです。

段階的学習カリキュラムの導入です。
視覚生成→軌道予測→行動生成の段階的な学習により、
視覚観測と低レベル行動の間のギャップを効果的に橋渡しします。

人間軌道の中間表現としての活用です。
人間手首キーポイント軌道をロボット行動と視覚予測の間の中間表現として使用し、
知識転移を促進します。

ActionVAEによる効率的行動表現です。
変分オートエンコーダーを用いて行動チャンクをコンパクトに表現し、
時間的一貫性と予測効率を向上させます。

大規模エゴセントリック動画データの活用です。
12Mという大規模なエゴセントリック人間操作動画を活用し、
ロボット操作データの不足問題に対する新しいソリューションを提示します。

## 3. 実験結果
### 3.1 実験設定
実験はカスタムLeRobot SO100操作データセットで実施されました。
3つのタスクにわたる798のデモンストレーションを含みます。

使用されたタスクは以下の通りです。
緑ブロックのピックアンドプレース（248デモ）、
イチゴのピックアンドプレース（249デモ）、
ペンをホルダーに入れる（301デモ）。

評価シナリオは段階的に複雑になります。
単一ターゲット操作、マルチターゲット操作、
気を散らすものがある環境での指示フォローイングで評価しました。

ベースラインとしてGR00T N1.5とPi0を使用し、同一のファインチューニングデータで比較しました。

### 3.2 主要な結果
RynnVLA-001は全ての評価シナリオで優れた性能を示しました。

主要な結果は以下の通りです。
RynnVLA-001：平均成功率90.6%、SR@1 56.7%
Pi0：平均成功率70.4%、SR@1 56.3%
GR00T N1.5：平均成功率55.6%、SR@1 37.2%

重要な発見として、最強ベースライン（Pi0）に対して20ポイント以上の改善を達成しました。
全てのタスク複雑度（単一ターゲットから気を散らすものまで）で一貫した性能を示し、
Pi0と同等のSR@1を示すことで同様の単試行精度を持つことが確認されました。

### 3.3 既存手法との比較
包括的なアブレーション研究により各コンポーネントの貢献を分析しました。

事前学習の効果性が明確に示されました。
RynnVLA-001-Scratch（スクラッチ学習）：平均4.4%（破滅的失敗）
RynnVLA-001-Chameleon（T2I初期化のみ）：平均50.0%
RynnVLA-001-Video（第1段階後）：平均84.4%
RynnVLA-001-Full（第2段階後）：平均90.6%

この段階的な性能向上により、各事前学習段階の貢献が検証されました。

技術コンポーネントアブレーション（Calvinベンチマーク）では、
画像解像度384x384がVQGAN互換性により最適であることが示されました。
行動表現ではVAE埋め込みが生の行動予測を上回り、
行動ヘッドの複雑さでは単純な線形層が深い5層MLPを上回りました。

## 4. 実用性評価
### 4.1 実装の容易性
RynnVLA-001は高い実装の容易性を提供します。
明確な3段階訓練プロトコルとモジュラーアーキテクチャにより段階的最適化が可能です。
事前学習済みコンポーネントの転移可能性が実証されており、
GitHub上でコードとモデルの公開により再現性が確保されています。

課題としては大規模エゴセントリック動画のキュレーションが必要であり、
新しい実施形態に対してドメイン固有のActionVAE訓練が必要です。
また、フロント＋手首カメラ構成への依存があります。

### 4.2 計算効率
計算効率面では優れた特性を示します。
段階的カリキュラムによりエンドツーエンドアプローチより訓練時間を削減できます。
推論時には視覚生成を破棄することで大幅にリアルタイム性能を向上させます。
ActionVAE圧縮により行動空間の複雑性を削減しています。

12M動画事前学習により大規模アプローチの実現可能性を実証し、
ActionVAE設計により複数実施形態拡張をサポートします。

### 4.3 応用可能性
RynnVLA-001の応用可能性は非常に幅広いです。

産業応用では、製造業での組み立て作業自動化、物流でのピッキング・パッキング自動化、
サービス業での接客支援ロボットへの活用が期待されます。

研究開発では、Vision-Language-Actionモデルの基盤技術、
マルチモーダルAI研究への貢献、ロボティクス分野でのファウンデーションモデル開発が期待されます。

教育では、ロボティクス教育での実践的学習支援、
人間とロボットのインタラクション研究への活用が期待されます。

医療・介護では、リハビリテーション支援ロボット、
手術支援システム、高齢者介護での生活支援ロボットへの展開が期待されます。

## 5. まとめと所感
### 5.1 論文の意義
この研究はロボティクス分野における重要な課題であるデータ不足問題に対して、
革新的で実用的な解決策を提供する画期的な研究です。

特に、以下の点で重要な意義を持ちます。
新しい学習パラダイムの提示により、人間動画からロボット操作への段階的知識転移を実現しました。
大規模データ活用により、12Mという大規模エゴセントリック動画の効果的活用を実証しました。
技術的革新により、ActionVAEによる効率的行動表現と段階的事前学習カリキュラムを提案しました。
実証的有効性により、最先端手法に対する大幅な性能向上を達成しました。

### 5.2 今後の展望
この研究の将来性は非常に高く、以下の発展が期待されます。

技術的発展では、複数実施形態への拡張、
より多様な環境での汎化性能向上、
リアルタイム性能とレスポンシブ性の更なる改善が期待されます。

応用展開では、製造業・サービス業での実用システム展開、
医療・介護分野での専門的ロボット開発、
教育・研究分野での標準ツールとしての普及が期待されます。

研究の方向性では、マルチモーダル学習の更なる発展、
人間とAIのコラボレーションの新しい形態創造、
ロボティクス分野でのファウンデーションモデル確立が期待されます。

この研究は人間のデモンストレーションを活用したロボット学習の新しい可能性を切り開き、
実用的なロボティクスシステムの実現に向けた重要な一歩を踏み出しています。