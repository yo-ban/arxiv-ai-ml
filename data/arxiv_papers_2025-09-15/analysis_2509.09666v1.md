# Can Understanding and Generation Truly Benefit Together — or Just Coexist?

## 基本情報
- **arXiv ID**: 2509.09666v1 (https://arxiv.org/abs/2509.09666)
- **著者**: Zhiyuan Yan*, Kaiqing Lin*, Zongjian Li*, Junyan Ye*, Hui Han, Zhendong Wang, Hao Liu, Bin Lin, Hao Li, Xue Xu, Xinyan Xiao†, Jingdong Wang, Haifeng Wang, Li Yuan† (*Equal Contributors, †Corresponding Authors)
- **所属**: 北京大学, Baidu ERNIE, Rabbitpre AI, 中山大学, 中国科学技術大学
- **投稿日**: 2025年09月15日
- **カテゴリ**: cs.AI, cs.LG

## 簡単に説明すると
この論文は、画像理解と画像生成を真に統合するマルチモーダルAIシステムについて研究している。従来のマルチモーダルモデルでは理解と生成が別々に訓練されがちで、両者の相互利益が活用されていないという問題を指摘し、オートエンコーダの視点から統一的な学習フレームワーク「UAE（Unified Auto-Encoder）」を提案している。理解を「エンコーダー」、生成を「デコーダー」として捉え、画像→テキスト→画像の再構成品質を統一的な学習目標とすることで、両者が相互に強化し合う「aha moment」を実現している。GitHubリポジトリ（https://github.com/PKU-YuanGroup/UAE）も公開されている。

## 1. 研究概要
### 1.1 背景と動機
マルチモーダル統合モデル（UMMs）の分野では、画像理解と画像生成の統合が長年の課題となっている。
現在のアプローチでは、これら2つのタスクを別々の取り組みとして扱い、別個の目的関数で訓練することが多く、
タスク間の相互利益を見逃している状況にある。

多くの既存研究では、拡散ベースの生成目的を最適化することが理解能力や学習された表現を負に劣化させ、
逆もまた同様であることが頻繁に報告されている。この現象により、多くの研究者が理解モジュールと生成モジュールを
個別に訓練し、潜在的なクロスタスクの利益を諦めるという設計選択を行っている。

真の統合とは、単に2つのタスクを併存させることではなく、両者を本質的に結びつける統一的で基本的な目的関数を
必要とするという考え方が、この研究の出発点となっている。著者らは、真に統合されたシステムでは、
各タスクが他方を強化するような明示的で双方向の利益が得られるべきだと主張している。

### 1.2 主要な貢献
この論文は、マルチモーダル統合の分野において画期的な貢献を4つの側面から行っている。

- **オートエンコーダ統合原理の初提案**: 理解を「エンコーダー（I2T）」、生成を「デコーダー（T2I）」として捉え、
  再構成品質を クロスモーダル情報の一貫性の測定可能な信号として活用する、初のオートエンコーダベースの
  統合原理を提案した。これにより、理解と生成間の長年の分裂を解決し、UMMsに対する実行可能で検証可能な
  目的関数を提供している。

- **相互利益をもたらす初の強化学習手法**: 理解と生成の両方を共同で相互に改善する初の強化学習スキーム
  「Unified-GRPO」を開発した。「Generation for Understanding」では、エンコーダーが再構成品質を最大化する
  情報的なキャプションを生成するように訓練し、「Understanding for Generation」では、デコーダーが
  これらのキャプションから詳細を活用して再構成するように調整することで、真の統合に向けた正のフィードバック
  ループを形成している。

- **マルチモーダル学習における創発的「aha moment」の発見**: RL の進行に伴い、エンコーダーが自律的に
  より長く詳細なキャプションを生成し、同時にデコーダーが驚くほど忠実な再構成を達成するという創発的現象を
  報告している。この共進化は、統一的マルチモーダル知能への画期的な証拠を提供している。

- **統合度評価ベンチマークの初開発**: 生成能力や理解能力を個別に評価するのではなく、UMMsの統合度を
  明示的に測定するために設計された初のベンチマーク「Unified-Bench」をリリースした。

## 2. 提案手法
### 2.1 手法の概要
UAEフレームワークは、大規模視覚言語モデル（LVLM）をマルチモーダル理解に、拡散トランスフォーマーを画像合成に
活用するコンパクトな「エンコード→プロジェクト→デコード」設計に従っている。

具体的には、LVLMが入力（画像およびオプションのプロンプト）を豊かな意味表現に変換し、軽量なプロジェクターが
この表現をデコーダーの条件付け空間にマッピングし、最終的に拡散モデルがこの条件を画素に展開する。
この分離により、インターフェースを最小限に抑え、各コンポーネントの強みを保持し、システムをモジュラーで
拡張可能にしている。

エンコーダーとしてはQwen-2.5-VL 3Bを、デコーダーとしてはSD3.5-largeを採用している。エンコーダーは
オートリグレッシブ言語モデルと組み合わせた視覚エンコーダーで構成され、視覚言語入力を処理できる。
生成時には、LVLMがプロンプトとマルチモーダルコンテキストをオートリグレッシブに処理して高次元で
コンテキストリッチな表現を生成する。

### 2.2 技術的詳細
UAEの核心となるのは3段階の「Unified-GRPO」アルゴリズムである。

**Stage 1: Cold-start reconstruction**では、基本的なアライメントを確保するために、画像→LVLM→DiTループを
閉じる意味的再構成損失のみでLVLMとDiTを共同初期化する。各画像xに対して、条件c（c_text）を形成し、
x̃ ~ p_θ(·|c)を生成し、セマンティック再構成損失 L = 1 - cos⟨f_I(x), f_I(x̃_i)⟩を最小化する。

**Stage 2: Generation for Understanding**では、LVLM π_φがポリシーとして機能し、DiT p_θは凍結され、
CLIP エンコーダーと共に報酬評価環境の一部として機能する。各入力画像xに対して、古いポリシー
π_{φ_old}(·|x)からG個のキャプションシーケンス{y^(i)}を サンプリングし、各シーケンスから最後の隠れ状態
h^(i)_Tを抽出して条件c^(i) = g(h^(i)_T)を形成する。

**Stage 3: Understanding for Generation**では、役割が逆転し、画像生成モデルp_θがポリシーとして機能し、
LVLMは凍結され、生成のための条件c = {c_text, c_img}を提供する役割を担う。

報酬信号は、凍結されたCLIP画像エンコーダーf_I(·)を使用した再構成報酬として定義される：
R_i(x,x̃_i) = cos⟨f_I(x), f_I(x̃_i)⟩

### 2.3 新規性
本手法の主要な新規性は以下の点にある：

**統一的目的関数の導入**: 従来の手法では理解と生成が別々の損失関数で訓練されていたのに対し、
UAEは再構成の忠実度という単一の統一目的を採用している。これにより、両タスクが同一の最適化ターゲットに
向けて協調的に働くことが可能になった。

**双方向強化学習の実現**: 既存の研究では一方向的な最適化が主流だったが、本手法では「Generation for Understanding」と
「Understanding for Generation」の2段階により、エンコーダーとデコーダーが相互に強化し合う真の双方向
最適化を実現している。

**長文コンテキストの活用**: 700Kの画像キャプションペア（250語以上の詳細なキャプション、1024解像度）を
構築し、GPT-4oから50Kの高品質サンプルを蒸留することで、従来手法では扱えなかった長文コンテキストでの
統合訓練を可能にした。これにより、より詳細で包括的な画像理解と生成が実現されている。

## 3. 実験結果
### 3.1 実験設定
実験は三つの主要なベンチマークで実施された：**Unified-Bench**（統合度評価）、**GenEval/GenEval++**
（テキストから画像生成）、および**DPG-Bench**（詳細なプロンプト理解）である。

**Unified-Bench**は、100の多様なソース画像から開始し、モデルがまず詳細なキャプションを生成し、
次に同じモデルがそのキャプションから画像を合成するという新しいプロトコルを採用している。
統合スコアは、再構成画像とソース画像との間の類似度を、4つの広く採用された視覚バックボーン
（CLIP、LongCLIP、DINO-v2、DINO-v3）を用いて計算し、バックボーン別の類似度と全体要約を報告する。

**GenEval**は構成的理解と指示追従を探査するベンチマークで、**GenEval++**はより困難な設定として
3つ以上のオブジェクトを含むプロンプトでの包括的な複数制約充足を要求する。**DPG-Bench**は
長いプロンプトでのエンティティ基盤化と関係処理能力を評価する。

訓練データとして、Stage 1では700Kのテキスト-画像コーパス（250語以上の詳細キャプション、1024解像度）と
GPT-4o蒸留による50K高解像度サンプル（平均300語）を使用し、RL段階では1Kの高品質実写画像とEcho-4o
データサブセットを使用した。

### 3.2 主要な結果
**統合評価**において、UAEはUnified-Benchで最高の総合統合スコア（86.09）を達成し、GPT-4o-Image（85.95）を
上回った。具体的には、CLIP（90.50）、DINO-v2（81.98）、DINO-v3（77.54）で最高結果を獲得し、
LongCLIP（94.35 vs. 94.37）では統計的に同等の性能を示した。これらの対比的特徴量（CLIPファミリー）と
自己教師あり特徴量（DINOファミリー）での一貫した向上は、UAEフレームワークがレイアウトレベルと
テクスチャレベルの意味論を保持し、より忠実な再構成につながることを示している。

**テキストから画像生成**では、GenEvalにおいてLLMリライト無しでUAEが統一モデル中最高の総合スコア（0.86）を
達成した。特にCounting（0.84）とColor attribution（0.79）で最高性能を示し、Color attribution では
BAGEL（0.63）に対して16ポイント、OmniGen2（0.76）に対して3ポイントの向上を実現した。

GenEval++のより困難な構成制御において、UAEは最高の総合スコア（0.475）を獲得し、Color/Count（0.550）と
Pos/Count（0.450）でトップ、Color/Pos（0.550）とMulti-Count（0.400）で2位の性能を示した。

**理解モデルの改善**として、Unified-Benchの「キャプション→生成→比較」プロトコルの下で、UAEの訓練された
理解モデルが生成するキャプションは、4つすべてのバックボーンで最高の再構成類似度を生み出した：
CLIP（90.50）、LongCLIP（94.35）、DINO-v2（81.98）、DINO-v3（77.54）、総合（86.09）。

### 3.3 既存手法との比較
商用LLM審査員（Claude-4.1、GPT-4o、Grok-4、o4-mini）によるペアワイズ比較において、UAEの理解モデルは
高い平均勝率を達成した：Show-oに対して94.7%、OmniGen2に対して71.4%、BAGELに対して64.3%、
Qwen-2.5-VL（3B/7B）に対して76.3%/71.5%を記録し、GPT-4o（47.2%）とも競争力を保持している。

DPG-Benchにおいて、UAEはEntity（91.43）、Attribute（91.49）、Relation（92.07）で最高スコアを獲得し、
総合で84.74となり、BAGEL（85.07）に僅差で続く2位となった。このサブスコアパターンは、UAEの利点が
長いプロンプトでの忠実なエンティティ基盤化と関係処理から生じ、統合アーキテクチャ内での
競争力のあるエンドツーエンド生成品質につながることを示している。

従来の統合モデルと比較して、UAEは真の相互利益を実現している点で際立っている。多くの既存手法では
理解と生成が並行して訓練されるが実際には独立して動作するのに対し、UAEでは再構成ループを通じて
両者が有機的に結合し、相互強化が実証されている。

## 4. 実用性評価
### 4.1 実装の容易性
UAEフレームワークは比較的実装しやすい設計となっている。既存の成熟したコンポーネント（Qwen-2.5-VL と SD3.5-large）
を基盤とし、軽量なプロジェクターでそれらを接続するモジュラー設計により、各コンポーネントを独立に扱えるため
開発とデバッグが容易である。

LoRA（Low-Rank Adaptation）を用いた効率的な調整により、フルモデルの再訓練を避けながら新しいタスクに
適応できる。著者らは同じLoRAセッティングをFlow-GRPOと維持しており、既存の実装からの移行も比較的スムーズである。
3段階の訓練プロセスは明確に定義されており、各段階の目的と実装方法が詳細に説明されている。

ただし、強化学習（GRPO）の実装には相応の技術的専門知識が必要であり、また3段階の訓練プロセスにより
従来の単一段階訓練よりも複雑になっている点は考慮すべきである。

### 4.2 計算効率
計算効率に関しては、3段階訓練による計算コストの増大が主要な考慮事項である。Stage 1での事前訓練、
Stage 2での理解モデルの強化学習、Stage 3での生成モデルの強化学習により、従来の単一目的訓練よりも
計算リソースを多く要求する。

しかし、LoRA適応により、フルモデルの調整と比較して大幅な計算量削減が実現されている。
また、モジュラー設計により、必要に応じて各コンポーネントを個別に最適化または交換できるため、
長期的な計算効率の観点では柔軟性を提供している。

再構成ベースの評価は推論時には単一のフォワードパス以上の計算を必要とするが、訓練された統合モデルは
理解と生成の両方のタスクで従来手法と同等以上の効率を提供すると考えられる。

### 4.3 応用可能性
UAEの応用可能性は非常に広範囲にわたる。基本的な画像理解と生成に加えて、以下の領域での応用が期待される：

**画像編集**: 論文中で言及されているように、編集を画像間（I2I）再構成の自然な拡張として捉えることができる。
編集指示を含む画像入力を処理し、編集領域外での画素レベル保存を追加要件とする拡張が考えられる。

**長文テキスト処理**: 700K長文コンテキストデータセット（250語以上）の構築により、従来では扱えなかった
詳細な画像説明や複雑な指示への対応が可能になっている。これは教育、技術文書、創作支援などの分野で有用である。

**マルチモーダル対話システム**: 統合されたアーキテクチャにより、画像を見て理解し、それに基づいて新しい
画像を生成する対話的なシステムの構築が可能である。

**クロスドメイン転用**: オートエンコーダーの汎用的な枠組みにより、視覚以外のモダリティ
（音声、動画、3Dデータ等）への拡張可能性も示唆される。

## 5. まとめと所感
### 5.1 論文の意義
本論文は、マルチモーダルAI分野において画期的な理論的・実用的貢献を行っている。最も重要な意義は、
従来の「理解と生成の並存」から「真の統合による相互強化」への パラダイムシフトを提案し、
それを具体的なフレームワークとして実現した点にある。

オートエンコーダーの視点から統合問題を捉え直すアプローチは極めて洞察に富んでおり、理解（エンコーダー）と
生成（デコーダー）を対称的で補完的な変換として定式化することで、両者の本質的な関係性を明確化している。
再構成忠実度という単一の統一目的により、これまで困難だった相互利益の定量化と最適化を可能にした。

「aha moment」の発見—強化学習の進行に伴いエンコーダーがより詳細なキャプションを自律的に生成し、
同時にデコーダーがより忠実な再構成を達成する創発現象—は、真のマルチモーダル知能への道筋を示す
重要な実証的証拠である。これは、適切な目的関数と訓練手順により、AIシステムに創発的な協調行動が
生じうることを示している。

Unified-Benchの提案により、統合度の定量的評価という従来欠けていた評価軸を確立し、
今後の研究の方向性を明確にした点も高く評価できる。

### 5.2 今後の展望
本研究は多くの将来的発展の可能性を示している。技術的な観点では、コネクターの精緻化による
LVLM出力空間と拡散デコーダー条件付け空間のより良いアライメント、純粋なデコーダーへの進化による
理解と生成のさらなる分離、OCRベース報酬によるテキスト描画能力の向上などが期待される。

理論的には、オートエンコーダー統合原理の他のモダリティ（音声、動画、3D）への拡張、
より複雑なマルチモーダルタスクへの適用、認知科学的観点からの統合メカニズムの解明などが挙げられる。

応用面では、インタラクティブな創作ツール、教育支援システム、技術文書の自動生成と視覚化、
アクセシビリティ向上のための画像-テキスト変換システムなど、社会的影響の大きい応用が考えられる。

ただし、計算コストの削減、より効率的な訓練手順の開発、エラーハンドリングの改善、
バイアスの軽減といった課題も残されており、これらの解決が実用化の鍵となるだろう。

本研究は、単に新しい手法を提案するだけでなく、マルチモーダルAI研究全体の方向性を示唆する
重要な理論的基盤を築いたと評価できる。
