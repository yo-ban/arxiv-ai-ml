# REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once

## 基本情報
- **arXiv ID**: 2507.10541v1 (https://arxiv.org/abs/2507.10541)
- **著者**: Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, H. Vicky Zhao, Conghui He, Lijun Wu
- **所属**: Tsinghua University, OpenDataLab/Shanghai Artificial Intelligence Laboratory, Renmin University of China
- **投稿日**: 2025年07月15日
- **カテゴリ**: cs.AI, cs.LG

## 簡単に説明すると
RESTは、大規模推論モデル（LRM）の新しい評価手法で、複数の問題を同時に一つのプロンプトに含めて解かせることで、モデルの本当の能力を測定します。
従来の評価では1問ずつ順番に解かせていましたが、RESTでは複数問題を一度に提示することで、より現実的な状況でのモデルの性能を評価できます。
例えば、数学の問題を5問同時に与えて、すべてを正しく解答できるかをテストします。
驚くべきことに、最先端のDeepSeek-R1でさえ、複数問題を同時に扱うと大幅に性能が低下することが明らかになりました。
GitHubリポジトリ（https://github.com/opendatalab/REST）でコードとデータが公開されています。

## 1. 研究概要
### 1.1 背景と動機
近年、大規模推論モデル（LRM）は数学的問題解決、コード生成、複雑な概念理解などの推論タスクで目覚ましい性能を示しています。
しかし、現在の評価手法には2つの根本的な問題があります。

第一に、多くのベンチマーク（GSM8KやMATHなど）でモデルの性能が飽和状態に達しており、例えばDeepSeek-R1はMATH500で97.0%という極めて高い精度を達成しています。
このため、モデル間の性能差を識別することが困難になり、コミュニティは既存のベンチマークを捨てて、より難しい新しいデータセットを作り続けるという非効率なサイクルに陥っています。

第二に、単一問題での評価では、実世界で求められるマルチコンテキストシナリオでのモデルの性能を適切に評価できません。
例えば、教育用チューターシステムでは、AIは学生からの複数の追加質問に対応しながら、以前の誤解を修正する必要があります。
また、技術サポートでは、複数のユーザーから報告された問題を単一のコンテキストウィンドウ内で処理する必要があります。

### 1.2 主要な貢献
本研究では、既存のベンチマークをより挑戦的なバリアントに変換するシンプルかつ強力な手法「REST（Reasoning Evaluation through Simultaneous Testing）」を提案しています。
RESTの主要な貢献は以下の通りです。
- 既存のベンチマークを複数問題形式に変換することで、モデルの真の推論能力を評価する新しい評価プロトコルの確立
- 34の最先端推論モデルと7つの代表的な推論ベンチマークを用いた包括的な評価の実施
- 最先端モデルでさえストレステストで大幅な性能低下を示すという重要な発見
- 「過剰思考（overthinking）」現象の特定と、その性能低下への影響の解明
- 「Long2Short」訓練技術がマルチ問題シナリオで優位性を持つことの実証

## 2. 提案手法
### 2.1 手法の概要
RESTは、既存のベンチマークを変換して認知的負荷を体系的に増加させる評価プロトコルです。
元のベンチマークを$\mathcal{Q} = \{q_1, q_2, \dots q_N\}$（$q_i$は個々の質問）とすると、RESTはこれを新しいプロンプト集合$\mathcal{P}^s$に変換します。
ここで、$s$個の連続する質問を各プロンプトに連結します。
パラメータ$s \in \mathbb{Z}^+$はストレスレベルと呼ばれ、大きな$s$はモデルにより大きな推論負荷を課します。

具体的には、各プロンプト$p_i^s$は以下のように定義されます：
$$p_{i}^{s} = \texttt{Compose}(q_{i}, q_{i+1}, \dots, q_{[(i+s-1)\bmod N]})$$

ここで、$\texttt{Compose}()$関数は複数の質問を単一のプロンプトにフォーマットします：
「Q1: {q_1}, ..., Qs: {q_s}. Answer the above questions one by one.」

### 2.2 技術的詳細
RESTの評価では、LRMモデル$f$に対して各プロンプト$p_i^s$を入力し、出力応答$o_i^s=f(p_i^s)$を取得します。
この応答から、個々の予測回答を抽出します。

正確な抽出を容易にするため、タスクに応じた特定のフォーマットで回答するようモデルに指示します：
- 数学問題とGPQA問題：各回答を「\boxed{}」内に記載
- コード生成タスク：Pythonコードブロック「```python ```」で回答をラップ

モデルの精度は、予測回答と正解を比較することで計算されます。
重要な点として、各元の質問$q_i$はすべてのプロンプトにわたって正確に$s$回出現し、連結されたプロンプト内の$s$個の可能な位置のそれぞれに正確に1回ずつ現れるように設計されています。
これにより、位置バイアスを軽減し、ストレスレベル全体での包括的なカバレッジを確保しています。

### 2.3 新規性
RESTの新規性は以下の点にあります：

第一に、単純な質問の連結という手法でありながら、モデルの隠れた弱点を効果的に露呈させる点です。
従来の研究では比較的単純なタスク（テキスト分類やコモンセンスQA）での複数問題プロンプティングに焦点を当てていましたが、RESTは複雑な推論タスクに適用することで、「LLMはマルチ問題ソルバーである」という一般的な仮定に疑問を投げかけています。

第二に、クロス問題の干渉と推論トークン配分という新しい評価軸を提供します。
マルチ問題評価では、異なる質問と推論パスが相互作用することで自然な干渉が生じ、潜在的な注意散漫の中でフォーカスを維持するモデルの能力を評価できます。

第三に、最小限のコストで既存のベンチマークのストレステスト版を生成できる点です。
人間の専門家によるアノテーションや検証に依存する従来のベンチマーク開発とは異なり、RESTは多様なドメインに適用可能なスケーラブルで費用対効果の高い代替手段を提供します。

## 3. 実験結果
### 3.1 実験設定
34の大規模推論モデル（パラメータサイズ1.5Bから671B）を評価しました。
温度とtop_pパラメータは各モデルの公式ガイドラインに従って設定し、推論モデルには最大32K、非推論モデルには8Kの出力トークン長を設定しました。

7つの代表的なベンチマーク（GSM8K、MATH500、AMC23、AIME24、AIME25、GPQA、LiveCodeBench）を使用し、各ベンチマークの難易度に応じて異なるストレスレベルを設定しました：
- 比較的簡単なベンチマーク（GSM8K）：$s \in \{1, 3, 6, 9, 12\}$
- 中程度の難易度（MATH500、AMC23）：$s \in \{1, 3, 5, 7, 9\}$
- 高難易度（AIME24、AIME25、GPQA、LiveCodeBench）：$s \in \{1, 2, 3, 4, 5\}$

### 3.2 主要な結果
実験結果から、以下の重要な知見が得られました：

**LRMは複数の簡単な問題は処理できるが、難しい問題では苦戦する**
GSM8Kでは、DeepSeek-R1-Distill-Qwen-7B（R1-7B）とDeepSeek-R1-Distill-Qwen-32B（R1-32B）のREST下での精度低下はそれぞれわずか0.43%と0.04%でした。
しかし、より難しい問題では性能が大幅に低下します。
例えば、DeepSeek-R1のAIME24とAIME25での精度は、単一問題設定と比較してそれぞれ29.17%と31.58%低下しました。

**RESTは既存ベンチマークの識別力を向上させる**
MATH500において、R1-7BとR1-32Bの単一問題精度はそれぞれ93.0%と94.6%でわずか1.6%の差でしたが、REST評価ではR1-7Bの精度が66.75%に急落し、R1-32Bは88.97%を維持し、22.22%の顕著な性能差が明らかになりました。

**ポストトレーニングの限界**
DeepSeek-R1の蒸留モデルに対する追加のRL訓練やSFTは、単一問題での精度向上をもたらしますが、RESTのようなマルチ問題シナリオではその優位性を維持できません。
例えば、AReaL-boba-RL-7BとLight-R1-7B-DSは、MATH500の単一問題で95.00%と93.20%の精度を達成しましたが、REST下ではそれぞれ60.77%と61.73%に低下し、R1-7Bの66.75%を下回りました。

### 3.3 既存手法との比較
Long2Short訓練を受けたモデルは、REST下で顕著な性能向上を示しました。
例えば、L1-Qwen-1.5B-Maxは、MATH500のストレスレベル$s = 9$でR1-1.5Bを44.71%の精度差で上回りました。
同様に、Efficient-R1-7B（$\alpha = 0.2$）は、単一問題精度は88.20%とR1-7Bの93.00%より低いものの、ストレスレベル$s = 9$では66.02%の精度を達成し、R1-7Bの49.42%を16.60%上回りました。

これらの結果は、Long2Short技術が推論の冗長性を減らし、マルチ問題処理能力を向上させる有望な方向性であることを示しています。

## 4. 実用性評価
### 4.1 実装の容易性
RESTは既存のベンチマークを単純に変換するだけで実装でき、特別なハードウェアや複雑なセットアップは不要です。
評価プロトコルは明確で、任意の推論ベンチマークに適用可能です。
提供されているGitHubリポジトリには、REST評価を実行するための完全なコードと詳細なドキュメントが含まれています。

また、回答抽出の方法も柔軟で、ルールベースの方法（正規表現を使用）とLLMベースの方法の両方をサポートしています。
これにより、様々なタスクタイプやモデル出力形式に対応できます。

### 4.2 計算効率
RESTは追加の計算リソースをほとんど必要としません。
単一問題評価と比較して、主な違いは入力プロンプトが長くなることですが、これは現代のLRMにとって大きな負担ではありません。
実験では、128Kトークンの出力制限でも32Kトークンと比較して無視できる程度の性能差しか観察されませんでした。

評価時間は基本的にモデルの推論時間に依存し、REST特有のオーバーヘッドは最小限です。
これにより、大規模な評価キャンペーンでも実用的な時間内で完了できます。

### 4.3 応用可能性
RESTの応用範囲は広く、以下のような分野で活用できます：

**モデル開発とデバッグ**：RESTは、単一問題評価では隠れているモデルの弱点を露呈させるため、開発者がモデルの改善点を特定するのに役立ちます。

**実世界アプリケーションの評価**：教育システム、カスタマーサポート、コーディングアシスタントなど、複数のタスクを同時に処理する必要がある実際のアプリケーションでのモデル性能を予測できます。

**新しい訓練手法の検証**：Long2Short訓練のような新しい手法が、実際にマルチタスク環境での性能向上に寄与するかを評価できます。

**ベンチマークの寿命延長**：飽和状態に達した既存のベンチマークを、新たな評価軸で再活用できます。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、大規模推論モデルの評価パラダイムに重要な転換をもたらしています。
単純な手法でありながら、最先端モデルの隠れた限界を効果的に露呈させ、「LLMは本質的にマルチ問題ソルバーである」という一般的な仮定に疑問を投げかけています。

特に重要なのは、RESTが既存のベンチマークの有用性を復活させる点です。
新しいベンチマークを継続的に作成する必要性を減らし、コミュニティのリソースをより効率的に活用できるようになります。

また、エラー分析により明らかになった「過剰思考」現象や質問位置バイアスなどの知見は、今後のモデル設計に重要な示唆を与えています。
これらの問題は、単一問題評価では検出が困難であり、RESTのようなストレステストの重要性を示しています。

### 5.2 今後の展望
今後の研究では、以下の方向性が期待されます：

**適応的推論努力配分の改善**：モデルが複数の問題に対して推論努力を動的に配分する能力を向上させる手法の開発が必要です。

**新しい訓練パラダイム**：RESTでの性能を直接最適化する訓練手法の開発により、より実用的なLRMの構築が可能になるでしょう。

**ドメイン特化型REST**：特定の応用分野（医療診断、法的推論など）に特化したREST変種の開発により、より実践的な評価が可能になります。

**認知科学との連携**：人間の認知負荷とモデルのストレステスト性能の関係を研究することで、より人間らしい推論システムの開発につながる可能性があります。

RESTは、モデル評価の新しい標準となる可能性を秘めており、より堅牢で実用的な推論システムの開発を促進する重要な貢献と言えるでしょう。