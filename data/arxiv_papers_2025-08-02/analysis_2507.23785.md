# Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis

## 基本情報
- arXiv ID: 2507.23785v1 (https://arxiv.org/abs/2507.23785)
- 著者: Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, Baining Guo
- 所属: University of Science and Technology of China, Microsoft Research Asia
- 投稿日: 2025年08月01日
- カテゴリ: cs.CV

## 簡単に説明すると
この論文は、ビデオから高品質な4D（3D+時間）コンテンツを生成する新しい手法「Gaussian Variation Field Diffusion」を提案しています。
従来の4D生成手法が抱えていた計算コストの高さと品質の問題を解決するため、3Dガウシアンスプラッティング（3DGS）の時間変化を効率的にモデル化する新しいアプローチを採用しています。
特に、メッシュデータから直接4DGSの変化を学習するVAEと、その変化を生成する拡散モデルを組み合わせることで、1つのビデオから数秒で高品質な4Dアニメーションを生成できます。
プロジェクトページ：https://gvfdiffusion.github.io/

## 1. 研究概要
### 1.1 背景と動機
近年、画像、ビデオ、3Dコンテンツ生成の分野では生成モデルが大きな進歩を遂げていますが、動的な3Dコンテンツ（4D）の生成は依然として大きな課題として残されています。
現実世界の現象は本質的に空間と時間の両方の次元を持っているにもかかわらず、効率的で高品質な4D生成手法の開発は十分に進んでいませんでした。

4D拡散モデルの学習には主に2つの技術的課題があります。
第一に、大規模な4Dデータセットの構築が非常に時間がかかることです。
各3Dアニメーションシーケンスに対して個別に動的ガウシアンスプラッティング（4DGS）表現をフィッティングする従来のアプローチでは、1インスタンスあたり数十分を要していました。
このため、スケーラビリティに大きな問題がありました。
第二に、3D形状、外観、動きを同時に表現するには通常10万以上のトークンが必要となり、この高次元性が拡散モデルによる直接的なモデリングを極めて困難にしていました。

### 1.2 主要な貢献
本研究の主要な貢献として、以下の4点があげられます。
- コストのかかる再構築を回避し、4Dメッシュデータから直接正準3DGSとその変動場をエンコードする新しいVAEを提案。
- メッシュベースのground truthモーションとガウシアンスプラッティング表現のギャップを埋める「mesh-guided loss」を導入し、高品質な動きの再構築を実現。
- 入力ビデオと正準3DGSを条件として、変動場の潜在空間分布を学習する「Gaussian Variation Field diffusion model」を開発。
- 合成データのみで学習しながら、実世界のビデオ入力に対しても優れた汎化性能を示すことを実証。

## 2. 提案手法
### 2.1 手法の概要
提案手法は、入力ビデオシーケンスから3DGSモデルのシーケンスを生成することを目的としています。
このタスクを、正準GS（第1フレームを正準とする）の生成と、各ガウシアンの属性の時間変化を表すガウシアン変動場の生成に分解します。

フレームワークは2つの主要コンポーネントから構成されています。
1. 3Dアニメーションシーケンスを効率的にコンパクトな潜在空間にエンコードする「Direct 4DMesh-to-GS Variation Field VAE」
2. 入力ビデオと正準GSを条件として変動場の潜在分布を学習する「Gaussian Variation Field diffusion model」

### 2.2 技術的詳細
**Direct 4DMesh-to-GS Variation Field VAE**では、メッシュアニメーションのシーケンスから点群を生成し、フレーム間の対応点の時間差分を変位場として計算します。
事前学習済みのmesh-to-GSオートエンコーダを活用して正準GS表現を取得し、perceiver型のトランスフォーマーネットワークを用いて動き情報を効果的にエンコードします。

特に重要な技術的工夫として、正準ガウシアン位置と正準点群の空間的な対応関係に基づいて動き認識クエリベクトルを生成する「mesh-guided interpolation」メカニズムを導入しています。
各正準ガウシアン位置に対してK近傍の点を特定し、局所的な点分布に基づいて適応的な影響半径を調整することで、メッシュの変形に対するネットワークの感度を向上させています。

**Gaussian Variation Field Diffusion**では、コンパクトな潜在空間を活用してDiffusion Transformer（DiT）アーキテクチャを採用しています。
時間的な一貫性を確保するため、標準的な空間の自己注意層に加えて専用の時間の自己注意層を導入しています。
DINOv2で抽出した入力ビデオフレームの特徴量と正準GSの幾何学的特徴を交差注意層を通じて条件付けしています。

### 2.3 新規性
既存手法との主な違いとして、以下の点があげられます。
- インスタンスごとの時間のかかる最適化を必要とせず、メッシュデータから直接4D表現を学習できるVAEフレームワーク。従来手法と比較して処理時間を数十分から数秒に短縮。
- メッシュの動きとガウシアン表現を結びつける新しいmesh-guided lossによる高品質な動きの再構築。
- 4D生成を正準3D生成と変動場モデリングに分解することで、既存の高品質3D生成モデルを活用しつつ計算複雑性を約16倍削減（N=8192からL=512へ）。
- 正準GSの位置情報を拡散モデルに組み込むことで、空間的な対応関係の認識を強化。

## 3. 実験結果
### 3.1 実験設定
実験はObjaverse-V1とObjaverse-XLデータセットを使用し、高品質なアニメーションを持つ34,000個のオブジェクトで学習しました。
評価には、Consistent4Dテストセットの7インスタンスとObjaverse-XLからの追加93インスタンスを組み合わせた100オブジェクトの包括的なテストセットを構築しました。

評価指標として、フレーム単位の品質評価にPSNR、LPIPS、SSIMを使用しています。
生成されたシーケンスの時間的な一貫性評価にはFVDを使用しています。
すべての評価は512×512解像度でレンダリングされた画像に対して実施されました。

### 3.2 主要な結果
定量的評価では、提案手法が既存の最先端手法と比較してすべての品質指標で一貫した改善を示しました。
- PSNR: 18.47（L4GMの17.03から改善）
- LPIPS: 0.114（L4GMの0.128から改善）
- SSIM: 0.901（L4GMの0.891から改善）
- CLIP: 0.935（L4GMの0.930から改善）
- FVD: 476.83（L4GMの529.10から改善）

生成時間は4.5秒（正準GS生成に3.0秒、ガウシアン変動場拡散に1.5秒）で、フィードフォワード手法のL4GM（3.5秒）とほぼ同等の効率性を実現しています。

### 3.3 既存手法との比較
SDS（Score Distillation Sampling）ベースの手法（Consistent4D、STAG4Dなど）は、ぼやけたテクスチャと貧弱なジオメトリを生成する傾向がありました。
これらの手法は数十分から数時間の最適化時間を必要とし、空間時間的な不整合や入力との整合性の問題も抱えていました。

フィードフォワード手法のL4GMは、2D生成事前学習モデルから生成された多視点画像を使用して4DGSシーケンスを再構築しますが、生成された多視点画像の3D不整合性に起因する問題がありました。

対照的に、提案手法は正準GSとガウシアン変動場を直接生成することで、時間的に一貫した高忠実度の3Dアニメーションを作成できることが示されました。

## 4. 実用性評価
### 4.1 実装の容易性
提案手法の実装は、既存の3D生成モデル（Trellis）のアーキテクチャを基盤として活用しており、比較的容易に実装可能です。
VAEの学習は2段階で行われ、最初に正準3Dのみでスパース GSデコーダを150K反復でファインチューニングし、その後4Dアニメーションデータで他のモジュールと共同学習を200K反復行います。

拡散モデルは24フレームのシーケンスで学習され、推論時には32フレームまで拡張可能です。
学習には8台のNvidia Tesla A100 GPU（80GB）で約1週間を要しますが、一度学習が完了すれば、単一のA100 GPUで数秒での生成が可能です。

### 4.2 計算効率
生成時の計算効率は非常に高く、4.5秒で完全な4Dアニメーションシーケンスを生成できます。
これは、従来の最適化ベース手法が数十分から数時間を要していたことと比較して、10倍から1000倍の高速化を実現しています。

VAEによる圧縮により、シーケンス長がN=8192からL=512に削減され、後続の拡散モデリング空間が16分の1に縮小されています。
この圧縮により、高次元の4D表現を扱いながらも実用的な計算コストを実現しています。

### 4.3 応用可能性
提案手法は以下のような幅広い応用が可能です。

**既存3Dアセットのアニメーション化**：学習済みモデルは、既存の3Dモデルを条件付きビデオに従ってアニメーション化できます。
ユーザーは3Dアセットをマルチビュー画像としてレンダリングし、ビデオ拡散モデルで2Dアニメーションを作成した後、提案手法で対応する4Dアニメーションを生成できます。

**実世界ビデオからの4D生成**：合成データのみで学習しているにもかかわらず、実世界のビデオ入力に対しても優れた汎化性能を示しています。
これにより、実際のビデオコンテンツから3Dアニメーションを作成する実用的なワークフローが可能になります。

**長時間アニメーションの生成**：自己回帰的なアプローチにより、学習時の長さを超えるアニメーションの生成も可能です。
生成されたセグメントの最終フレームのGSを次のセグメントの正準GSとして使用することで、一貫性のある長時間アニメーションを作成できます。

## 5. まとめと所感
### 5.1 論文の意義
本論文は、4D生成分野における重要な技術的進歩を示しています。
特に、高次元の4D表現を効率的に扱うための新しいアプローチを提案し、実用的な生成速度と高品質を両立させた点が評価できます。

Direct 4DMesh-to-GS Variation Field VAEによるエンコーディングは、従来のインスタンスごとの最適化アプローチの限界を克服しました。
これにより、大規模な4D生成モデルの学習が可能になりました。
また、mesh-guided lossとmesh-guided interpolationの導入により、メッシュとガウシアン表現のギャップを埋めることに成功しました。

合成データのみで学習して実世界のビデオに対して優れた汎化性能を示したことは、実用的な観点から特に重要です。
これにより、プライバシーや著作権の問題を回避しつつ、実用的な4D生成システムを構築できる可能性が示されました。

### 5.2 今後の展望
本研究にはいくつかの限界と改善の余地があります。

**静的3D生成モデルへの依存**：現在の2段階生成プロセスでは、静的3D生成モデルと条件付きビデオとの不整合は重要な課題です。
この不整合によって、最終的なアニメーションの品質が低下します。
将来的には、正準表現とその時間変化を共同で生成するエンドツーエンドの4D拡散フレームワークの開発に期待が寄せられています。

**より複雑な動きへの対応**：現在のモデルは主に剛体変換や単純な変形を扱っていますが、より複雑な非剛体変形や相互作用を含むシーンへの拡張が今後の課題です。

**リアルタイム性の向上**：現在の4.5秒という生成時間は既に実用的ですが、インタラクティブなアプリケーションのためにはさらなる高速化が望まれます。

本研究は、静的3D生成と4Dコンテンツ作成のギャップを埋める重要な一歩であり、高品質な4Dコンテンツ生成への道を開いたと言えるでしょう。