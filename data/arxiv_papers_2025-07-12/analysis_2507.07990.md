# Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs

## 基本情報
- **arXiv ID**: 2507.07990v1 (https://arxiv.org/abs/2507.07990)
- **著者**: Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim
- **所属**: Yonsei University, Carnegie Mellon University, NAVER Cloud, Adobe Research
- **投稿日**: 2025年7月11日
- **カテゴリ**: cs.CV, cs.LG

## 簡単に説明すると
この論文は、動画を理解するAI（Video LLM）の処理速度を大幅に改善する手法「STTM」を提案している。動画は画像の連続であるため処理するデータ量が膨大になるが、STTMは動画内の似た部分（空間的・時間的な冗長性）を自動的に見つけて統合することで、データ量を削減する。例えば、背景が変わらない部分や、同じ物体が複数フレームに現れる部分を効率的にまとめる。50%のデータ削減で2倍の高速化を達成しながら、精度低下は0.5%に抑えている。学習不要でプラグイン可能なため、既存のVideo LLMにすぐに適用できる。

関連リンクは以下の通りである：
- プロジェクトページ: https://www.jshyun.me/projects/sttm
- GitHub: （論文中に記載なし）

## 1. 研究概要
### 1.1 背景と動機
Video LLMは、動画理解において強力な能力を示すが、時空間トークンの数が多いため計算コストが二次的にスケールする課題がある。従来の手法は主に2つのアプローチを取っている。初期のVideo LLMはQ-Formerなどの抽象化器を学習してビジュアル情報を圧縮したが、これには追加学習が必要である。一方、FlashAttentionの登場により長文脈LLMの学習が可能になり、高解像度の空間特徴を保持するLLaVAスタイルのVideo LLMが登場した。

既存のトークン削減手法の多くはクエリ依存（query-aware）であり、質問に基づいてトークンを削除する。しかし、実用的なシナリオでは同じ動画に対して複数の質問を行うことが多く、KVキャッシュの再利用が重要となる。VideoMAEの研究により、動画は画像よりも低いトークン比率から再構成可能であることが示されており、これは動画の時空間的連続性による冗長性を示唆している。

### 1.2 主要な貢献
STTMの主要な貢献は以下の通りである：
- 学習不要（training-free）で既存のVideo LLMに容易に統合可能な時空間トークンマージング手法の提案
- クエリ非依存（query-agnostic）な設計により、同一動画に対する複数質問でKVキャッシュを再利用可能
- 空間的マージングと時間的マージングを分解したアプローチにより、多粒度の動画トークン表現を実現
- 6つのビデオQAベンチマークで既存手法を上回る性能を達成（50%トークンで0.5%、30%トークンで2%の精度低下）
- 細粒度理解が必要なVNBenchで特に優れた性能を示し、空間的詳細の保持能力を実証

## 2. 提案手法
### 2.1 手法の概要
STTM（Spatio-Temporal Token Merging）は、動画トークンを空間次元と時間次元に沿って段階的にマージする手法である。LLMの中間層（主に第3層）に挿入され、以下の2段階で処理を行う：

1. **空間的マージング**：各フレームを多粒度の空間トークンに変換。Quadtree構造を用いて粗から細へ階層的に探索し、類似度が高い領域は粗いトークンで、詳細が必要な領域は細かいトークンで表現
2. **時間的マージング**：連続フレーム間で空間的に重なるトークンを比較し、類似度が高いトークンペアを時空間グラフとして連鎖。より早いフレームへ向けてマージすることで、コンテンツが最初に現れたトークンに時間的変化を蓄積

### 2.2 技術的詳細
**空間的マージング（Quadtree探索）**：
- 各レベルで親ノードと4つの子ノード間のコサイン類似度を計算
- 類似度が閾値τ_Sを超える場合、その領域は低詳細と判断し、粗いトークンで表現
- 類似度が低い場合、より細かいレベルへ細分化して高周波情報を保持
- 計算複雑度：O(HW)per frame（最悪ケースでも4HW/3に収束）

**時間的マージング（有向グラフ構築）**：
- 空間的に重なるトークン間の類似度を計算し、閾値τ_Tを超えるペアを接続
- 多対一（細→粗）：複数の細かいトークンを早いフレームの粗いトークンへマージ
- 一対多（粗→細）：最も類似した細かいトークンではなく、実装効率のため左上トークンを選択
- ベクトル化されたUnion-Findアルゴリズムによる効率的な実装
- 計算複雑度：O(THW)per video

**トークン再配列**：
- 空間順序：各トークンの左上座標に基づくZ字スキャン
- 時間順序：早いフレームのトークンを優先
- RoPE（回転位置埋め込み）の3つの戦略を評価：Merged、Survived、Reassigned

### 2.3 新規性
既存手法と比較したSTTMの新規性は以下の点にある：
- **学習不要かつクエリ非依存**：既存のFastVやDyCokeなどはクエリ依存でKVキャッシュを再利用できない
- **多粒度表現**：単一粒度の削減ではなく、Quadtreeによる階層的な空間表現を実現
- **分解型アプローチ**：Octreeなどの剛直な時空間分割ではなく、空間→時間の段階的マージング
- **局所性の活用**：動画の時空間的連続性を明示的に活用した初の学習不要手法

## 3. 実験結果
### 3.1 実験設定
評価は以下の設定で実施された：
- **データセット**：6つのビデオQAベンチマーク
  - 短編：EgoSchema、NExT-QA
  - 長編（時間単位）：VideoMME、LongVideoBench、MLVU
  - 細粒度理解：VNBench（needle in a haystackタスク）
- **モデル**：LLaVA-Video-7B、LLaVA-OneVision-7B、Qwen2VL-7B、LLaVA-Video-72B
- **評価指標**：精度、TTFT（Time-To-First-Token）、視覚トークン数
- **実装詳細**：1FPSでサンプリング、最大フレーム数128（VNBenchは180）

### 3.2 主要な結果
**LLaVA-Video-7Bでの性能**：
- 50%トークン削減時：平均0.5%の精度低下で2倍の高速化
- 30%トークン削減時：平均2.2%の精度低下で3倍の高速化
- VNBench（30%）：他のQuery-Agnostic手法が約18%低下に対し、STTMは2.0%の低下に留まる

**他モデルへの汎化**：
- LLaVA-OneVision：精度が1.1%向上しながら3.1倍の高速化（50%削減時）
- Qwen2VL：精度が0.5%向上しながら4.5倍の高速化（30%削減時）
- LLaVA-Video-72B：精度が1.3%向上しながら44.2%のトークンのみ使用

**可視化結果**：
- VideoMME（30%削減）でトークン保持率は3.3%～51.2%と動画内容に応じて適応的に変化
- 意味的に類似したトークンは効果的にマージ（196→63トークン/フレーム）
- OCR精度：全トークン使用時67.6%に対し、STTM使用時65.5%を維持

### 3.3 既存手法との比較
6つのベンチマーク平均でSTTMは以下の性能を示した：
- **Query-Aware手法**（FastV、FrameFusion）：精度は高いがKVキャッシュ再利用不可
- **Query-Agnostic手法**（ToMe、DyCoke-stage1）：
  - ToMe：空間的構造を考慮せず、VNBenchで大幅な精度低下
  - DyCoke-stage1：時間的情報のみ考慮、細粒度タスクで劣る
- **STTM**：両カテゴリを上回る性能、特にVNBenchでの優位性が顕著

## 4. 実用性評価
### 4.1 実装の容易性
STTMの実装は非常にシンプルである：
- 既存のVideo LLMの早期層（第3層推奨）に挿入するだけで動作
- 学習不要で追加のパラメータも不要
- Quadtree探索とUnion-Findは標準的なアルゴリズムで実装容易
- PyTorchなどの一般的なフレームワークで実装可能
- バッチ処理との互換性については今後の検討が必要

### 4.2 計算効率
計算効率の観点では以下の特徴がある：
- 空間マージング：O(HW)の線形計算量、並列実装により高速
- 時間マージング：O(THW)の計算量、ベクトル化により効率的
- トークン削減によるメモリ使用量削減（50%削減で約半分）
- KVキャッシュの再利用により、同一動画への複数質問で大幅な効率化
- 72Bモデルで平均17.7秒→約半分に短縮

### 4.3 応用可能性
STTMは以下の応用分野で活用可能である：
- **リアルタイム動画理解**：監視カメラ、ライブストリーミング分析
- **長時間動画処理**：映画、講義動画の要約・検索
- **マルチモーダル対話システム**：同一動画に対する複数回の質問応答
- **エッジデバイス展開**：計算リソースが限られた環境での動画AI
- **大規模動画検索**：効率的な動画インデクシングと検索

## 5. まとめと所感
### 5.1 論文の意義
STTMは、Video LLMの実用化における重要なボトルネックである計算コストの問題に対する実践的な解決策を提供している。学習不要でプラグイン可能という特性により、既存のVideo LLMにすぐに適用できる点が特に重要である。

この研究の意義は以下の点にある：
1. **実用性の高さ**：追加学習なしで2-3倍の高速化を実現し、即座に展開可能
2. **KVキャッシュの再利用**：クエリ非依存設計により、実用シナリオでの効率を大幅に改善
3. **細粒度理解の保持**：VNBenchでの優れた性能が示すように、重要な視覚的詳細を保持
4. **汎用性**：異なるVideo LLMアーキテクチャで一貫した改善を実証

### 5.2 今後の展望
STTMの今後の発展可能性として以下が考えられる：

1. **適応的閾値選択**：現在は手動調整が必要な閾値（τ_S、τ_T）を自動的に決定する機構
2. **動的トークン予算**：コンテンツの複雑さに応じて削減率を動的に調整
3. **マルチモーダル拡張**：音声や字幕など他のモダリティとの統合
4. **ハードウェア最適化**：GPU/TPU向けの専用カーネル実装による更なる高速化
5. **ストリーミング対応**：リアルタイム動画処理のための逐次的マージング

総じて、STTMはVideo LLMの実用化に向けた重要な一歩であり、計算効率と精度のトレードオフに新しいベースラインを確立した革新的な研究である。
