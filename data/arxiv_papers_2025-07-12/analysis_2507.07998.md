# PyVision: Agentic Vision with Dynamic Tooling

## 基本情報
- arXiv ID: 2507.07998v1 (https://arxiv.org/abs/2507.07998)
- 著者: Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei
- 所属: Shanghai AI Lab, Rice University, CUHK, NUS
- 投稿日: 2025年7月12日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると
PyVisionは、視覚的な問題を解くために動的にPythonコードを生成・実行できるエージェント型のマルチモーダルAIフレームワークである。従来の固定的なツールセットに依存する手法とは異なり、PyVisionはタスクに応じて必要なツールをその場でPythonコードとして生成する。この革新的なアプローチにより、画像の切り抜き、エッジ検出、ヒストグラム解析、視覚的な補助線の描画など、多様な視覚処理を動的に実行できる。

関連リンクは以下の通りである。
- プロジェクトページ: https://agent-x.space/pyvision/
- GitHub: https://github.com/agents-x-project/PyVision
- HuggingFace Demo: https://huggingface.co/spaces/Agents-X/PyVision

## 1. 研究概要
### 1.1 背景と動機
近年、大規模言語モデル（LLM）は単なるテキスト生成から、自律的に計画を立てて実行するエージェントへと進化してきた。しかし、視覚推論の分野では、これまでの手法は事前に定義されたワークフローと静的なツールセットに依存しており、柔軟性に欠けていた。

PyVisionは、マルチモーダル大規模言語モデル（MLLM）の高度なコーディング能力と推論能力を活用し、視覚的な問題に対して動的にツールを生成するという新しいアプローチを提案する。これは、ベンジャミン・フランクリンの「人間は道具を作る動物である」という言葉に象徴されるように、AIシステムが単にツールを使うだけでなく、必要なツールを発明する能力を持つことの重要性を示している。

### 1.2 主要な貢献
PyVisionの主要な貢献は以下の通りである。
- 視覚推論タスクにおいて、動的にPythonコードを生成・実行する対話型フレームワークの提案
- 生成されたツールの分類体系（タクソノミー）の構築と、タスク・ドメイン別のツール使用パターンの分析
- 6つのベンチマークにおける一貫した性能向上の実証（GPT-4.1でV*ベンチマークにおいて+7.8%、Claude-4.0-SonnetでVLMsAreBlind-miniにおいて+31.1%の改善）
- バックエンドモデルの固有の強みを増幅する効果の発見

## 2. 提案手法
### 2.1 手法の概要
PyVisionは、MLLMとPythonランタイムの間で対話的なマルチターンループを構築するフレームワークである。各セッションでは、MLLMが入力に応じてPythonコードを生成し、隔離されたPythonランタイムで実行する。実行結果（テキスト、視覚、またはその両方）はMLLMのコンテキストにフィードバックされ、モデルは最終的な答えを出すまで推論を繰り返し改善できる。

このアプローチの特徴は、検出やセグメンテーションモデルなどの固定ツールセットに依存せず、Pythonを唯一のツール作成インターフェースとして使用することである。これにより、OpenCV、Pillow、NumPy、Pandas、Scikit-learn、Scikit-imageなどの豊富なPythonライブラリエコシステムを活用できる。その結果、多様なタスクに適応したツールを生成できる。

### 2.2 技術的詳細
PyVisionの実装には、次のような重要な設計原則が組み込まれている。

システムプロンプト設計では、以下の要素が含まれる。
- 画像は事前に`image_clue_i`という変数名でロードされ、モデルは追加のロードコードなしで参照可能
- コード出力は`print()`（テキスト結果）と`plt.show()`（画像可視化）を通じて期待される
- 生成されたコードブロックは`<code>`タグで囲まれ、確実な解析を可能に
- 最終回答は`<answer>`タグで囲まれ、一貫した評価を実現

マルチターン相互作用の仕組みには、次の3つの要素がある。
1. プロセス分離：各コードスニペットは動的に生成されたサブプロセスで実行され、クラッシュや副作用が全体のセッションに影響しないよう設計
2. ターン間の永続性：ランタイム環境は変数と状態をターン間で保持し、前のターンの中間結果を再利用・修正可能
3. ファイルシステム安全なI/O：ランタイムとMLLM間の通信は構造化された変数受け渡しで処理され、ホストファイルシステムへの直接依存を回避

### 2.3 新規性
PyVisionの新規性は以下の点にある。
- 動的ワークフロー、動的ツール生成、マルチターンフレームワークの3つを同時に実現する初めての視覚推論システム
- 事前定義されたツールセットや外部モデルに依存せず、Pythonコードのみで柔軟なツール生成を実現
- タスクとドメインに応じて適応的にツールを生成し、問題固有の解決策を創出する能力

## 3. 実験結果
### 3.1 実験設定
PyVisionの評価には、6つの多様なベンチマークを使用した。
- マルチモーダル数学：MathVistaとMathVision（視覚的知覚と数値推論を組み合わせた数学問題）
- ドメイン・論理推論：MMMU（複数分野にわたる大学レベルの知識を要求）とVisualPuzzles（アルゴリズム的、類推的、演繹的、帰納的、空間的推論）
- 記号的視覚：VLMsAreBlind（抽象的で構造化された視覚プリミティブの解析と推論）
- 細粒度の視覚検索：V*（微妙な視覚的詳細を特定する必要がある高解像度サンプル）

また、医療画像VQAとリモートセンシングVQAの2つの特殊ドメインでも評価した。

### 3.2 主要な結果
PyVisionは、GPT-4.1とClaude-4.0-Sonnetの両方のバックエンドモデルで一貫した性能向上を示した。

GPT-4.1での結果は以下の通りである。
- V*ベンチマーク：68.1% → 75.9%（+7.8%）
- MathVista：69.9% → 71.7%（+1.8%）
- MMMU：71.9% → 74.3%（+2.4%）
- VisualPuzzles：44.9% → 47.4%（+2.5%）

Claude-4.0-Sonnetでの結果は以下の通りである。
- VLMsAreBlind-mini：48.1% → 79.2%（+31.1%）
- VisualPuzzles：42.7% → 51.0%（+8.3%）
- MathVista：71.4% → 76.2%（+4.8%）

### 3.3 既存手法との比較
PyVisionは既存の視覚プログラミング手法と比較して、動的ワークフロー、動的ツール生成、マルチターンフレームワークの全てを備えた唯一のシステムである。具体的にはNMN、IEP、VisProg、ViperGPT、Visual Sketchpadなどの手法と比較した。

また、最新のモデル（GPT-4o、o1、o3）との比較でも、PyVisionを適用することで競争力のある結果を達成している。

## 4. 実用性評価
### 4.1 実装の容易性
PyVisionの実装は比較的容易である。必要な要素は以下の通りである。
- 強力なコーディング能力を持つMLLM（GPT-4.1やClaude-4.0-Sonnet）
- Pythonランタイム環境と標準的な科学計算・画像処理ライブラリ
- 適切に設計されたシステムプロンプト

オープンソースコードがGitHubで公開されており、HuggingFaceでデモも利用可能である。

### 4.2 計算効率
PyVisionの計算効率に関しては、以下の点が考慮される。
- マルチターン推論により、単一ターンよりも多くのトークンを消費する可能性がある
- 一方で、動的なツール生成により、タスクに最適化された処理が可能
- プロセス分離により、コードの実行が安全かつ効率的に行われる

### 4.3 応用可能性
PyVisionは多様な領域で応用可能である。主な応用例は以下の通りである。
- 医療画像解析：コントラスト強調やヒストグラム解析により、病理学的領域の特定を支援
- リモートセンシング：セグメンテーションツールを動的に生成し、衛星画像から建物や地形を分析
- 科学問題の解決：視覚的スケッチとマーキングにより、数学や物理の問題を解決
- ビデオ理解：フレーム選択と分析を動的に行い、長時間ビデオから必要な情報を抽出

## 5. まとめと所感
### 5.1 論文の意義
PyVisionは、視覚推論における新しい研究方向を示す重要な研究である。この研究の意義は以下の点にある。

第一に、AIシステムが単にツールを使うだけでなく、状況に応じてツールを発明する能力を実証した。これは、より柔軟で創造的なAIシステムへの道を開くものである。

第二に、動的ツール生成がバックエンドモデルの固有の強みを増幅することを発見した。知覚に強いモデル（GPT-4.1）は知覚タスクでより大きな改善を示した。抽象推論に強いモデル（Claude-4.0-Sonnet）は推論タスクでより大きな改善を示した。

第三に、生成されたツールの詳細な分析により、タスクとドメインに応じた適応的な戦略が明らかになった。これは、AIシステムが問題の性質を理解し、それに応じた具体的なアプローチを選択できることを示している。

### 5.2 今後の展望
PyVisionの今後の発展可能性として以下が考えられる。

1. より高度なツール生成：現在は主に画像処理と数値解析に限定されているが、3D処理、時系列分析、マルチモーダル融合などのより複雑なツールの生成が可能になる。

2. 学習による最適化：現在は推論時にのみツールを生成している。将来的には過去の経験から学習し、より効果的なツール生成戦略を獲得する可能性がある。

3. 他分野への拡張：視覚推論以外の分野（音声処理、ロボティクス、科学実験など）でも同様の動的ツール生成アプローチが有効である可能性がある。

4. 安全性と信頼性の向上：動的に生成されたコードの安全性を確保し、より信頼性の高いシステムを構築する。

総じて、PyVisionは視覚推論における新しい研究方向を提示し、より自律的で創造的なAIシステムへの道を切り開く画期的な研究である。