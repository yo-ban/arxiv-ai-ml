# LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation

## 基本情報
- arXiv ID: 2508.03694 (https://arxiv.org/abs/2508.03694)
- 著者: 
  - Jianxiong Gao (Fudan University/Shanghai AI Laboratory)
  - Zhaoxi Chen (S-Lab, Nanyang Technological University)
  - Xian Liu (NVIDIA)
  - Jianfeng Feng (Fudan University)
  - Chenyang Si (Nanjing University)
  - Yanwei Fu (Fudan University)
  - Yu Qiao (Shanghai AI Laboratory)
  - Ziwei Liu (S-Lab, Nanyang Technological University)
- 所属: 南京大学、復旦大学、南洋理工大学S-Lab、
  NVIDIA、上海AI研究所
- 投稿日: 2025年08月07日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると
LongVieは、マルチモーダル制御信号（密な制御信号と疎な制御信号の両方）を使用して、最大1分間の超長尺動画を生成できる制御可能な動画生成フレームワークです。従来の手法は短いクリップには効果的でしたが、時間的な不整合や視覚的劣化の問題でスケールアップが困難でした。

本研究では、4つの主要技術を導入してこれらの問題を解決しています。それは統一ノイズ初期化戦略、グローバルな制御信号の正規化、マルチモーダル制御フレームワーク、劣化を認識する訓練戦略です。プロジェクトページ（https://vchitect.github.io/LongVie-project/ ）で詳細情報が公開されています。

## 1. 研究概要
### 1.1 背景と動機
近年、動画生成の進歩は大規模データセットと強力な生成アーキテクチャ、特に拡散モデルの開発によって大きく促進されています。CogVideoX、HunyuanVideo、Kling、Sora、Wanx2.1などのモデルは、テキストプロンプトで高品質な動画を生成できるようになりました。

しかし、動画生成における主要な課題は、生成された動画がユーザーの創造的ビジョンとシームレスに一致することを保証する正確な制御を達成することです。最近の研究では、制御フレームワークを生成プロセスに統合することで、より洗練されたカスタマイズ可能な出力を実現しようとしています。一方で、これらの手法は主に短い動画クリップの制御可能な生成に焦点を当てており、1分に及ぶような長い制御可能な動画の生成に関連する課題への探求は限定的です。

実用的なアプリケーションでは、長く一貫性があり視覚的に一貫した動画を生成する能力が重要ですが、これは根本的に複雑で未解決の問題として残っています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の4点にまとめられます。

- 既存の制御可能な動画生成モデルの長尺動画における限界を包括的に分析し、長期的な時間的不整合と視覚的劣化という2つの主要な課題を特定しました。これらの問題に対処するため、制御可能な長尺動画生成のための初の自己回帰フレームワークであるLongVieを提案しました。

- 視覚品質を向上させるため、密な制御信号と疎な制御信号を統合してそれぞれの利点を活用するマルチモーダル制御メカニズムと、時間経過に伴ってそれらの貢献度をバランスする劣化を認識する訓練戦略を提案しました。

- 時間的一貫性を強化するため、時間ステップにわたって世界一貫性のある生成ダイナミクスを実現する統一ノイズ初期化とグローバルな制御信号の正規化を活用しました。

- 100本の多様で高品質な動画（各1分以上）で構成される制御可能な長尺動画生成のための評価データセットLongVGenBenchを導入しました。

## 2. 提案手法
### 2.1 手法の概要
LongVieは、CogVideoXをControlNetスタイルのアーキテクチャで拡張し、外部制御信号を組み込んでいます。軽量な制御ブランチが、ベースモデルと部分的に共有されながら制御信号を処理します。短い動画合成には効果的ですが、CogVideoXとその変種を含むほとんどの制御可能な拡散ベースのモデルは、1分のシーケンスのような長時間の生成を処理するように設計されていません。

本研究では、深度条件付きCogVideoXの変種を使用して自己回帰的にビデオを生成するアプローチを採用しています。しかし、この戦略には2つの主要な課題があります。連続するクリップ間の時間的不整合と時間経過に伴う累積エラーによる漸進的な品質劣化です。

### 2.2 技術的詳細
LongVieは以下の4つの主要な技術的コンポーネントで構成されています。

**マルチモーダル制御注入**: 深度マップを密な制御信号として、ポイントマップを疎な制御信号として採用しています。ControlNetアーキテクチャに着想を得て、事前訓練されたCogVideoX DiTの初期レイヤーを複製し、マルチモーダル条件付け入力を組み込みながら、ベースモデルを凍結したままにしています。具体的には、元のDiTブロックのパラメータθを凍結し、密（深度）と疎（ポイント）のそれぞれの制御モダリティに対応する2つの訓練可能なブランチを構築しています。

**グローバル正規化**: 独立に正規化された制御入力によって引き起こされる時間的不整合を削減するため、深度ビデオのグローバル正規化戦略を採用しています。ビデオシーケンス全体のすべてのピクセル値の5パーセンタイルと95パーセンタイルを計算し、これらをグローバルな最小および最大の境界として正規化に使用します。

**統一ノイズ初期化**: 時間的一貫性をさらに強化するため、生成中のすべてのビデオセグメントで共有ノイズ初期化を使用します。各クリップに対して異なるノイズベクトルをサンプリングする代わりに、単一のノイズインスタンスをサンプリングして、シーケンス全体に均一に適用します。

**モーダルバランスのための劣化戦略**: 密な信号と疎な信号の相対的な影響を調整し、両方のモダリティのよりバランスの取れた利用を促進するために設計された劣化ベースの訓練戦略を提案しています。この戦略は、特徴レベルとデータレベルの両方で制御された摂動を通じて、密な入力の支配を弱めます。

### 2.3 新規性
既存手法との主な違いは以下の点です。

- 短い動画クリップを連続して生成する自己回帰的アプローチを採用しながら、時間的不整合と視覚的劣化の問題を体系的に解決した初めての手法である
- 密な制御信号（深度マップ）と疎な制御信号（キーポイント）を組み合わせたマルチモーダル制御により、それぞれの長所を活かしながら短所を補完
- グローバル正規化と統一ノイズ初期化により、クリップ間の一貫性を約30%向上
- 劣化を認識する訓練により、モダリティ間のバランスを動的に調整し、長期的な品質維持を実現

## 3. 実験結果
### 3.1 実験設定
LongVieの実装では、各モデルで18個のDiTブロックをコピーして微調整しています。訓練中、Video Depth Anythingを使用して深度マップを抽出し、密な制御信号として使用します。その後、SpatialTrackerを適用して正規化された深度に基づいて3Dポイントを追跡します。

訓練には130,000本の動画を使用し、ACID、Vchitect-T2V-DataVerse、MovieNetで構成されています。各訓練動画は480×720の解像度で毎秒8フレームの49フレームクリップに分割されます。訓練はAdamWオプティマイザーを使用し、学習率1e-4で実施しました。8台のA100 GPUで約3,000イテレーション、効果的なバッチサイズ64で訓練し、完了まで約5日かかりました。

推論時、LongVieは6秒の動画をサンプリングするのに約4.5分を必要とします。その結果、1分の制御可能な動画を生成するには、単一のA100 GPUで約45分かかります。

### 3.2 主要な結果
LongVGenBenchでの定量的評価では、LongVieは全てのベースラインの中で優れた時間的一貫性と制御可能性を達成し、高いパフォーマンスを示しました。

評価指標として、VBenchに従って7つの指標（背景一貫性、被写体一貫性、全体的一貫性、時間的スタイル、動的度合い、時間的フリッカリング、画像品質）を採用し、時間的一貫性と視覚的忠実度を評価しました。また、SSIMとLPIPSなどの従来の類似性ベースの指標も報告しています。

LongVieは、すべての評価指標において高いスコアを達成しました。特に、SSIMで0.557、LPIPSで0.290という優れた結果を示し、他の手法を大きく上回りました。

### 3.3 既存手法との比較
ユーザースタディでは、60人の参加者を招いて5つの側面（視覚品質、プロンプトと動画の一貫性、条件一貫性、色一貫性、時間的一貫性）で評価しました。LongVieは全ての基準で高いスコアを獲得し、特に視覚品質で4.387、時間的一貫性で4.365という評価を得ました（5点満点）。

CogVideoX、StreamingT2V、DAS-LV、Depth-LVなどの既存手法と比較して、LongVieは長尺動画生成における制御可能性と品質の両面で優位性を示しました。

## 4. 実用性評価
### 4.1 実装の容易性
LongVieは既存のCogVideoXアーキテクチャを拡張する形で実装されており、ControlNetスタイルの設計により、制御ブランチの追加が比較的容易です。事前訓練済みモデルの大部分を凍結し、制御ブランチのみを訓練することで、計算リソースを効率的に活用できます。

### 4.2 計算効率
単一のA100 GPUで1分の動画生成に約45分かかるため、リアルタイム生成には適していませんが、高品質な長尺動画生成タスクとしては実用的な処理時間と言えます。訓練も8台のA100 GPUで5日間と、大規模モデルとしては比較的効率的です。

### 4.3 応用可能性
LongVieは様々な下流タスクに適応可能です。

**動画編集**: 初期フレームをFLUXのfillモデルで編集し、LongVieで時間的に一貫した編集動画を生成できます。

**モーション＆シーン転送**: ソース動画のモーションやレイアウトを転送し、深度とポイントマップを制御信号として使用して転送されたモーションやシーンを保持しながら時間的・視覚的一貫性を確保します。

**制御可能なメッシュから動画への変換**: テクスチャのないアニメーション3Dメッシュから長尺動画を生成できます。3Dエンジンでメッシュをレンダリングし、深度マップとポイント軌跡を抽出してLongVieをガイドします。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、制御可能な長尺動画生成という困難な課題に対して体系的なアプローチを提供しています。時間的不整合と視覚的劣化という2つの主要な問題を特定し、それぞれに対して効果的な解決策を提案している点が高く評価できます。特に、マルチモーダル制御と劣化を認識する訓練の組み合わせは、長期的な品質維持において重要な貢献です。

LongVGenBenchの導入も重要で、制御可能な長尺動画生成の研究を促進する標準的なベンチマークとして機能することが期待されます。100本の高品質な1分以上の動画で構成されるこのデータセットは、今後の研究の重要な基盤となるでしょう。

### 5.2 今後の展望
今後の研究方向として、計算効率のさらなる改善が挙げられます。現在の45分という生成時間を短縮することで、より実用的なアプリケーションへの展開が可能になるでしょう。また、より長い動画（例えば5分、10分）への拡張や、インタラクティブな制御機能の追加も興味深い方向性です。

さらに、他の制御モダリティ（オーディオ、セマンティックマップなど）の統合や、生成プロセス中のリアルタイム調整機能の実装も、将来的な拡張として考えられます。LongVieの基本的なフレームワークは、これらの拡張に対して柔軟に対応できる設計となっており、制御可能な動画生成分野の発展に大きく貢献することが期待されます。