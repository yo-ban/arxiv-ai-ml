# CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward

## 基本情報
- arXiv ID: 2508.03686 (https://arxiv.org/abs/2508.03686)
- 著者: 
  - Shudong Liu (Shanghai AI Lab/University of Macau)
  - Hongwei Liu、Junnan Liu、Linchen Xiao、Songyang Gao
  - Chengqi Lyu、Yuzhe Gu、Wenwei Zhang (Shanghai AI Lab)
  - Derek F. Wong (University of Macau)
  - Songyang Zhang、Kai Chen (Shanghai AI Lab)
- 所属: Shanghai AI Laboratory、NLP²CT Lab (University of Macau)
- 投稿日: 2025年08月07日
- カテゴリ: cs.CL, cs.AI

## 簡単に説明すると
CompassVerifierは、大規模言語モデルの評価と強化学習における報酬モデルとして機能する、正確でロバストな軽量検証器です。従来の手法では正規表現マッチングや汎用LLMを使用していましたが、反復的なカスタマイズが必要でした。本研究では多領域（数学、知識、推論）をカバーする統一的な検証器を開発しました。この検証器は選択式、数式、長文推論などの多様な回答タイプを処理できます。複数のモデル出力から収集し、人間による分析でメタエラーパターンを含むVerifierBenchベンチマークも導入しています。
GitHubで公開されています。
URL: https://github.com/open-compass/CompassVerifier

## 1. 研究概要
### 1.1 背景と動機
答案検証は大規模言語モデルの評価と訓練において重要な役割を果たしており、特に検証可能な答えを持つ客観的な問題において重要です。評価レベルでは、モデル間のパフォーマンスの違いを正確に測定でき、訓練レベルでは、自己改善のための品質チェックとして機能します。大規模推論モデル（LRM）と強化学習（RL）の急速な発展により、答案検証はルールベースの報酬を構築する重要なコンポーネントとなり、モデルの最適化と反復を導くフィードバック信号を提供しています。

既存の答案検証手法は大きく2つのタイプに分類できます。第1のタイプは正規化された文字列マッチングに依存します。「The answer is」に続く内容を抽出して参照答案と比較したり、数学タスクではmath-verifyのようなツールを使用して数式の等価性をチェックします。

第2のタイプは汎用LLMを使用して一貫性を判断します。候補答案と参照答案の整合性を評価するよう特定のプロンプトを設計します。

しかし、両方のアプローチには重大な制限があります。前者は異なるタスクでマッチングルールの反復的なカスタマイズが必要で、抽出エラーによる検証失敗が発生しやすいです。後者は多様なタスク、ドメイン、答案タイプで頻繁なプロンプト調整が必要で、モデルの幻覚による誤判断のリスクもあります。

### 1.2 主要な貢献
本研究の貢献は以下の3つです。
- 検証能力の詳細な評価のために綿密に設計された新しく挑戦的なベンチマークであるVerifierBenchの提案
- 提案した3つの技術により強化された、多様なドメインとタスクで高いパフォーマンスを達成する、ロバストで計算効率の良い検証モデルシリーズCompassVerifierの開発
- LLMベースの検証における一般的な失敗モード（特徴的な幻覚現象とエラー伝播を含む）の体系的な分析を通じた、将来の検証システムの設計とロバスト性を向上させるための実用的な洞察の導出

## 2. 提案手法
### 2.1 手法の概要
CompassVerifierは効率的で高性能、かつロバストな答案検証を提供するように設計されています。システムはVerifierBenchからフィルタリングされた（質問、参照答案、モデル応答）のトリプルを活用します。訓練監督としてのゴールデン判断も使用します。

性能を向上させるための3つの主要技術を提案しています。複雑な数式拡張は数式の変形の検証を強化します。エラー駆動の敵対的拡張は失敗に対する耐性を強化します。汎用性拡張はクロスドメインとクロスプロンプトの適用性を保証します。

### 2.2 技術的詳細
エラー駆動の敵対的拡張では、3段階のアプローチを採用しています。手順は以下の通りです。

1. ドメインエキスパートが5,000個の注釈付きサンプルを手動で検証する。タスク制約のLLMによる誤解、質問内の重要情報の誤解釈、異なる判定モデル間の異なるペナルティ閾値などの失敗理由を文書化する。

2. これらの理由に密度ベースのクラスタリングを適用する。20以上の高影響エラーカテゴリを明らかにし、特に視点取得とフォーマット遵守における脆弱性を明らかにする。

3. 各エラークラスタに対して構造化テンプレートを開発する。質問特性（ドメイン固有の要件、コンテンツ/フォーマット制約）と応答エラーパターン（失敗タイプ、局所化、重要度）をエンコードする。

複雑な数式拡張戦略では、各問題インスタンスに対して複数の記法変形答案を体系的に生成します。参照答案を正準表現に変換し、DeepSeek-v3を活用して1〜3個の代替定式化を生成します。

これには記号的な再配置（分母の有理化、代数的恒等式の適用など）が含まれます。また、精度を保持する浮動小数点の展開、等価な整数または分数表現も含まれます。

生成されたすべての変形は記号代数エンジンを使用して等価性が自動的にチェックされます。サブセットは主題専門家によって正確性と表現の自然さが手動で確認されます。

汎用性の拡張戦略では、公開データセット（TheoremQA、GPQA、GAOKAOBenchなど）と実世界のシナリオから多様なプロンプトを収集し、20以上のタスクタイプをカバーします。各プロンプトタイプに対して、質問スタイル、コンテキスト長、言語レジスター、指示の粒度を変えて複数のバリアントを設計します。訓練中には、プロンプトのランダムサンプリング、動的ミキシング、プロンプト不変メカニズムを導入して、過学習を防ぎ、異なるプロンプト定式化間で一貫した判断を促進します。

### 2.3 新規性
既存手法との主な違いは以下の点です。
- ルールベースのマッチングや汎用LLMの使用に依存せず、専用の検証モデルを訓練することで、高い精度とロバスト性を実現
- 多段階の検証パイプラインと人間のアノテーションを組み合わせて、高品質なベンチマークを構築
- メタエラーパターンの分析に基づく敵対的データ拡張により、一般的な失敗モードに対する耐性を向上

## 3. 実験結果
### 3.1 実験設定
VerifierBenchで3Bから32Bのパラメータまでの様々なモデルスケールのCompassVerifierを包括的に評価しました。ベースラインモデルには、Qwen2.5、Qwen3、DeepSeek-V3、GPT-4oなどの汎用LLMが含まれます。

また、最近提案された2つの専門的な検証モデル（xVerifyとTencent-Qwen2.5-7B-Instruct-RLVR）も含まれます。モデルに与えられた応答の最終判断を直接生成させ、メトリクスとしてF1と精度を報告します。

評価はVerifierBenchのドメインカテゴリ（知識、数学、科学、推論）と答案タイプ（選択式、数値、短文、数式、複数サブ問題、シーケンス、ブール値）の両方の観点から行いました。

### 3.2 主要な結果
CompassVerifierはすべてのVerifierBenchカテゴリで新しい高水準のパフォーマンスを確立しました。32B構成で84.1〜95.1%の精度と80.8〜94.8%のF1スコアを達成しました。3つの発見が明らかになりました。

検証能力はスケールの増加とともに漸進的な改善を示しました。パラメータが3Bから32Bにスケールアップするにつれて、精度が85.0%から90.8%へ、F1スコアが80.4%から87.7%へ向上しました。検証専用アーキテクチャは大幅な利益をもたらし、CompassVerifier-7Bは同様のサイズの元のQwen2.5-7B-Instructを絶対F1スコアで41.3%上回りました。進歩にもかかわらず、数学的検証は依然として困難（最高F1スコア80.8%対知識の94.8%）であり、段階的な論理的検証における持続的なギャップを強調しています。

答案タイプの観点から見ると、CompassVerifier-7Bはすべてのカテゴリで一貫した改善を達成しました。複数選択問題は最も簡単なカテゴリとして現れ、ほとんどのモデルが強いパフォーマンスを達成しました。

しかし、ベースラインモデルは数式ベースの答案、複数サブ質問、シーケンシャル答案の処理において顕著な欠陥を示しました。特にシーケンシャル答案では40 F1スコアを超えるものはありませんでした。

### 3.3 既存手法との比較
CompassVerifierの最小の3Bバリアントは、GPT-4.1を絶寞F1スコアで10.6%上回りました。これにより、パラメータ効率を実証しました。

VerifyBenchの最近の並行研究であるハードサブセットでのテストでも、CompassVerifierは優れた結果を示しました。同様のサイズの汎用LLM、専門的な検証モデル、さらにはDeepSeek-V3よりも優れたパフォーマンスを示しました。

汎用性拡張により、VerifyBenchのプロンプト（より深い分布外設定）の下でも、CompassVerifierはロバストなパフォーマンス（スコア>86）を維持しました。

一方、xVerifyとTencent-Qwen2.5-7B-RLVRは指示への対応で大きく失敗しました。

## 4. 実用性評価
### 4.1 実装の容易性
CompassVerifierは軽量でありながら高性能な検証を提供するように設計されています。1.7Bから32Bまでの複数のモデルサイズが利用可能で、計算リソースと精度要件のトレードオフに基づいて選択できます。モデルとベンチマークはGitHubで公開されており、再現性と将来の研究を促進しています。

### 4.2 計算効率
CompassVerifierはバッチ処理に対応し、複数のサンプルを同時に処理することでGPUメモリの活用効率を高めています。最小の1.7Bモデルでも強力なベースラインを上回るパフォーマンスを示しています。

これにより、リソース制約のある環境でも高品質な検証が可能になります。また、バッチ処理をサポートしており、大規模な評価タスクでの使用が可能です。

### 4.3 応用可能性
CompassVerifierはRL訓練における報酬モデルとしての有効性も実証されています。GRPOアルゴリズムを使用したRL訓練において、CompassVerifierを報酬モデルとして使用したモデルは、ベースモデルを上回りました。また、ルールベースの検証器（Math-Verify）で訓練されたモデル、汎用LLMまたは代替検証器を報酬モデルとして使用したモデルも超えました。これは、CompassVerifierがRL訓練で生成されたロールアウト軌跡により正確な評価を提供し、優れた報酬モデルとしての可能性を強調しています。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、大規模言語モデルの評価と強化学習における重要な課題である答案検証に対して、包括的なソリューションを提供しています。VerifierBenchは、既存の検証手法の限界を明らかにし、モデルの検証能力を厳密に評価するための挑戦的なベンチマークを提供します。CompassVerifierは、多様なドメインと答案タイプに対応できる統一的な検証モデルとして、実用的な価値が高いです。

特に注目すべきは、人間の分析とケーススタディを組み込んだ包括的なフレームワークとしてのVerifierBenchの設計です。30以上のメタエラーパターンの特定と分類は、LLMベースの答案検証における間違いと幻覚の根本的な原因を理解する上で貴重な洞察を提供しています。

### 5.2 今後の展望
将来の研究方向として、プロセス検証への拡張が考えられます。現在の研究は結果検証に焦点を当てていますが、中間ステップの推論エラーを検出するプロセス検証も重要です。また、より複雑な答案タイプ（証明ベースの質問、オープンエンドの問題など）への対応も今後の課題です。長期的には、CompassVerifierのような専門的な検証モデルが、より信頼性の高いAIシステムの開発において重要な役割を果たすことが期待されます。特に、強化学習を通じたモデルの自己改善において、正確な報酬信号を提供することで、学習の安定性と収束速度の向上を可能にする可能性があります。