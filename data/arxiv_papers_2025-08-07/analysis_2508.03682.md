# Self-Questioning Language Models

## 基本情報
- arXiv ID: 2508.03682 (https://arxiv.org/abs/2508.03682)
- 著者: Antiquus S. Hippocampus, Natalia Cerebro, Amelie P. Amygdale (Cranberry-Lemon University); Ji Q. Ren, Yevgeny LeNet (University of the Witwatersrand); Coauthor
- 所属: Department of Computer Science (Cranberry-Lemon University), Department of Computational Neuroscience (University of the Witwatersrand)
- 投稿日: 2025年08月07日
- カテゴリ: cs.CL, cs.AI

## 簡単に説明すると
Self-Questioning Language Models (SQLM)は、大規模言語モデルが外部データなしに自己改善する手法です。単一のプロンプト（たとえば「代数の文章問題」）を与えるだけで、モデルは自分で問題を生成し、それを解くことで推論能力を向上させます。非対称な自己対戦（asymmetric self-play）フレームワークを使用し、問題生成者（proposer）と解答者（solver）の2つの役割を強化学習で訓練します。実装にはverl（https://github.com/sheng2024hybridflow）を使用しています。プロジェクトのウェブサイト（self-questioning.github.io）で詳細情報が公開されています。

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデルのポストトレーニングは、依然として手作業でキュレートされたデータセットに大きく依存しており、相当なエンジニアリング努力と人間の監督を必要としています。研究者たちは正解がない場合の強化学習のために、モデルの内部信頼度や多数決などのプロキシを使用する教師なし報酬関数を開発してきました。しかし、これらの手法も適切に形成された入力プロンプトや質問の存在を前提としており、ボトルネックはラベル付きの答えのキュレーションから高品質な質問のキュレーションへと移行しただけです。これは依然として労働集約的で、大規模な自動化が容易ではありません。

興味深いことに、事前訓練済み言語モデル自体が、この課題に対処するための大きく未開拓なリソースを表しています。人間と同様に、これらのモデルはトレーニングデータの受動的な受信者としてだけでなく、能動的な生成者としても見ることができます。たとえば、刑務所のような限られた環境に閉じ込められた人間は、自分で質問を生成し答えることで自己反省的で認知的に豊かな演習に従事できます。何兆トークンものテキストで事前訓練された大規模言語モデルも同様の能力を持つ可能性があります。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです。
- 外部データを一切使用せずに言語モデルが自己改善できるSQLMフレームワークの提案
- 問題生成者と解答者の非対称な自己対戦による効果的な学習メカニズムの実現
- 3つのベンチマーク（3桁の掛け算、代数問題、プログラミング問題）での有効性の実証

## 2. 提案手法
### 2.1 手法の概要
SQLMは非対称な自己対戦フレームワークで、事前訓練済み言語モデルが高レベルドメイン（たとえば「代数の文章問題」）でプロンプトされ、自分自身の問題を生成して解くことで改善を学習します。モデルは2つの役割を果たします。新しい問題を作成する提案者（proposer）と、それらを解こうとする解答者（solver）です。両方の役割は強化学習を介して訓練されます。

生成器・検証器ギャップが小さい場合（算術など、解の生成と検証が同様に難しい場合）、正確性のプロキシとして複数のソルバー出力に対する多数決投票を使用します。ギャップが大きい場合（コード生成など、単体テストによる検証が正しい解を書くよりも簡単な場合）、テスト結果のような外部検証信号に依存します。

### 2.2 技術的詳細
提案者ポリシー$\pi_{P_t}(x)$と解答者ポリシー$\pi_S(y_{\text{pred}} \mid x)$は、それぞれの期待報酬を最大化するために強化学習を介して訓練されます。

生成器・検証器ギャップが小さいドメインでは、各問題に対してモデルからN個の生成をサンプリングし、多数派の答えを正解のプロキシとして使用します。解答者の報酬は、生成が多数派の答えと一致する場合は1、そうでない場合は0です。提案者の報酬は、問題が「妥当」である場合（すべてのN個の生成が一致する場合（簡単すぎる）または一致しない場合（難しすぎる）は0、それ以外は1）に基づいています。

生成器・検証器ギャップが大きいドメインでは、提案者が生成した単体テストを使用して解答者の出力を検証します。解答者の報酬は通過したテストケースの割合で、提案者は解答者がいくつかのテストケースを通過したが、すべてではない場合に報酬1を受け取ります。

### 2.3 新規性
既存手法との主な違いは以下の点です。
- 正解だけでなく、人間が作成した質問も必要としない徹底した自己教師あり学習
- 非対称な自己対戦による適応的な難易度調整メカニズム
- 生成器・検証器ギャップに応じた柔軟な報酬設計

## 3. 実験結果
### 3.1 実験設定
3つのタスクで評価しました。
1. 算術：3桁の掛け算問題（TinyZeroのセットアップに従った4096問題のテストセット）
2. 代数：線形方程式を含む代数文章問題（OMEGAベンチマークから100問題）
3. コーディング：LeetCodeの簡単な問題に類似したプログラミング問題（Codeforces/Eurus-2データセットから123例）

すべての実験でQwen2.5-3B-InstructまたはQwen2.5-Coder-3B-Instructを使用しました。

### 3.2 主要な結果
外部データを一切使用せずに、Qwen2.5-3B-Instructの精度を算術において14%、代数において16%向上させました。Qwen2.5-Coder-3B-Instructはコーディングにおいて7%向上しました。具体的な結果は以下の通りです。

- 算術：0.791 → 0.948 ± 0.009
- 代数：0.440 → 0.600 ± 0.010
- コーディング：0.320 → 0.391 ± 0.019

また、フォーマット報酬ベースライン（正しいフォーマットの場合のみ報酬を与える）との比較でも、提案手法が明確に優れていることが示され、これらが真の推論能力の向上であることが確認されました。

### 3.3 既存手法との比較
本手法は、既存の教師なし報酬手法と比較して、人間が作成した質問さえ必要としない点で大きく異なります。また、自己対戦プロセスを通じて、モデルが解答者の能力に応じて問題の分布を継続的に洗練させるため、単純なプロンプトから「多様な」または「難しい」問題を生成するよりも効果的です。

## 4. 実用性評価
### 4.1 実装の容易性
実装はverlフレームワークの上に構築されており、比較的シンプルです。主要なハイパーパラメータとして、提案者の更新頻度があり、実験では5ステップごとの更新が全体的に良好な結果を示しました。ただし、特定のトピックに対するプロンプトの構築と洗練において、手作業を十分に排除できていません。

### 4.2 計算効率
バッチサイズ64、ポリシーミニバッチサイズ32で訓練し、100訓練ステップで評価しています。学習率は$1 \times 10^{-6}$と比較的小さく設定されており、安定した学習が可能です。提案者の更新頻度を適切に設定することで、訓練の安定性と効率のバランスを取ることができます。

### 4.3 応用可能性
この手法は、正解ラベルやキュレートされたデータセットが入手困難または高価な領域で特に有用です。モデルが継続的に新しい問題を生成し解決することで、下流タスクでの性能向上が期待できます。ただし、生成される問題が妥当で安全で関連性があることを保証するセーフガードが現在存在しません。そのため、大規模な展開には追加の工夫が必要です。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、キュレートされたトレーニングデータを必要としない、より自律的な言語モデルの改良への一歩を示しています。非対称な自己対戦セットアップ内で提案者と解答者の二重の役割でモデルをキャストすることで、推論スキルを有意に改善できることを実証しました。言語モデルは自己生成コンテンツとの相互作用のみを通じてこれを達成します。これは、ポストトレーニングが人間がキュレートしたデータセットに依存しなければならないという従来のパラダイムに挑戦し、徹底的に自己教師ありモデル学習への道を開きます。

特に注目すべきは、モデルが抽象的な指示（「多様に生成せよ」や「難しく生成せよ」など）をプロンプトだけから確実に操作化することに苦労する一方で、非対称な自己対戦プロセスによって提供される定量的で適応的な難易度の概念から恩恵を受けるという発見です。

### 5.2 今後の展望
将来の研究方向として、プロンプト設計の自動化が挙げられます。また、生成される質問の妥当性・安全性・関連性を確保するメカニズムの開発も重要です。既存のラベル付きデータセットを半教師あり設定で組み込むことによる学習プロセスの正則化も考えられます。この手法を他のドメインや、より複雑なタスクに拡張することも興味深い方向性です。長期的には、大規模言語モデルが受動的な学習者以上の存在となります。自身のトレーニングループにおける能動的なエージェントとなることで、将来のAIシステムの自律性に大きな影響を与える可能性があります。