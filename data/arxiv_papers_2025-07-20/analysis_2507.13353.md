# VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding

## 基本情報
- arXiv ID: 2507.13353v1 (https://arxiv.org/abs/2507.13353)
- 著者: Shihao Wang、Guo Chen、De-An Huang、Zhiqi Li、Minghan Li、Guilin Liu、Jose M. Alvarez、Lei Zhang、Zhiding Yu
- 所属: The Hong Kong Polytechnic University、Nanjing University、NVIDIA、Harvard University
- 投稿日: 2025年07月18日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると
この論文は、ビデオ大規模言語モデル（Video-LLMs）のフレーム選択の問題を解決する新しい手法「VideoITG」を提案しています。長い動画からユーザーの質問に答えるために重要なフレームを自動選択する技術です。

従来の手法では均一にフレームを選択していたため、重要な情報を見逃してしまうことが問題でした。VideoITGは、ユーザーの指示に基づいて動画内の重要な時間帯を特定し、その中から最適なフレームを選択します。例えば、「歯を磨いた後に香水をつけたか」という質問に対して、両方のアクションが含まれるフレームを選択できるようになります。

研究チームは「VidThinker」という自動アノテーションパイプラインを開発しました。これにより4万本の動画から50万件以上のアノテーションを含む「VideoITG-40K」データセットを構築しました。動画理解タスクの性能が向上し、プロジェクトウェブサイトでデモや詳細情報が公開されています。

## 1. 研究概要
### 1.1 背景と動機
ビデオ大規模言語モデル（Video-LLMs）の急速な発展により、動画のキャプション生成、視覚的な質問応答、エージェントアプリケーションなどの複雑なタスクが可能になりました。しかし、長時間の動画を扱う際には、メモリと計算量の大きな制約があります。

多くの既存手法は、この問題に対して均一なフレームサンプリングを採用しています。これはシンプルですが、正確な動画理解に必要なキーフレームを頻繁に見逃してしまい、パフォーマンスの低下を招きます。研究者たちは様々な戦略を提案してきました。たとえば、フレーム間の冗長性を削減する方法、シーケンス長を拡張してより多くのトークンを取り込む方法、質問に基づいて特徴を抽出する方法などがあります。

しかし、これらのアプローチにも限界があります。特に、長時間動画と短時間動画の間のパフォーマンスギャップは、長時間動画コンテンツのトレーニングデータが限られていることが原因で、依然として存在しています。

人間が長時間動画を分析するとき、自然に段階的なアプローチを採用します。まず全体的なコンテキストを把握し、質問に関連する手がかりを特定し、その後特定のセグメントに焦点を当てます。このプロセスからインスピレーションを得て、本研究ではVideoITGを提案しています。

### 1.2 主要な貢献
本論文の主要な貢献は以下の3つです。

- VideoITG-40Kデータセット: VideoITGのタスクを定義し、「VidThinker」という自動データアノテーションパイプラインを開発しました。これにより、4万本の動画と500Kの指示依存アノテーションを含む大規模データセットを構築しました。
- VideoITGモデル: VideoITG-40Kデータセットに基づいた包括的な分析から得られた知見を活用し、異なるアテンションとデコーディング戦略を持つVideoITGモデルファミリーを導入しました。
- 一貫した性能向上: さまざまなマルチモーダル動画理解ベンチマークで一貫した性能向上を達成しました。InternVL2.5-8BモデルでVideoITGを統合することで、CG-Benchで9.0%、MLVUで8.6%、Video-MMEで4.0%、LongVideoBenchで3.6%の改善を実現しました。

## 2. 提案手法
### 2.1 手法の概要
VideoITGは、ユーザーの指示に基づいた時間的グラウンディングをビデオに適用する新しいアプローチです。一般的な時間的ビデオグラウンディングが単一の時間的手がかりと記述的言語クエリに基づいてビデオ内のイベントローカライゼーションを強調するのに対し、VideoITGは特定のタスク要件に合わせてフレーム選択戦略をカスタマイズします。

本手法の核心は、人間のアノテーションプロセスを明示的に模倣する「VidThinker」パイプラインです。このパイプラインは3つの段階から構成されています。

1. 指示に基づいたクリップキャプション生成: まず、指示に条件付けられた詳細なクリップレベルのキャプションを生成します。
2. 指示ガイド推論による関連ビデオセグメントの取得: 次に、指示ガイド推論を通じて関連するビデオセグメントを取得する。
3. 細かいフレーム選択: 最後に、最も情報量の多い視覚的証拠を特定するための細かいフレーム選択をする。

### 2.2 技術的詳細
指示を以下の4つのタイプに分類し、それぞれが動画QAにおける異なる推論要件を反映している。

- Semantic-only: オブジェクトやシーンなどの視覚的コンテンツに焦点を当て、外観やコンテキストベースの質問をサポートする。
- Motion-only: 動きや速度などの動的手がかりをターゲットにし、モーション中心の質問に答える。
- Semantic & Motion: 視覚的および時間的推論を統合し、外観と動きを両方含む質問を扱う。
- Non-clues: 明確な意味的または動きの焦点がない一般的な指示で、全体的な理解のために視覚的多様性を最大化することを目指す。

モデル設計に関しては、3つのバリアントを提案している。

1. テキスト生成ベース分類: 次トークン予測タスクとして定式化し、テキストトークンを出力する。
2. アンカーベース分類: 識別パラダイムを採用し、各ビデオフレームに対応する視覚トークンを分類する。アンカートークンを導入して複数の時間的手がかりを扱えるようにしている。
3. プーリングベース分類: 因果アテンションマスクを除去し、視覚トークンとテキスト指示トークンがシーケンス全体で完全なアテンションを通じて相互作用できるようにする。

### 2.3 新規性
VideoITGの新規性は以下の点にあります。

1. ユーザー指示主導のアプローチ: 従来のフレーム選択フレームワークと異なり、VideoITGはさまざまなタスクに対して複数の時間的手がかりを効果的に扱うことができる。

2. 統合的な解決アプローチ: 時間的関係を理解するために複数のクリップから時間的手がかりをローカライズする。速度変化を検出するためにイベントローカライゼーションと均一フレームサンプリングを併用する。コンテンツキャプションや存在判断のために多様なサンプリング方法を実施する。

3. 自動アノテーションパイプライン: VidThinkerは人間の推論プロセスにインスパイアされた粗から細への戦略を採用し、GPT-4oを使用して詳細なクリップ説明を生成し、その後「Needle-In-A-Haystack」アプローチを使用して指示ガイドクリップ取得する。

## 3. 実験結果
### 3.1 実験設定
実験では、LLaVA-Videoのトレーニングアプローチに従い、事前訓練されたモデルをVideoITGモデルの初期化として使用しました。SigLIPをビジョンエンコーダとして、Qwen2を言語モデルとして使用しています。

トレーニングは次のように進めました。
- 最初に、バッチサイズ256、学習率$1 \times 10^{-3}$で画像キャプションデータセットでMLPプロジェクタをトレーニング
- 次に、LLaVA-OV-SIとLLaVA-Videoデータセットですべてのモデルパラメータをファインチューニング
- ビデオフレームサンプリングレートは64に設定し、LLMの最大シーケンス長は16Kに設定
- 最後に、VideoITG-40KデータセットでVideoITGモデルをトレーニングし、ビデオサンプリングレートを1fpsに調整

評価は、LongVideoBench、MLVU、VideoMMEの3つの長時間動画データセットで行いました。

### 3.2 主要な結果
実験結果は、VideoITGのフレーム選択戦略が、均一サンプリングを大幅に上回ることを示しました。これは32フレームおよび64フレームの両方の設定で確認されました。

具体的な成果として：
- InternVL2.5-26Bモデルでは、InternVL2.5-8Bと比較してより高いベースラインを持つにもかかわらず、依然として大幅な性能向上を示しました。LongVideoBenchで7.4%、MLVUで7.6%、CG-Benchで9.0%の向上です。
- 特に注目すべきことに、InternVL2.5-8BにVideoITGを統合した場合、平均性能が64.3%に達し、InternVL2.5-26Bのベースライン（61.6%）を上回りました。これは、インテリジェントなフレーム選択を使用した小さなモデルが、標準的な均一サンプリングを使用するより大きなモデルを凌駕できることを示しています。
- 長時間動画ベンチマークでの性能向上が特に顕著で、VideoITGを搭載したInternVL2.5-8BモデルはCG-Benchで46.7%を達成し、InternVL2.5-26Bのベースライン40.6%を超えました。

### 3.3 既存手法との比較
異なるフレーム選択手法を比較した結果、以下が明らかになりました。

- 均一サンプリング法（ベースライン）は平均スコア62.7を達成
- SigLIPはわずかにスコアを向上させ64.0に。しかし、SigLIPは短い記述テキストしか扱えず、複雑なユーザー指示を理解する能力が乏しいため、性能が劣ります。
- InternVL2.5-8Bを選択と応答の両方のLMMとして使用した場合、さらに改善が見られ、平均スコア65.2を達成。このアプローチはスタンドアロンVLMを使用して各フレームと指定された質問との関連性を評価します。
- VideoITG-7Bモデルは、平均スコア68.1で他のすべての手法を上回りました。提案するVideoITGフレームワークは、データアノテーションとモデル設計の両方が指示追従と時間モデリングを認識しているため、優れています。

## 4. 実用性評価
### 4.1 実装の容易性
VideoITGはプラグアンドプレイ方式で設計されており、既存のVideo-LLMsに容易に統合できます。提案された3つのモデルバリアントは、異なる要件や制約に対応できる柔軟性を提供します。

### 4.2 計算効率
全体的に、VideoITGはインテリジェントなフレーム選択により計算効率を向上させます。重要なフレームに焦点を当てることで、全フレームを処理することなく、より少ない計算リソースでより高い性能を達成できます。

テキスト生成パラダイムに基づくバリアントAは推論中の効率が悪く、トレーニング中の教師強制戦略によるショートカット学習の問題があります。一方、識別パラダイム（バリアントBとC）はより効率的です。

### 4.3 応用可能性
VideoITGは幅広い動画理解タスクに適用可能です。実験で示されたように、長時間動画理解、マルチモーダル動画質問応答、動画キャプション生成など、様々なベンチマークで一貫した性能向上を達成しています。

ビジュアライゼーションの例からも示されるように、VideoITGは複数の時間的手がかりを必要とする問題や、動画の最後のアクションを特定するような迅速な連続動作の把握が必要な問題で特に優れた性能を発揮します。

## 5. まとめと所感
### 5.1 論文の意義
本論文は、Video-LLMsにおけるフレーム選択の重要性を浮き彫りにし、この問題に対する体系的な解決策を提示した点で重要です。VideoITGは、ユーザーの指示に基づいたフレーム選択という新しいパラダイムを導入し、動画理解タスクの性能を大幅に向上させました。

特に注目すべきは、スマートなフレーム選択が単純なモデルサイズの拡大よりも効果的であることを示した点です。VideoITGを搭載した8Bモデルが26Bモデルのベースラインを上回るという結果は、計算リソースの効率的な使用とアルゴリズムの革新の重要性を浮き彫りにしています。

また、VidThinkerパイプラインとVideoITG-40Kデータセットの作成は、今後の研究に大きな貢献をもたらすでしょう。これらのリソースは、動画理解分野の更なる発展を促進する基盤となると期待されます。

### 5.2 今後の展望
本論文で提示された制限にもあるように、現在のフレームワークは推論時に2つの別々のモジュールから構成されています。フレーム選択のためのVideoITGと質問応答のためのスタンドアロンVideo-LLMです。トレーニング中にこれら2つのモジュール間の勾配最適化がないため、最適でない結果が得られる可能性があります。

将来の研究の方向性としては、以下が考えられます。
- 強化学習技術を探索してこれら2つのモジュールを橋渡しすること
- フレーム選択とマルチターン会話間の閉ループ最適化システムの構築
- 以前の会話コンテキストに基づいてフレーム選択戦略を動的に調整できるシステムの開発
- より長い動画やより複雑な時間的関係を扱うためのモデルの拡張

VideoITGフレームワークは有望な出発点として機能し、指示主導の動画理解を進展させるための基礎を提供しています。
