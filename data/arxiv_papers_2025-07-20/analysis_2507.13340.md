# Latent Policy Steering with Embodiment-Agnostic Pretrained World Models

## 基本情報
- arXiv ID: 2507.13340v1 (https://arxiv.org/abs/2507.13340)
- 著者: Yiqi Wang、Mrinal Verghese、Jeff Schneider
- 所属: Carnegie Mellon University
- 投稿日: 2025年07月18日
- カテゴリ: cs.RO, cs.AI, cs.LG

## 簡単に説明すると
この論文は、ロボットが新しいタスクを少ないデータで学習できるようにする技術を提案している。複数の異なるロボットや人間のデータを活用してWorld Model（世界モデル）を事前学習する。

重要な工夫は「オプティックフロー」を使うことです。オプティックフローは画像間の動きを表すもので、異なるロボットでも同じような動作パターンを捉えることができる。例えば、物を掴む動作は異なるロボットでも視覚的に似ている。

学習したWorld Modelを使って「Latent Policy Steering (LPS)」という手法でロボットの行動を改善する。30個の実演データで50%以上、50個のデータで20%以上の性能向上を実現した。コードは公開予定です。

## 1. 研究概要
### 1.1 背景と動機
模倣学習によるロボット制御は広く採用されていますが、高い成功率を達成するには多くの実演データが必要です。データ収集は時間がかかり、特定のロボットや環境に依存するため、異なる環境では再度収集が必要になります。

既存の大規模データセットを活用した汎用ロボットポリシーの研究が進んでいますが、新しいタスクへの汎化性能が十分ではありません。大規模モデルをファインチューニングする場合も相当量のデータが必要です。たとえば、π0という手法を使って新タスクで高い成功率を達成するには、5-10時間の追加データが必要です。

本研究では、複数のロボットや人間のデータを活用できる「身体性に依存しない」World Modelの事前学習を提案します。World Modelはポリシーと比べて身体固有の情報への依存が少なく、複数の身体性からの学習に適しています。

### 1.2 主要な貢献
本論文の主要な貢献は以下の3つです。

- オプティックフローを身体性に非依存な行動表現として使用し、複数のロボットや人間のデータからWorld Modelを事前学習する手法の提案
- Latent Policy Steering (LPS)という推論時の手法を開発し、BCデータセット内の全ての状態を目標状態とみなす価値関数でポリシーを改善
- シミュレーションと実世界実験で提案手法の有効性を実証し、少量データでの大幅な性能向上を確認

## 2. 提案手法
### 2.1 手法の概要
提案手法は2つの核心的な要素から構成されます。

1. Flow-as-Action: オプティックフローを身体性に依存しない行動表現として使用し、複数のロボットや人間のデータからWorld Modelを事前学習する。オプティックフローは視覚的な動きのパターンを捉え、異なるロボットでも同様の動作（物を掴む等）は似た視覚的パターンを示す。

2. Latent Policy Steering (LPS): World Modelの潜在空間で最適な行動系列を探索し、行動クローニングポリシーの出力を改善する。BCデータセット内の全ての状態は成功した実演からのものなので「目標状態」とみなし、これらに近い状態を訪問するように価値関数を学習する。

### 2.2 技術的詳細
World Modelの事前学習では、以下の手順を実施する。
1. 各エピソードに対してオプティックフローを事前計算（GMFlowを使用）
2. CNNとMLPでオプティックフローをエンコードし、ロボット行動と同じ次元（7次元等）に投影
3. 事前学習中はこのエンコーダをWorld Modelと共に最適化
4. ターゲット身体性へのファインチューニング時は、オプティックフローエンコーダを破棄し、生のロボット行動を使用

LPSアルゴリズム（Algorithm 1）では、以下の処理を行う。
1. ロボットデータセットから観測、行動、報酬をサンプリング
2. World Modelをロボット行動でファインチューニング
3. ポリシーから予測行動計画をサンプリング
4. データセットの行動と予測行動の両方でWorld Modelをロールアウト
5. 状態間のコサイン類似度を負の報酬に変換し、データセットからの逸脱をペナルティ化
6. Lambda-Returnで価値関数を最適化

推論時には、以下の手順で実行する。
- BCポリシーから大量の行動計画をサンプリング
- 各計画に対してWorld Modelで将来状態を予測し、価値を計算
- 割引係数を使った重み付け平均で計画レベルの価値を算出
- 最高価値の行動計画をオープンループで実行

### 2.3 新規性
既存手法との主な違いは以下の通りです。

1. 身体性に非依存な事前学習：HPTのような既存手法は身体固有の情報（固有感覚、行動）を含むため、新しい身体性への適応に多くのデータが必要である。本手法はオプティックフローを使うことで、この依存性を除去する。

2. 全状態を目標とする価値関数：従来の手法は最終目標画像との類似度を使うが、長期タスクでは効果が限定的である。本手法は「BCデータセット内の全ての状態は目標状態」という洞察に基づき、データ分布に近い状態を訪問するように誘導する。

3. ポリシー操舵の新しいアプローチ：単純な報酬ベースの価値関数ではなく、データセットからの逸脱を明示的にペナルティ化することで、分布シフトに対してロバストな価値関数を学習する。

## 3. 実験結果
### 3.1 実験設定
シミュレーション実験（Robomimic）では、以下の設定を用いた。
- タスク: Lift（ブロックを持ち上げる）、Can（缶をビンに入れる）、Square（ナットをペグに配置）、Transport（2腕長期タスク）
- データ量: 30、50、100実演
- 評価: 3シード平均、各シード150エピソード以上

実世界実験では、以下の設定を用いた。
- Frankaロボット、FastUMI Fingertips使用
- タスク: 大根を鍋に入れる、カップを重ねる、オーブンを開けて鍋を入れて閉じる
- データ量: 30、50実演
- 事前学習データ: Open X-Embodimentから2000実演、または人間が1時間物体で遊ぶ動画

### 3.2 主要な結果
十分なデータ（100実演）での結果は以下の通りです。
- 提案手法（WM-V(s)）が全てのタスクで一貫して性能向上
- 平均成功率: BC (63.7%) → WM-V(s) (68.5%)
- 特にCanタスクで大幅改善: 79.8% → 87.3%

少量データでの事前学習効果については、以下の結果が得られた。
- 30実演: 36.6% → 40.0% (相対的に9.3%向上)
- 50実演: 67.8% → 73.9% (相対的に9.0%向上)
- Liftタスクでは事前学習なしでは性能低下したが、事前学習により改善

実世界での実験結果は以下の通りです。
- 30実演: BC (43.3%) → WM-V(s)-human (68.3%) - 相対的に57.7%向上
- 50実演: BC (63.3%) → WM-V(s)-human (80.0%) - 相対的に26.4%向上
- 人間データでの事前学習がOpenXデータより良好な結果（同じ環境で収集されたため）

アブレーション実験により、以下の知見が得られた。
- ホライズン長: 4-16で効果的、24では性能低下（状態類似度の報酬がノイジーになるため）
- 報酬設計: データセットからの逸脱を捉える報酬が重要（除去すると性能大幅低下）
- モデルアーキテクチャ: フルアテンション > 因果アテンション > テキスト生成パラダイム

### 3.3 既存手法との比較
**ベースラインとの比較**では、以下のような結果が得られました。
- IQL: 分布シフトに弱く、効果的な操舵ができない
- WM-goal: 長期タスクでは最終目標画像が遠すぎて効果限定的
- HPT-large: 身体固有情報に依存し、少量データでは過学習（実世界で11.7-21.7%の成功率）

**提案手法の優位性**には以下のような点があります。
- 小規模モデル（8B）でインテリジェントなフレーム選択により、大規模モデル（26B）を上回る性能
- 複合エラーに対してロバスト（価値関数によりデータ分布に近い状態を維持）
- 身体性に依存しない事前学習により、新しい身体性への適応が容易

## 4. 実用性評価
### 4.1 実装の容易性
実装の容易性は中程度です。

**必要な要素**には以下のようなものがあります。
- オプティックフロー計算ツール（GMFlow等）
- DreamerV3ベースのWorld Model実装
- 拡散ポリシーなどのBCポリシー実装
- 価値関数の学習とLambda-Return実装

**利点**には以下のような点があります。
- オプティックフローは既存ツールで容易に計算可能
- World Modelのファインチューニングは標準的な手法
- コードは公開予定

**課題**には以下のような点があります。
- 複数のコンポーネントの統合が必要
- 価値関数の学習には工夫が必要（データ分布外の状態も含める）

### 4.2 計算効率
計算効率は妥当なレベルです。

**事前計算**については以下のような特徴があります。
- オプティックフローの事前計算が必要（データセット作成時に1回のみ）
- World Modelの事前学習は標準的な計算量

推論時の計算については以下の特徴があります。
- 大量の行動計画のサンプリングと評価が必要
- World Modelでのロールアウトは並列化可能
- リアルタイム制約のあるタスクでは課題となる可能性

**メモリ効率**については以下のような特徴があります。
- World ModelとBCポリシーの両方を保持する必要あり
- ただし、HPTのような大規模モデルと比較すると効率的

### 4.3 応用可能性
幅広い応用が期待できます。

**適用可能な場面**には以下のようなものがあります。
- 少量データでのロボット学習が必要な産業応用
- 複数のロボットを使用する研究環境
- 人間のデモンストレーションを活用したい場面
- 長期的な操作タスク

**制約事項**には以下のような点があります。
- 固定カメラからの視覚観測が前提（動くカメラでは追加の工夫が必要）
- オプティックフローが意味のある行動を表現できるタスクに限定
- リアルタイム性が強く要求されるタスクでは課題

**将来の拡張**については以下のような可能性があります。
- Epic-KitchenやEgo4Dなどの大規模データセットへの適用
- より一般的な身体性に非依存な表現の開発
- 動的カメラへの対応

## 5. まとめと所感
### 5.1 論文の意義
本論文は、ロボット学習における少量データ問題に対する実用的な解決策を提示しています。特に重要な貢献は以下の点です。

**理論的意義**には以下のような点があります。
- オプティックフローという既存の概念を身体性非依存の行動表現として活用する発想が斬新
- 「BCデータセット内の全状態は目標状態」という洞察が、価値関数設計の新しい視点を提供
- World Modelとポリシーの組み合わせ方に新しいアプローチを提示

**実用的意義**には以下のような点があります。
- 実世界実験での30実演による50%以上の性能向上という顕著な結果
- 人間データの活用により、高価なロボットデータ収集の負担を軽減
- 既存のマルチエンボディメントデータセットを有効活用できる枠組み

**学術的影響**には以下のような点があります。
- World Modelの事前学習に関する新しい研究方向を開拓
- 身体性の違いを超えた知識転移の可能性を実証
- 少量データ学習における価値関数の役割を再定義

### 5.2 今後の展望
本研究は多くの発展可能性を秘めています。

**技術的改善の方向性**には以下のような点があります。
- 動的カメラ環境でのオプティックフロー活用法の開発
- リアルタイム応用向けの、計算時間を短縮した推論アルゴリズムの開発
- 他の身体性に非依存な表現の探索（3Dフローなど）

**応用範囲の拡大**については以下のような可能性があります。
- Epic-KitchenやEgo4Dなどの大規模な人間行動データセットの活用
- マルチモーダル（視覚＋触覚など）への拡張
- より複雑な長期タスクへの適用

**理論的発展**については以下のような方向性があります。
- オプティックフローと実際の行動の対応関係の理論的解析
- 最適な事前学習データの選択基準の確立
- 価値関数の理論的保証の研究

**限界と課題**には以下のような点があります。
- 固定カメラの制約は実用上の制限となる可能性
- オプティックフローが有効でないタスク（精密な力制御など）への対処
- 計算コストとリアルタイム性のトレードオフ

本論文は、ロボット学習における実用的な問題に対して理論的に興味深いアプローチを提示しています。また、実践的にも有効な手法であり、今後の研究に大きな影響を与えると期待されます。
