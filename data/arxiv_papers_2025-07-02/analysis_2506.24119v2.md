# SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning

## 基本情報
- arXiv ID: 2506.24119v2 (https://arxiv.org/abs/2506.24119v2)
- 著者: Bo Liu*, Leon Guertler*, Simon Yu*, Zichen Liu*, 他9名
- 所属: National University of Singapore, Centre for Frontier AI Research (CFAR) A*STAR, Northeastern University, Sea AI Lab, 他2機関
- 投稿日: 2025年6月29日 (最終更新日)
- カテゴリ: cs.LG, cs.AI

## 簡単に説明すると
この論文は、言語モデルが自己対戦（セルフプレイ）を通じて推論能力を向上させる新しい手法「SPIRAL」を提案している。従来の手法は人間が作成した問題集や報酬設計に依存していた。一方、SPIRALは二人零和ゲームでの自己対戦だけで、数学的推論や一般的な推論タスクで大幅な性能向上を実現した。特筆すべきは、ゲームの訓練だけで数学的な内容を一切見ていないにもかかわらず、数学推論能力が8.6%向上したことである。これは、ゲームを通じて獲得した体系的分解、期待値計算、場合分け分析といった認知パターンが、他の領域に転移することを示している。

## 1. 研究概要
### 1.1 背景と動機
近年、OpenAI o1やDeepSeek-R1などの大規模言語モデルは、強化学習を用いることで推論能力の飛躍的な向上を実現している。しかし、これらのアプローチには根本的な拡張性の問題がある。それは、慎重に設計された報酬関数、ドメイン固有のデータセット、専門家による監督に依存していることである。新しい推論領域ごとに、専門家が評価指標を作成し、訓練問題を整理し、推論過程を検証する必要がある。この手動プロセスは、より一般的な知能を追求する上で持続不可能になりつつある。

自己対戦（セルフプレイ）は、この拡張性の危機に対する解決策を提供する。自己対戦では、モデルは自分自身のコピーと競争することで学習し、各対戦の結果が自動的なフィードバックを提供する。モデルが改善されると、対戦相手も同様に改善され、継続的な学習を促す一貫した挑戦レベルが維持される。このパラダイムは、TD-Gammonのバックギャモン、AlphaGoの囲碁、OpenAI FiveのDota 2など、多くの領域でAIに革命をもたらしてきた。しかし、言語モデルの推論能力向上への自己対戦の適用は、これまでほとんど探求されていなかった。

### 1.2 主要な貢献
SPIRALフレームワークは、以下の3つの主要な貢献を提供する。

- 完全オンライン、マルチターン、マルチエージェントRL フレームワーク: 分散アクター・ラーナーアーキテクチャを開発した。これにより、複数の二人零和言語ゲームでフルパラメータ更新によるオンライン自己対戦が可能になった。
- 役割条件付きアドバンテージ推定（RAE）: マルチエージェント設定用に特別に設計された分散削減アドバンテージ推定器を導入した。
- 転移の実証的発見: ゼロサムゲームでの自己対戦が、ドメイン固有の訓練データなしで学術的推論ベンチマークを改善することを実証した。

## 2. 提案手法
### 2.1 手法の概要
SPIRALは、二人零和言語ゲームの集合での自己対戦を通じて実装される。各ゲームは交互ターン制の二人零和マルコフゲームで、プレイヤーは同時に行動するのではなく交互に行動する。重要な点は、両プレイヤーが単一のポリシーπ_θを共有し、システムプロンプトによる役割条件付けで区別されることである。このアプローチにより、モデルは両方の視点から戦略を学習し、より堅牢な推論能力を開発できる。

フレームワークは、トークンレベルMDPではなくターンレベルMDPを採用している。各ターンで、言語モデルは状態s_tを観察し、以下の形式で完全な応答を生成する。
```
<think>ct</think><answer>at</answer>
```
ここで、ctは推論過程を外部化し、atは実行可能なアクションである。この構造により、モデルは明示的な推論トレースを生成しながら、同時に戦略的な決定を下すことができる。

### 2.2 技術的詳細
役割条件付きアドバンテージ推定（RAE）は、SPIRALの中核的な技術革新です。標準的な強化学習アルゴリズムは、マルチエージェント設定で高い分散に悩まされます。RAEは、各ゲームGと役割番号に対して別々のベースラインを維持することでこの問題に対処します。

ベースラインは指数移動平均で更新されます。
```
b_{G,p} ← αb_{G,p} + (1-α)R_p(τ)
```

アドバンテージは次のように計算されます。
```
A_{G,p}(τ) = R_p(τ) - b_{G,p}
```

この設計は、役割の非対称性（例：先手有利）から生じる分散に対処し、安定した学習を可能にします。実験では、RAEなしではモデルが「思考崩壊」を起こし、推論トレースを徐々に放棄してしまうことが示されました。

実装面では、SPIRALは分散アクター・ラーナーアーキテクチャを採用しています。並列アクターがvLLMを使用してゲームトラジェクトリをサンプリングします。中央集権的なラーナーがこれらのトラジェクトリを処理してポリシーを更新します。このアーキテクチャにより、オンラインでの学習が可能となり、モデルは絶えず進化する対戦相手（自分自身）に継続的に適応できます。

### 2.3 新規性
SPIRALの新規性は、以下の点にあります。

1. 自己対戦によるカリキュラムの自動生成: 固定された対戦相手や人間が設計したカリキュラムとは異なり、SPIRALは対戦相手が継続的に改善されることで、自然に難易度が上昇するカリキュラムを生成します。

2. ゲームから推論への転移メカニズム: 単純なゲームが、数学や一般的な推論タスクに転移可能な認知パターンを開発することを初めて実証しました。

3. マルチゲーム学習の相乗効果: 異なるゲームが補完的なスキルを開発し、マルチゲーム訓練がこれらの能力を相乗的に組み合わせることを示しました。

## 3. 実験結果
### 3.1 実験設定
実験では、Qwen3-4B-Baseモデルを基盤として使用し、3つのゲームで訓練しました。

- チックタックトー: 空間推論、パターン認識、敵対的計画を開発
- クーンポーカー: 確率的推論、不確実性下での意思決定、ブラフを開発
- シンプル交渉: 多制約最適化、心の理論、戦略的コミュニケーションを開発

訓練は400ステップ×128サンプル=51,200ゲーム遷移で行われました。学習率は1×10^-6、バッチサイズは128、EMA減衰率αは0.95で設定されました。8台のH100 GPUを使用し、実験あたり約25時間を要しました。

評価は、訓練に使用したゲームでの性能、未見のゲーム（Snake、Connect Four、Pig Diceなど）での性能で行われました。さらに、数学ベンチマーク（MATH500、AIME、OlympiadBench、Minerva Math）と一般推論ベンチマーク（GPQA、MMLU-Pro）でも評価しました。

### 3.2 主要な結果
推論ベンチマークへの転移（RQ1）について、クーンポーカーのみでの訓練が、数学的推論で平均8.6%（Minerva Mathで最大18.1%）、一般的推論で平均8.4%の改善をもたらしました。これは、25,000の専門家ゲームトラジェクトリでのSFTを上回る結果です。

転移可能な推論パターンの分析により、以下の3つのパターンが特定されました。
- 場合分け分析: 72%の転移率（体系的な列挙）
- 期待値計算: 28%の転移率（確率的な意思決定）
- パターン認識: 45%の転移率（増幅効果を示す）

自己対戦と固定対戦相手の比較（RQ2）について説明します。固定ランダム対戦相手で完全な訓練崩壊（「ターンの呪い」）が発生しました。固定モデル対戦相手では初期学習後に搾取プラトーに達しました。対照的に、自己対戦は訓練全体を通じて50-52%の勝率を維持し、継続的な改善を示しました。数学タスクでは、自己対戦が40%の性能を達成したのに対し、最良の固定対戦相手は35%でした。

**RQ3: ゲーム固有のスキル開発**
各ゲームが特化したスキルを開発することが確認されました。チックタックトー専門家は空間ゲームで56%の勝率、クーンポーカー専門家は確率的ゲームで91.7%、交渉専門家は戦略的ゲームで55.8%を達成しました。マルチゲーム訓練は相乗的な利点を示し、平均44.9%の性能を達成しました（専門家の27.1-34.4%に対して）。さらに、強力なモデル（DeepSeek-R1-Distill）でも2.0%の改善が見られました。

### 3.3 既存手法との比較
SPIRALは、以下の点で既存手法を上回りました：

1. **人間の監督なし**: 専門家が作成した問題集やドメイン固有の報酬設計を必要としません
2. **継続的な改善**: 固定対戦相手訓練とは異なり、自己対戦は継続的な挑戦を提供します
3. **転移可能性**: ゲーム訓練が学術的ベンチマークに転移することを初めて実証しました
4. **効率性**: 25,000の専門家トラジェクトリでのSFTよりも少ないデータで優れた結果を達成しました

## 4. 実用性評価
### 4.1 実装の容易性
SPIRALの実装は、複雑な報酬関数の設計や大規模なデータセットの収集を必要としないため、従来のRLVRアプローチよりも容易です。必要なのは、明確なルールを持つゲーム環境とTextArenaなどのゲームシミュレーターのみです。フレームワークはオープンソースとして公開されており、研究者が容易に再現・拡張できます。

ただし、分散システムの設定と、マルチエージェント学習の安定性を確保するためのハイパーパラメータ調整には注意が必要です。特に、RAEのEMA減衰率αの選択は、学習の安定性に大きく影響します。

### 4.2 計算効率
計算要件は依然として substantial で、8台のH100 GPUで約25時間の訓練が必要です。これは多くの研究グループにとってアクセス可能な範囲内ですが、大規模な実験には依然として大きなリソースが必要です。

効率面での利点は、継続的な改善が可能なことです。固定データセットでの訓練とは異なり、SPIRALは理論上無限のトレーニングデータを生成できるため、長期的な改善の可能性があります。ただし、実験では訓練が進むにつれて改善が鈍化することも観察されました。

### 4.3 応用可能性
SPIRALの応用可能性は広範囲に及びます：

1. **教育分野**: 学生の推論能力を向上させるための対話型学習システム
2. **問題解決エージェント**: 複雑な多段階問題を解決するAIアシスタント
3. **戦略的意思決定**: ビジネスや政策立案における戦略的分析ツール
4. **ゲームAI**: より人間らしい対戦相手の開発

特に興味深いのは、異なるゲームが補完的なスキルを開発することです。これは、特定の認知能力を対象としたゲームを設計することで、カスタマイズされた推論能力の向上が可能であることを示唆しています。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、言語モデルの推論能力開発において重要なパラダイムシフトを示しています。人間が設計した報酬関数やデータセットに依存せず、単純なゲームでの自己対戦だけで汎用的な推論能力を向上させることができるという発見は、AIの自律的な能力開発の可能性を示しています。

特に重要なのは、ゲームと推論タスクの間の転移メカニズムの解明です。体系的分解、期待値計算、場合分け分析といった認知パターンが、ドメインを超えて転移することの実証は、人間の認知発達の理解にも示唆を与えます。競争的な圧力が暗記を排除し、真の推論を強制するというメカニズムは、教育心理学の観点からも興味深い知見です。

また、役割条件付きアドバンテージ推定（RAE）の導入は、マルチエージェント強化学習の技術的進歩としても評価できます。この手法は、言語モデルに限らず、他のマルチエージェントシステムにも応用可能な一般的な技術です。

### 5.2 今後の展望
今後の研究方向として、以下が考えられます：

1. **より複雑なゲームへの拡張**: 現在は比較的単純なゲームに限定されていますが、より複雑な戦略ゲームや部分観測可能なゲームへの拡張が期待されます。

2. **協力ゲームの探求**: 現在は競争的なゼロサムゲームに焦点を当てていますが、協力的なゲームが異なる種類の推論能力を開発する可能性があります。

3. **ゲーム設計の原理**: 特定の推論能力を対象としたゲームを原理的に設計する方法論の開発が重要です。

4. **自己改善エージェントのエコシステム**: 複数のエージェントが相互作用しながら自律的に能力を向上させるシステムの構築。

5. **転移メカニズムの理論的理解**: なぜゲームスキルが他のドメインに転移するのかの理論的な説明の構築。

制限事項として、現在のアプローチは依然として設計されたゲーム環境を必要とし、計算要件も considerable です。また、実世界のタスクへの直接的な評価が不足しており、学術的ベンチマークでの改善が実用的な能力向上につながるかは更なる検証が必要です。

総合的に見て、SPIRALは言語モデルの自律的な推論能力開発において重要な一歩を示しており、人工知能が人間の監督なしに自己改善する未来への道筋を示しています。