# Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach

## 基本情報
- arXiv ID: 2512.07814v1 (https://arxiv.org/abs/2512.07814)
- 著者: Hua Yang, Alejandro Velasco, Sen Fang, Bowen Xu, Denys Poshyvanyk
- 所属: North Carolina State University, William & Mary
- 投稿日: 2025年12月11日
- カテゴリ: cs.CR, cs.LG

## 簡単に説明すると
この研究は、コード生成用大規模言語モデル（LLMs for Code）における個人情報（PII）の漏洩リスクを、訓練動態と因果分析を用いて体系的に調査した画期的な研究です。
APIキー、パスワード、ユーザー名、メールアドレス、IPアドレスなど異なる種類のPIIが、学習の難易度によって漏洩リスクが大きく異なることを発見しました。
学習が容易なPII（IPアドレスなど）は漏洩リスクが高く、学習困難なPII（APIキーやパスワード）は漏洩頻度が低いという因果関係を初めて実証しています。

## 1. 研究概要
### 1.1 背景と動機
コード生成用大規模言語モデル（LLMs for Code）は、ソフトウェアエンジニアリングにおいて開発者の生産性を大幅に向上させています。
しかし、これらのモデルはGitHubなどのオープンソースリポジトリから大規模な訓練データを収集するため、深刻なプライバシーリスクを抱えています。

オープンソースリポジトリには、APIキー、パスワード、ユーザー名、メールアドレス、IPアドレス、個人名など膨大な個人情報（PII）が含まれています。
強力な記憶能力を持つLLMsは、訓練中にこれらの機密情報を記憶し、推論時に再現してしまうリスクがあります。
実際に、MicrosoftのAI研究チームが38TBの私的データを誤って露出させる事件や、Toyotaで29万人以上の顧客データにアクセス可能な認証情報が5年間GitHub上に露出していた事件など、深刻な事例が報告されています。

GDPRやHIPAAなどの規制フレームワークがPIIの厳格な取り扱いを要求する中、従来の研究ではPIIを単一の均質なカテゴリとして扱い、異なるPII種類が持つ異質なリスクを見過ごしてきました。

### 1.2 主要な貢献
本研究では、PIIの種類によって学習・漏洩リスクが異なるという仮説を、訓練動態と因果分析を用いて体系的に検証します。

- 実世界のコードリポジトリから多種類のPIIデータセットを構築し、自動検出・LLM改良・人的検証を組み合わせた高信頼性の手法を開発
- 異なるPII種類の訓練動態を信頼度と変動性の観点から分析し、相対的な学習難易度を特性化
- 複数のモデルファミリー・規模において、学習動態と漏洩行動を関連付ける系統的なPII攻撃実験を実施
- 学習難易度が漏洩に与える因果効果を定量化する構造因果モデルを定式化し、複数の反駁戦略で堅牢性を検証
- PIIの種類によって漏洩リスクが異なることの初の因果的証拠を提供し、LLMs for Codeにおける種類認識・学習可能性認識防御機構の設計に実用的な洞察を提示

## 2. 提案手法
### 2.1 手法の概要
本研究は4段階の系統的な方法論を採用しています。
まず、実世界のコードリポジトリから多様なPII種類を含む高品質なデータセットを構築します。
次に、異なる規模・アーキテクチャの代表的なLLMs for Codeをファインチューニングして、モデルファミリー間のパターンを捉えます。
第3段階では、実際のPIIデータに対する訓練動態を計算し、第4段階で構造因果モデルを定式化して学習難易度が漏洩に与える因果効果を推定し、複数の反駁戦略で検証します。

脅威モデルとして、攻撃者がオープンソースリポジトリから入手可能な文脈情報を部分的に再現する入力を構築してPIIを抽出する現実的な設定を想定しています。
攻撃者はモデルへの入出力アクセスのみを持ち、内部構造・パラメータ・訓練データへのアクセスはありません。

### 2.2 技術的詳細
**訓練動態の計算**
各PIIトークンの学習動態を、信頼度と変動性の2つの指標で特性化します。

信頼度は、モデルが訓練エポック全体でPIIトークンの正解ラベルに割り当てる平均確率として定義されます：
$$\hat{\mu}_i = \frac{1}{E} \sum_{e=1}^{E} p_{\theta^{(e)}}(y_i^* \mid x_i)$$

変動性は、訓練過程における信頼度スコアの安定性を反映し、信頼度の標準偏差で測定されます：
$$\hat{\sigma}_i = \sqrt{ \frac{1}{E} \sum_{e=1}^{E} \left( p_{\theta^{(e)}}(y_i^* \mid x_i) - \hat{\mu}_i \right)^2 }$$

**データマップによる分類**
Swayamdipta et al.のデータマップ手法を適用し、信頼度と変動性に基づいてPIIインスタンスを3つの領域に分類します：
- 学習容易領域：高信頼度・低変動性（左上）
- 学習困難領域：低信頼度・低変動性（左下）
- 曖昧領域：高変動性（右側）

**構造因果モデル**
Pearlの因果推論の階層に基づく構造因果モデル（SCM）を定式化します。
学習動態（容易・曖昧・困難）が攻撃成功率に与える因果効果を、統計的・機械学習手法を用いて推定し、感度分析による堅牢性検証を実施します。

### 2.3 新規性
従来研究がPIIを単一カテゴリとして扱っていたのに対し、本研究は初めてPII種類間の学習・漏洩行動の異質性を体系的に分析しています。
訓練動態と漏洩リスクの関係を相関にとどまらず因果的に解明した点が画期的です。

DoCodeフレームワークを適用した構造因果モデルによる因果解析により、見かけ上の相関と真の因果効果を区別し、学習難易度が漏洩行動に直接影響するかを厳密に検証しています。
実世界のコードリポジトリから収集した多様なPII種類による大規模実験により、理論的洞察と実用的含意の両方を提供しています。

## 3. 実験結果
### 3.1 実験設定
**データセット構築**
実世界のLLM訓練コーパスから各PII種類を含むコードスニペットを抽出し、自動化されたデータ収集・精製ワークフローを設計しています。
regexベースのフィルタ、LLM支援判定、ヒューリスティックルールを統合し、人的検証で精度と信頼性を確保しています。
各PII種類について、高度に機密なPIIを含む1,500のコードファイルを収集しました。

**モデル設定**
異なる規模とアーキテクチャの代表的なLLMs for Codeをファインチューニングし、モデルファミリー間で一般化するパターンを捉えています。
CodeLlama-7B、CodeLlama-13B、Qwen系モデルなど複数のモデルを10エポックでファインチューニングし、効果的な学習が行われたことを訓練損失曲線で確認しています。

**評価指標**
PIIトークンに対応する信頼度と変動性を訓練エポック全体で測定し、データマップ可視化により学習動態を定性的に分析します。
攻撃実験では、部分的な文脈情報を用いたPII抽出攻撃の成功率を測定し、学習動態との関係を調査しています。

### 3.2 主要な結果
**RQ1: PII種類間の学習難易度の違い**
データマップ分析により、PII種類間で明確な学習動態の違いが確認されました。

APIキーとIPアドレスのインスタンスは、信頼度0.9近く・変動性0近くの学習容易領域に多く位置しています。
対照的に、メールアドレスとユーザー名のインスタンスは、モデル予測がエポック間で変動する曖昧領域を多く占めています。
APIキーの大部分、およびパスワード・ユーザー名のインスタンスは、一貫して低信頼度を示す学習困難領域に分類されます。

この結果は直感的理解と一致しており、パスワード・ユーザー名・APIキーは不規則で構造的規則性に欠けるランダム文字列であるため、人間でも記憶が困難です。
一方、IPアドレスは規則的なパターンを持つため、比較的学習が容易です。

**RQ2: 学習難易度と漏洩リスクの関係**
攻撃実験により、学習動態と漏洩行動の間に強い相関関係が確認されました。
学習容易なインスタンス（IPアドレスなど）は高い漏洩リスクを示し、学習困難なインスタンス（APIキーやパスワード）は漏洩頻度が低くなっています。
曖昧な種類はより複雑なパターンを示し、一部の文脈では漏洩を減少させ、他の文脈では増幅させています。

**RQ3: 因果関係の解明**
構造因果モデルによる分析により、学習難易度が漏洩に与える因果効果が定量化されました。
容易・曖昧・困難の3つの学習難易度レベルが攻撃成功率に与える影響が、複数の反駁戦略により検証され、堅牢性が確認されています。

### 3.3 既存手法との比較
従来研究では、PIIを均質なカテゴリとして扱い、記憶化現象の存在は示されていましたが、種類間の差異は体系的に分析されていませんでした。
Niu et al.やHuang et al.の研究では、商用コード補完モデルが機密情報を再現可能であることが示されていますが、因果関係の解明には至っていませんでした。

本研究は、初めてPII種類別の学習・漏洩パターンを定量化し、訓練動態と漏洩行動の因果関係を厳密に証明しています。
CodexLeaksやYour Code is My Code等の先行研究と比較し、相関にとどまらない因果的理解を提供している点で画期的です。

実世界のリポジトリから収集した多様なPII種類による大規模実験により、従来手法では捉えられなかったPII種類間の異質性と、その背後にある因果機序を明らかにしています。

## 4. 実用性評価
### 4.1 実装の容易性
本研究で提案された手法は、実用化において高い実装容易性を持っています。

データセット構築パイプラインは、regexベースフィルタ・LLM支援判定・ヒューリスティックルール・人的検証を組み合わせた自動化ワークフローとして設計されており、他の組織でも再現可能です。
訓練動態の計算は標準的な機械学習フレームワークで実装でき、信頼度と変動性の計算は既存のファインチューニングパイプラインに容易に統合できます。

構造因果モデルの実装も、DoCodeフレームワークに基づく標準的な因果推論手法を使用しており、既存の因果解析ライブラリを活用できます。
研究チームは実装詳細とアーティファクトを公開しており（https://anonymous.4open.science/r/pii_final-42A1）、研究の再現性と実用化を促進しています。

### 4.2 計算効率
提案手法は計算効率の観点で実用的です。

訓練動態の計算は、通常のファインチューニング過程で追加的に実行でき、大幅な計算オーバーヘッドを生じません。
信頼度と変動性の計算は、各エポックでのモデル予測を記録するだけで実現でき、メモリ使用量の増加も最小限です。

PII攻撃実験は入出力アクセスのみを要求するため、モデルの内部構造へのアクセスが不要で、商用モデルに対しても効率的に実行できます。
因果分析は一度のオフライン計算で実行でき、リアルタイム推論には影響しません。

複数のモデル規模（7B、13Bパラメータ）での実験結果により、計算コストが実用的な範囲内であることが実証されています。

### 4.3 応用可能性
本研究の応用可能性は極めて広範囲にわたります。

**セキュリティ・プライバシー分野**
GDPR、HIPAAなどの規制遵守において、PII種類別のリスク評価と適応的防御戦略の設計が可能になります。
金融、医療、政府機関など機密性要求の高い業界での安全なLLMs for Code展開に直接適用できます。

**モデル開発・運用**
訓練データのプライバシーリスク評価、記憶化の予防、推論時の機密情報露出防止において実用的な指針を提供します。
種類認識・学習可能性認識の防御機構開発により、より安全なAIシステム構築が実現できます。

**研究基盤**
因果的解釈可能性のフレームワークは、他のAIシステムの安全性分析にも拡張適用可能です。
大規模言語モデルの記憶化現象の理解深化により、より根本的な安全性向上手法の開発基盤となります。

**産業応用**
コード生成ツール、開発環境、CI/CDパイプラインでのセキュリティ強化において、実装されたリスク評価手法が活用できます。
オープンソースソフトウェアの安全性向上、企業での内製AI開発における規制遵守支援など、幅広い実用的価値があります。

## 5. まとめと所感
### 5.1 論文の意義
本論文は、LLMs for Codeにおけるプライバシーリスクの理解を革新的に深化させた極めて重要な研究です。

**理論的意義**
従来研究がPIIを均質なカテゴリとして扱っていた限界を打破し、PII種類間の学習・漏洩行動の異質性を初めて体系的に解明しました。
訓練動態と漏洩リスクの関係を相関から因果へと昇華させ、構造因果モデルによる厳密な因果推論により、見かけの相関と真の因果効果を明確に区別しています。
この因果的理解は、AIの安全性研究において新しいパラダイムを確立する画期的な貢献です。

**実用的価値**
実世界のコードリポジトリから収集した大規模実験により、理論的洞察と実用的含意を両立しています。
GDPRやHIPAAなどの規制環境下での安全なLLMs for Code展開において、種類別リスク評価と適応的防御戦略の設計指針を提供しています。
APIキー、パスワード等の高リスクPIIと、IPアドレス等の相対的低リスクPIIを区別する科学的根拠により、効率的なセキュリティ投資配分が可能になります。

**方法論的革新**
regexベース検出・LLM支援判定・ヒューリスティックルール・人的検証を統合した多段階検証プロセスは、他のプライバシー研究にも適用可能な汎用的手法です。
DoCodeフレームワークを活用した構造因果モデルの適用により、AIシステムの解釈可能性研究に新しい分析手法を提供しています。

**社会的インパクト**
Microsoftの38TBデータ露出事件、Toyotaの5年間認証情報露出事件など、実際の深刻な事例を背景とした研究により、社会的な緊急性と実用性を兼ね備えています。
オープンソースソフトウェア開発の安全性向上、企業でのAI技術採用における規制遵守支援など、広範囲な社会的貢献が期待されます。

### 5.2 今後の展望
本研究の成功は、複数の重要な研究・開発方向への道筋を開いています。

**手法の拡張**
より多様なPII種類（生体認証情報、金融情報、位置情報等）への拡張により、包括的なプライバシーリスク評価フレームワークの構築が期待されます。
他のモダリティ（自然言語処理、画像生成、音声合成等）のAIシステムへの因果分析手法の適用により、汎用的なAI安全性評価基盤の確立が可能になります。

**防御技術の発展**
学習可能性認識の前処理、種類別適応的匿名化、動的プライバシー保護機構など、因果的理解に基づく次世代防御技術の開発が加速するでしょう。
差分プライバシー、連合学習、同態暗号などの既存プライバシー保護技術との統合により、より堅牢なプライバシー保護システムの構築が実現できます。

**規制・標準化への影響**
AI規制（EU AI Act等）、業界標準（ISO/IEC 27001等）、企業ガバナンス（MLOps、ResponsibleAI等）における科学的根拠に基づくガイドライン策定への貢献が期待されます。
PII種類別のリスク分類とそれに基づく段階的規制アプローチの確立により、効率的かつ効果的な規制フレームワークの発展に寄与するでしょう。

**産業応用の拡大**
コード生成ツール、統合開発環境、CI/CDパイプラインでの実装により、日常的な開発ワークフローにおけるセキュリティ強化が実現されます。
クラウドサービス、SaaS製品、エンタープライズソフトウェアでの組み込み型プライバシー保護機能の開発により、大規模な社会実装が進展するでしょう。

本研究は、AIの安全性とプライバシー保護の分野において、理論と実践を架橋する傑出した成果として、今後長期にわたって影響を与え続けることが確実です。