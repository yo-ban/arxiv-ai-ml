# UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation

## 基本情報
- arXiv ID: 2512.07831v1 (https://arxiv.org/abs/2512.07831)
- 著者: Jiehui Huang, Yuechen Zhang, Xu He, Yuan Gao, Zhi Cen, Bin Xia, Yan Zhou, Xin Tao, Pengfei Wan, Jiaya Jia
- 所属: HKUST, CUHK, Tsinghua University, Kling Team (Kuaishou Technology)
- 投稿日: 2025年12月11日
- カテゴリ: cs.CV, cs.LG

## 簡単に説明すると
この研究は、複数のモダリティを統一的に学習することで、世界を理解した高品質な動画生成を実現するフレームワーク「UnityVideo」を提案しています。
対象モダリティには、セグメンテーションマスク、人体スケルトン、DensePose、オプティカルフロー、深度マップが含まれます。
従来の単一モダリティによる制限を克服し、異なる視覚情報を組み合わせることで物理世界の理解を深め、ゼロショット汎化性能を向上させています。
プロジェクトページは https://jackailab.github.io/Projects/UnityVideo で、コードとデータは https://github.com/dvlab-research/UnityVideo で公開されています。

## 1. 研究概要
### 1.1 背景と動機
近年の大規模言語モデル（LLM）は、自然言語、コード、数式などの多様なテキストベースのモダリティを統一的な学習パラダイム内で処理することで、強力な汎化性と推論能力を実現している。
この統合されたアプローチは、補完的なテキストサブモダリティがタスクパフォーマンスを向上させ、創発的な推論を支援することを示している。

同様に、最近のビデオファウンデーションモデルは、スケールとパラメータの増加に伴って有望な世界モデリング能力を示している。
しかし、視覚的スケーリングは主にRGBビデオのみに焦点を当てており、これは言語モデルをプレーンテキストのみで訓練することに類似している。
このギャップは、深度、オプティカルフロー、セグメンテーション、スケルトン、DensePoseなどの視覚的サブモダリティを統合することで、統一されたテキスト学習がLLMに対して行ったように、モデルの物理世界への理解を強化できるかという疑問を提起している。

従来の研究では、深度マップ、オプティカルフロー、スケルトン、セグメンテーションマスクなどの単一の補助入力がビデオ生成に利益をもたらすことが示されている。
しかし、統一的な学習パラダイムがクロスモーダル相互作用と世界認識に与える効果は不明確であった。

### 1.2 主要な貢献
本論文では、複数の視覚モダリティとタスクを単一のDiffusion Transformerで統合する革新的なフレームワーク「UnityVideo」を提案している。
このアプローチは相互知識転送を可能にし、収束を改善し、単一タスクベースラインを上回る性能を実現している。

- 複数のビデオタスクとモダリティを統合し、相互知識転送、より良い収束、単一タスクベースラインを上回る性能を可能にする新しい統一フレームワークUnityVideoの提案
- モダリティ適応スイッチャー、インコンテキスト学習器、動的ノイズスケジューリング戦略の導入により、多様な目的での効率的な共同訓練と大規模データセットへのスケーラビリティを実現
- 1.3百万ペアのマルチモーダルビデオデータセット「OpenUni」と、Unreal Engineから派生した30千サンプルのベンチマーク「UniBench」の構築とリリース

## 2. 提案手法
### 2.1 手法の概要
UnityVideoは、ビデオ生成とマルチモーダル理解を単一のDiffusion Transformer内で統一するフレームワークである。
モデルは共有DiTバックボーンを通じてRGBビデオ、テキスト条件、補助モダリティを処理し、動的にタスクタイプをサンプリングし、対応するノイズスケジュールを適用する。

フレームワークは3つの補完的な学習パラダイムを同時に処理する：補助モダリティからのRGBビデオ生成、RGBビデオからの補助モダリティ推定、ノイズからの両方の共同生成。
RGBトークンとモダリティトークンは幅方向に連結され、セルフアテンションモジュールを通じて相互作用する。

### 2.2 技術的詳細
#### 動的タスク配分
3つのパラダイムでの同時最適化を可能にするため、訓練中に確率的タスク選択を導入している。
各反復で、確率p_cond、p_est、p_jointでタスクタイプをサンプリングし、タイムステップtでRGBとモダリティトークンに適用するノイズスケジュールを決定する。
タスク確率は学習難易度に反比例して割り当てられる：p_cond < p_est < p_joint。

#### インコンテキスト学習器
モデルの内在的な文脈推論能力を活用するため、ビデオ内容ではなくモダリティタイプを記述するモダリティ固有のテキストプロンプトC_mを注入している。
連結されたRGBトークンV_rとモダリティトークンV_mに対し、デュアルブランチクロスアテンションを別々に実行：V_r' = CrossAttn(V_r, C_r)でRGB特徴量と内容キャプション、V_m' = CrossAttn(V_m, C_m)でモダリティ特徴量とタイプ記述。

#### モダリティ適応スイッチャー
スケーラビリティのため、kモダリティに対する学習可能埋め込みリストL_m = {L_1, L_2, ..., L_k}を導入している。
各DiTブロック内で、AdaLN-Zeroがタイムステップ埋め込みに基づいてRGB特徴量の変調パラメータ（スケールγ、シフトβ、ゲートα）を生成する。
このメカニズムをモダリティ固有パラメータに拡張：γ_m, β_m, α_m = MLP(L_m + t_emb)。

#### 訓練目的
Conditional Flow Matchingに従い、3つのモードを適応的に切り替える動的訓練戦略を採用している：
- L_cond(θ; t) = E[||u_θ(r_t, [m_0, c_txt], t) - v_r||²] （条件生成）
- L_est(θ; t) = E[||u_θ(m_t, r_0, t) - v_m||²] （モダリティ推定）
- L_joint(θ; t) = E[||u_θ([r_t, m_t], c_txt, t) - [v_r, v_m]||²] （共同生成）

### 2.3 新規性
UnityVideoの主要な新規性は、複数の視覚モダリティとタスクを単一の統一フレームワークで処理する点にある。
既存のアプローチが単一モダリティまたは単一アーキテクチャに分離されているのに対し、本研究は真の意味でのマルチタスク学習を単一フレームワークで実現している。

動的ノイズスケジューリング戦略により、従来の段階的訓練で一般的だった破滅的忘却を防ぎ、モデルが3つの分布すべてを同時に学習できる。
また、インコンテキスト学習器とモダリティ適応スイッチャーの組み合わせにより、意味レベルとアーキテクチャレベルの両方でモダリティ認識を実現している。

## 3. 実験結果
### 3.1 実験設定
実験は10Bパラメータの内部DiTバックボーンを使用し、2段階で訓練を実施している。
第1段階では、50万の人間中心ビデオクリップでモデルを16千ステップ訓練し、第2段階では130万のビデオクリップの大規模データセットで追加40千ステップ訓練を行った。
バッチサイズ32、学習率5×10^-5で訓練し、推論時には50 DDIMサンプリングステップとCFGスケール7.5を使用している。

評価は公開VBenchデータセットと新規構築したUniBenchデータセットで実施された。
UniBenchは、正確なビデオ推定評価のためのUnreal Engine（UE）から得られた200の高品質サンプルと、制御可能生成およびセグメンテーション評価のための実世界ビデオから手動でキュレーションされた200サンプルで構成される。

OpenUniデータセットは130万のビデオクリップで5つのモダリティ（オプティカルフロー、深度、DensePose、スケルトン、セグメンテーション）にわたっている。
データセットには37万358の単一人物クリップ、9万7468の二人クリップ、Koala36Mからの48万9445クリップ、OpenS2Vからの34万3558クリップが含まれる。

### 3.2 主要な結果
包括的評価では、UnityVideoがすべてのメトリクスで最先端または競合する性能を達成していることが実証された。
テキストtoビデオ生成において、UnityVideoは背景一貫性97.44、美的品質64.12、全体一貫性23.57、動的度47.76を達成し、Kling1.6、OpenSora2、HunyuanVideo-13B、Wan2.1-14Bなどの商用および最先端モデルを上回った。

制御可能生成タスクでは、UnityVideoは背景一貫性96.04、全体一貫性21.86、動的度64.42を達成し、full-ditやVACEを大幅に上回った。
ビデオ推定タスクでは、セグメンテーションでmIoU 68.82、mAP 23.25、深度推定で絶対相対誤差0.022、δ<1.25で98.98%を達成し、既存の専門化手法を上回った。

特に重要なのは、統一マルチモーダル訓練が単一モダリティ訓練やRGBファインチューニングベースラインと比較してRGBビデオ生成で最低の最終損失を達成したことである。
これは、マルチモーダル学習の相互利益を明確に示している。

### 3.3 既存手法との比較
UnityVideoは新しい統一パラダイムを導入しているため、直接的に比較可能なモデルは存在しない。
評価は3つの関連カテゴリーの先進モデルと行われた：ビデオ生成、ビデオ推定、ビデオセグメンテーション。

ビデオ生成では、Keling-1.6などの商用モデル、OpenSora、Hunyuan-13B、Wan-2.1-13BなどのオープンソースモデルとKELING-1.6と比較し、VACE、Full-DiTなどの制御可能生成モデル、Aetherなどのビデオと深度の共同生成モデルとも比較した。

ビデオ推定では、DepthCrafter、Geo4D、Aetherなどのdiffusionベース深度推定モデルと比較し、ビデオセグメンテーションではSAMWISE、SeCなどのプロンプトベースオブジェクトセグメンテーションをサポートする最近のモデルと比較した。

すべての評価において、UnityVideoは一貫して最先端の性能を達成し、特に統一学習の利点を実証している。
単一タスクに特化したモデルと比較しても競合またはそれを上回る性能を示し、真の統一フレームワークの価値を証明している。

## 4. 実用性評価
### 4.1 実装の容易性
UnityVideoの実装は既存のDiffusion Transformerアーキテクチャを基盤としており、実装の複雑さは中程度である。
フレームワークはプラグアンドプレイのモダリティ選択を推論時に可能にし、新しいモダリティの追加は比較的容易である。
ただし、複数モダリティの同期と動的ノイズスケジューリングの実装には注意深い設計が必要である。

モダリティ適応スイッチャーとインコンテキスト学習器は軽量な設計であり、既存のアーキテクチャへの統合は実現可能である。
提供されるコードベース（https://github.com/dvlab-research/UnityVideo）により、研究者が手法を再現し、独自のデータセットで拡張することが可能である。

### 4.2 計算効率
10Bパラメータのモデルは大規模であるが、統一フレームワークにより複数の専門化モデルを個別に訓練する必要がなくなるため、全体的な計算効率は良好である。
動的タスク配分により、単一の訓練サイクルで3つの異なる目的を学習でき、段階的訓練と比較してより効率的である。

ただし、130万サンプルのOpenUniデータセットでの訓練には相当な計算リソースが必要である。
推論時には50 DDIMステップを使用するため、リアルタイム応用には最適化が必要であるが、高品質な結果を求める応用には適している。

### 4.3 応用可能性
UnityVideoの応用可能性は非常に広範囲にわたる。
ビデオ生成、制御可能生成、モダリティ推定を単一モデルで処理できるため、統一されたビデオ理解システムの構築が可能である。

コンテンツ制作においては、テキストからビデオ生成、既存ビデオの深度やセグメンテーション推定、特定の制御信号に基づくビデオ編集など、多様なワークフローをサポートできる。
ロボティクスや自動運転などの領域では、世界認識能力の強化により、より正確な環境理解が可能になる。

ゼロショット汎化能力により、訓練データに含まれない新しいオブジェクトやスタイルに対しても適用可能であり、実世界での展開において重要な利点となる。
また、UniBenchベンチマークの提供により、統一ビデオモデルの標準化された評価が可能になり、この分野の発展を加速する。

## 5. まとめと所感
### 5.1 論文の意義
UnityVideoは、ビデオ生成とマルチモーダル理解の分野において画期的な貢献を成している。
従来の単一モダリティ学習の限界を克服し、複数の視覚モダリティを統一的に学習することで、物理世界への理解を深める新しいパラダイムを提示している。

特に重要なのは、統一学習が相互利益をもたらし、収束を加速し、ゼロショット汎化を改善することを実証的に示した点である。
これは大規模言語モデルにおける統一テキスト学習の成功を視覚ドメインに拡張した意義深い成果といえる。

技術的な革新として、動的ノイズスケジューリング、モダリティ適応スイッチャー、インコンテキスト学習器の組み合わせは、マルチモーダル学習における新しい設計パラダイムを確立している。
また、OpenUniデータセットとUniBenchベンチマークの提供により、この分野の標準化と発展に大きく貢献している。

実験結果は一貫して最先端性能を示しており、理論的提案が実用的価値を持つことを明確に実証している。
特に、単一の統一モデルが複数の専門化モデルと競合またはそれを上回る性能を達成したことは、統一学習の強力さを証明している。

### 5.2 今後の展望
今後の発展方向として、いくつかの重要な領域が考えられる。
まず、より多様なモダリティ（音声、3Dポイントクラウド、温度マップなど）の統合により、さらに包括的な世界モデリングが可能になるだろう。

スケーラビリティの観点では、より大規模なモデルとデータセットでの訓練により、創発的な推論能力の発現が期待される。
また、計算効率の改善により、リアルタイム応用やエッジデバイスでの展開が可能になるかもしれない。

実用面では、特定のドメイン（医療画像、衛星画像、産業検査など）への適応により、専門分野での応用価値が向上するだろう。
また、ユーザーインタラクションの改善により、より直感的で使いやすいインタフェースの開発が可能になる。

研究面では、なぜ統一学習が相互利益をもたらすのかの理論的理解の深化、異なるモダリティ間の知識転移メカニズムの解明、最適なモダリティ組み合わせの探索などが重要な課題である。
これらの進展により、次世代の世界モデルへの道筋がより明確になるだろう。