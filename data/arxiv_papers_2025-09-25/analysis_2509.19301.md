# Residual Off-Policy RL for Finetuning Behavior Cloning Policies

## 基本情報
- arXiv ID: 2509.19301v1 (https://arxiv.org/abs/2509.19301)
- 著者: Lars Ankile, Zhenyu Jiang, Rocky Duan, Guanya Shi, Pieter Abbeel, Anusha Nagabandi
- 所属: Amazon FAR (Frontier AI & Robotics), Stanford University, Carnegie Mellon University, UC Berkeley
- 投稿日: 2025年09月25日
- カテゴリ: cs.AI, cs.LG

## 簡単に説明すると
この研究は、行動クローニング（Behavior Cloning, BC）で学習したロボット制御ポリシーを、
オフポリシー強化学習（Off-policy RL）で効率的に改善する手法「ResFiT」を提案しています。
従来の手法では、大規模なBCポリシー全体をRLで最適化するのは計算的に困難でした。
この研究では凍結されたBCポリシーに軽量な残差修正を学習することで問題を解決しています。
特筆すべきは、29自由度の二足歩行ヒューマノイドロボットでの実世界実験に成功していることです。
これは5本指の手を持つヒューマノイドロボットでの実世界RL学習として初めての成果となっています。
プロジェクトウェブサイト: https://residual-offpolicy-rl.github.io/

## 1. 研究概要
### 1.1 背景と動機
近年、人間のデモンストレーションから学習する行動クローニング（BC）により、
視覚運動制御ポリシーの分野で目覚ましい進歩が見られています。
しかし、BCには根本的な限界があります。
第一に、デモンストレーションの質に依存します。
人間の遠隔操作者の性能は一般的に最適ではありません。
第二に、手動でのデータ収集には膨大な労力と高度なインフラが必要です。
第三に、データ量を増やしても性能向上には限界があります。これは最近の研究で明らかになっています。

一方で、強化学習（RL）は環境との試行錯誤を通じて自律的に学習する手法です。
様々な分野で大きな成功を収めています。
しかし、実世界のロボットに直接RLを適用するのは困難です。
理由として、サンプル効率の低さ、安全性への懸念、
高自由度システムでの長期間タスクにおけるスパース報酬からの学習の困難さなどが挙げられます。

### 1.2 主要な貢献
この研究の主要な貢献は以下の4点にまとめられます。

まず、残差学習フレームワークを提案しました。BCポリシーをブラックボックスの基底として扱い、
軽量な段階的残差修正をサンプル効率の高いオフポリシーRLで学習する手法を開発しました。

次に、高自由度システムでの実用的なRLを実現しました。スパース二値報酬信号のみで、
シミュレーションと実世界の両方で高自由度システムにおける操作ポリシーを改善できました。

さらに、器用な手を持つヒューマノイドロボットでの実世界RL学習に初めて成功しました。

最後に、様々な視覚ベースタスクで高い性能を達成し、実世界でのRL展開への実用的な道筋を示しました。

## 2. 提案手法
### 2.1 手法の概要
提案手法ResFiTは二段階のアプローチを取っています。
第一段階では、デモンストレーションデータセットを用いて行動クローニングで基底ポリシーを訓練し、
その後この基底ポリシーを凍結します。
第二段階では、基底ポリシーの行動を修正する残差ポリシーをRLで学習します。

基底ポリシーは行動チャンクを出力するが（時刻tにおいて将来のk個の行動を予測）、
残差ポリシーは各時刻で単一の修正を提供する。
最終的な行動は、基底行動と残差行動の和となる：
a_t = a_t^{base} + a_t^{res}

この設計により、基底ポリシーのパラメータ化や訓練方法に依存しない手法となり、
残差の大きさを制御することで安全な探索が可能となる。

### 2.2 技術的詳細
残差RLの定式化では、標準的なオフポリシーRL手法を拡張している。
通常のRLでは$Q_\phi(s_t, a_t)$と$\pi_\theta(s_t)$を学習するが、
残差設定では$Q_\phi(s_t, a_t^{base} + \pi_\theta(s_t, a_t^{base}))$と
$\pi_\theta(s_t, a_t^{base})$を学習する。

重要な設計決定として、以下の要素を含む：
- **UTD比（Update-to-Data ratio）**: データ1つあたりの更新回数を制御
- **n-step returns**: TD目標でのn段階リターンの使用
- **Layer normalization**: アクターとクリティックMLPでの層正規化
- **Ensemble methods**: 複数のQ関数によるアンサンブル学習
- **Demonstration mixing**: オンラインRL段階でのデモンストレーションの継続利用

### 2.3 新規性
従来の残差RL手法と比較して、この研究の新規性は以下の点にある。

まず、既存手法の多くはシミュレーション環境に限定されていたが、
この研究は実世界での高自由度システムへの適用を実現した。
特に、29自由度のヒューマノイドロボットでの実世界学習は前例がない。

技術的には、行動チャンクを使用する大規模BCポリシーとの組み合わせを可能にし、
単一ステップの修正のみを学習することでRL最適化の次元を大幅に削減している。
例えば、29自由度のロボットで30ステップの行動チャンクを使用する場合、
870次元の行動空間をRLで扱う必要があるが、
残差アプローチでは29次元の修正のみを学習すればよい。

さらに、オフポリシーRL設計において、デモンストレーションデータの多重利用
（事前訓練、クリティック初期化、オンライン学習での混合）を実現している。

## 3. 実験結果
### 3.1 実験設定
シミュレーション実験では、RobomimicとDexMimicGenのタスクを使用し、
単一アーム操作（Can、Square）と両手協調タスク（BoxCleanup、CanSort、Coffee）を評価した。
すべてのタスクで画像観測とロボット関節状態を使用し、
特権的なオブジェクト状態情報は使用しない現実的な設定で実験を行った。

実世界実験では、Dexmate社のVegaヒューマノイドロボットを使用した。
このロボットは2つの7自由度アーム、2つの6自由度OyMotion器用ハンド、
頭部に搭載された3自由度のZedカメラを持つ29自由度システムである。

### 3.2 主要な結果
シミュレーション結果では、ResFiTは全てのタスクでほぼ完璧なポリシーに収束することを示した。
従来のオンポリシー残差RL（PPO使用）と比較して、
約200倍のサンプル効率改善（200kステップ vs 40Mステップ）を達成した。

最も困難なタスクであるCoffeeタスクでは、
長期間ホライゾンと高精度が要求される中で、
ResFiTのみが効率的にほぼ完璧なタスク性能に収束した。
ベースライン手法や改良版は、成功率がゼロに崩壊するか、
高性能達成により長時間を要した。

実世界実験では、2つのタスク（WoollyBallPnP、PackageHandover）において、
ResFiTによる残差RLアプローチが基底モデルよりも
有意な性能向上を示すことを確認した。

### 3.3 既存手法との比較
ResFiTを以下の手法と比較した：

**RLPD（最適化版）**: 同じオフポリシーRL設計決定を適用した最適化版RLPDでは、
基底ポリシーを使用せずに直接学習を行うが、
高自由度タスクでは90%を超える成功率の達成に苦労した。

**IBRL**: 事前訓練されたBCポリシーを行動提案とターゲット値ブートストラップに使用する手法だが、
長期ホライゾンタスクでは性能が低下した。

**Filtered BC**: 基底ポリシーから開始して成功した軌道のみを
データセットに追加する手法では、安定性は保たれるものの
改善は最小限に留まった。

## 4. 実用性評価
### 4.1 実装の容易性
ResFiTの大きな利点の一つは、実装の容易性である。
基底ポリシーをブラックボックスとして扱うため、
既存のBCポリシーの詳細な構造や訓練方法を知る必要がない。
残差ポリシーは軽量なMLPで実装でき、
基底ポリシーのパラメータ化（拡散モデル、トランスフォーマー等）に依存しない。

ただし、実世界での展開には人間の監視が必要であり、
リセット機構や報酬ラベリングの自動化が今後の課題となっている。

### 4.2 計算効率
計算効率の観点から、ResFiTは非常に優秀である。
従来のオンポリシー手法と比較して200倍のサンプル効率を実現し、
実世界での学習を現実的な時間内で可能にした。

高いUTD比により、データ収集よりもモデル更新がボトルネックになったため、
アクターとラーナーを別プロセスに分割することで対処した。
これにより、シーンリセット中もモデル更新を並行して実行できる。

### 4.3 応用可能性
ResFiTの応用可能性は非常に高い。
基底ポリシーに依存しない設計により、
様々なBCアーキテクチャ（拡散ポリシー、トランスフォーマー、
アクションチャンク等）に適用可能である。

特に、マルチタスク設定での応用が期待される。
タスク特有の残差改善を汎用的な基底ポリシーに蒸留することで、
より能力の高い汎用ポリシーを構築できる可能性がある。

ただし、学習された行動は基底ポリシーの周辺に制約される傾向があり、
根本的に異なる戦略やスキルの発見には限界がある。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、ロボット学習分野における重要な課題である
「BCの限界をいかにして克服するか」に対する実用的な解決策を提供している。
特に、実世界での高自由度ロボットシステムへの適用という
従来困難とされていた問題に取り組み、実際に成功を収めた点は高く評価される。

技術的には、事前訓練と微調整の段階を分離することで、
現在のロボット学習パラダイムにおける緊張関係を解決している。
行動チャンクや大規模ポリシーはBC性能を向上させるが、
RLには扱いにくい高次元行動空間を作り出す。
ResFiTは凍結された基底ポリシーの上で単一ステップ修正のみを学習することで、
長期間推論を維持しながら最適化を扱いやすく保っている。

### 5.2 今後の展望
今後の発展方向として、いくつかの興味深い可能性が考えられる。

第一に、凍結された基底制約を安定性を犠牲にすることなく
緩和する適切な方法の発見が重要である。
改良された統合ポリシーからより精密で信頼性が高く
高速な行動を基底ポリシーに蒸留できれば、
残差モデルがさらなる改善を行う余地を提供できる。

第二に、自動リセット機構、成功検出、安全レールなしでは、
サンプル効率の高いRL手法であっても人間の監視に依存しない
自律的なスキル改善のスケーリングは実現できない。
これらの要素の開発が実用化への重要な次のステップとなる。

最後に、マルチタスク設定での応用展開により、
タスク特有の改善を汎用的な基底ポリシーに統合する
継続的な学習システムの構築が期待される。
