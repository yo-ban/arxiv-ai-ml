# EXPO: Stable Reinforcement Learning with Expressive Policies

## 基本情報
- arXiv ID: 2507.07986v1 (https://arxiv.org/abs/2507.07986)
- 著者: Perry Dong (Stanford University), Qiyang Li (UC Berkeley), Dorsa Sadigh (Stanford University), Chelsea Finn (Stanford University)
- 所属: Stanford University, UC Berkeley
- 投稿日: 2025年07月11日
- カテゴリ: cs.LG

## 簡単に説明すると

EXPO（Expressive Policy Optimization）は、表現力豊かなポリシー（拡散モデルやフローマッチングポリシーなど）を用いて強化学習を安定的に実行するための新しい手法です。従来の手法では、これらの表現力豊かなポリシーを直接価値関数に対して最適化すると不安定になるという問題がありました。EXPOは、大きな表現力豊かなベースポリシーを模倣学習で訓練し、小さなガウシアン編集ポリシーでアクションを改善することで、この問題を解決します。最終的に、ベースポリシーと編集されたアクションの中から最もQ値の高いアクションを選択する「オンザフライ」ポリシーを構築します。この手法により、オフラインデータを活用したオンライン強化学習において、従来手法と比較して2-3倍のサンプル効率向上を実現しています。

## 1. 研究概要
### 1.1 背景と動機

ロボティクス分野では、大規模データセットを用いた模倣学習により、表現力豊かなポリシーを訓練することで、挑戦的な実世界タスクにおいて大きな進歩を遂げてきました。しかしながら、模倣学習手法は、大規模データセットにスケールアップしても、実世界の使用ケースに必要な高い信頼性とパフォーマンスを達成するのに苦労することがよくあります。

強化学習（RL）によるファインチューニングは、オンラインでの自己改善を通じて高いパフォーマンスを可能にすることで、原理的にはこの問題に対処できます。
しかし、既存のオンライン強化学習手法は通常、単純なガウシアンポリシー分布向けに設計されています。
そのため、模倣学習で一般的に使用される拡散ポリシーやフローマッチングベースのポリシーなどの表現力豊かな事前訓練ポリシーを効果的に活用できません。

表現力豊かなポリシーをオンラインRLでファインチューニングする際の主要な課題があります。
これらのポリシーは長い除雑音ステップのチェーンでパラメータ化されています。
そのため、価値関数に対してアクションを最適化しようとすると、アクション出力からポリシーパラメータへの安定した勾配伝播が妨げられるのです。

### 1.2 主要な貢献

本研究の主要な貢献は、表現力豊かなポリシークラスに対する安定的で効果的なオンラインRLファインチューニング手法であるEXPO（Expressive Policy Optimization）の提案です。
主な貢献として、以下の点が挙げられます。

- 表現力豊かなポリシーの直接的な価値最大化を回避する新しいアプローチを提案した。代わりに安定した教師あり学習目的で訓練されたベースポリシーと、Q値を最大化するための軽量な編集ポリシーを組み合わせる。
- オンザフライポリシーパラメータ化により、ベースアクションと編集されたアクションから高いQ値を持つアクションを選択する仕組みを導入した。
- 12の挑戦的なタスクにおいて、従来手法と比較して平均2-3倍のサンプル効率向上を実現した。
- 事前訓練なしでも高い性能を発揮し、オフラインからオンラインへの移行時のパフォーマンス低下を回避した。

## 2. 提案手法
### 2.1 手法の概要

EXPOは、表現力豊かなポリシーを用いてオンラインファインチューニングのサンプル効率を従来手法の2-3倍に向上させる2つの主要コンポーネントから構成されています。
第一のコンポーネントは、ベースポリシーから生成されたアクションを改良する編集ポリシーです。
これはQ値を最大化しながら探索的なアクションを促進します。
第二のコンポーネントは、オンザフライポリシーパラメータ化です。
これによりオリジナルと編集されたアクションの中から価値最大化アクションを選択します。

手法の核心的な洞察は、表現力豊かなポリシークラスの価値最大化についてです。
表現力豊かなポリシー自体の直接的な価値に対する最適化を回避することで、より効果的かつ安定的に行えます。
代わりに、安定した教師あり学習目的を使用してベース表現力豊かなポリシーを訓練します。
そして、2つのステップを通じて価値を最大化するオンザフライポリシーを構築します。

### 2.2 技術的詳細

EXPOアルゴリズムは以下の要素から構成されています。

1. ベースポリシー（π_base）: 拡散モデルなどの表現力豊かなポリシーであり、模倣学習（DDPM目的関数）で訓練される。オフラインデータセットとオンラインリプレイバッファの両方からサンプリングして継続的に更新される。

2. 編集ポリシー（π_edit）: ガウシアンポリシーであり、状態とベースポリシーからのアクションを入力として受け取り、アクション編集を出力する。編集されたアクションは ã = a + â となる。編集ポリシーは以下の損失関数で訓練される。
   L(π_edit) = -E[(Q_φ(s, a + â) - α log π_edit(â|s, a))]

3. オンザフライポリシー（π_OTF）: ベースポリシーからN個のアクションをサンプリングし、それぞれに対して編集を適用する。全ての候補（オリジナルと編集済み）の中から高いQ値を持つアクションを選択する。

4. 編集距離制約: 編集ポリシーの出力を[-β, β]の範囲にスケーリングすることで、編集されたアクションがオリジナルアクションから大きく離れないように制約する。βは探索の必要性に応じて小さく（例：0.05）または大きく（例：0.7）設定できる。

### 2.3 新規性

既存手法との主な違いとして、以下の点が挙げられます。

- 安定性の向上: 表現力豊かなポリシーを直接Q関数に対して最適化せず、安定した模倣学習目的で訓練する。これにより長い除雑音チェーンを通じた不安定な勾配伝播を回避する。
- オンザフライサンプリング: アクションサンプリングとTDバックアップの両方でオンザフライポリシーを使用する。Q関数の変化がエージェントの行動およびTD Q値ターゲットへ即座に反映される。
- 効果的な探索: 編集ポリシーにエントロピー正則化を適用する。これにより表現力豊かなポリシー単体では困難な、行動分布を超えた状態依存的なアクションノイズの追加が可能となる。
- 汎用性: 特定のポリシークラス（拡散、フローマッチングなど）に依存せず、任意の事前訓練ポリシーからファインチューニング可能である。

## 3. 実験結果
### 3.1 実験設定

実験は12の挑戦的な連続制御タスクで実施されました。
すべてのタスクはスパース報酬を特徴としています。
実験対象のタスクは以下の通りです。

- Antmaze（D4RL）: 4足歩行アリを制御して迷路をナビゲートし、目標位置に到達するタスク（medium、largeサイズ）
- Adroit（D4RL）: 28自由度ロボットハンドでペンの回転、ドアの開閉、ボールの移動を実施するタスク
- Robomimic: 7自由度Frankaロボットアームを制御する操作タスク（Lift、Can、Square）
- MimicGen: より複雑な操作タスク（threading、stack）

### 3.2 主要な結果

オンラインRL設定（事前訓練なし）において、EXPOはほぼすべてのタスクで既存のベースラインを上回るか、同等の性能を達成しました。
特に注目すべき結果は以下の通りです。

- RLPDと比較して、EXPOは一貫して2-3倍のサンプル効率を達成した（relocate-binary-v0を除く）
- IDQLやQSMなどの表現力豊かなポリシークラスを使用する手法は、事前訓練なしでは効果的に学習できないことが多い
- EXPOは表現力豊かなポリシークラスの力を活用して、より単純なポリシークラスよりも優れた性能を達成した

オフラインからオンラインRL設定において、EXPOは模倣学習のみでポリシーを事前訓練したにもかかわらず、ベースラインと比較して優れたサンプル効率と漸近性能を達成しました。
主な結果は以下の通りです。

- 従来のオフラインからオンラインRL手法とは異なり、EXPOはオフライン事前訓練からオンラインファインチューニングへの移行時にパフォーマンス低下を経験しない
- RobomimicとMimicGenタスクでは、EXPOは一貫してすべてのタスクで顕著な改善を示し、高いサンプル効率を実現した

### 3.3 既存手法との比較

主要なベースラインとの比較結果は以下の通りです。

- IDQL: オンライン探索にのみ暗黙的ポリシーを使用し、TDバックアップには暗黙的Q学習損失関数を使用する。多くのタスクでオンラインでの性能改善ができなかった。
- RLPD: より単純なガウシアンポリシーを使用するが、高いサンプル効率を持つ。EXPOはほぼすべてのタスクでRLPDを上回る。
- DAC: オフラインRLとして強力な事前訓練性能を示すが、オンライン訓練では急速に崩壊し、事前訓練モデルのファインチューニングには不適である。
- Cal-QL: 簡単なタスクでは強力な性能を示すが、より困難なタスクでは全体的なサンプル効率が低い。

## 4. 実用性評価
### 4.1 実装の容易性

EXPOの実装は比較的簡単で、以下の要素から構成されています。

- ベースポリシーは標準的な拡散モデル（DDPM）として実装
- 編集ポリシーは単純な3層MLPのガウシアンポリシー
- ハイパーパラメータは固定セットでよく機能し、主に編集ポリシーのβパラメータのみを調整

実装上の推奨事項として、良好なオフラインデータセットを持つタスクでは小さいβ値（0.05または0.1）から開始します。
一方、探索がより重要なタスクでは大きいβ値（0.5、0.7）から開始することが推奨されています。

### 4.2 計算効率

主な計算コストは以下の通りです。

- TDバックアップのために多くのアクションをサンプリングする必要がある。バッチ内のすべての例に対してこれらのアクションをサンプリングする必要があるため、計算的に高価である。
- 実験では、N=8個のアクションサンプルを使用した。
- UTD比は20に設定され、環境ステップごとに20回のグラディエント更新をした。

計算効率の改善は今後の課題として挙げられています。

### 4.3 応用可能性

EXPOは以下の点で高い応用可能性を持ちます。

- 汎用性: 特定のポリシーパラメータ化に依存せず、任意の事前訓練ポリシーからファインチューニング可能である。
- 実用的な前提条件: オフラインデータセットまたはポリシーを通じた合理的な事前知識を仮定する（多くの実用的な設定でこの仮定は成り立つ）。
- ロボティクスへの応用: 事前訓練された基盤モデルのファインチューニングに特に適している。
- 価値関数なしでの事前訓練: 多くの事前訓練ロボットモデルには事前訓練された価値関数が付属していない。そのため、ポリシーのみの事前訓練で動作するEXPOは実用的である。

## 5. まとめと所感
### 5.1 論文の意義

本論文は、表現力豊かなポリシーを用いた強化学習における長年の課題である安定した価値最大化の問題に対して、エレガントな解決策を提示しています。
直接的な価値最大化を回避し、代わりにオンザフライポリシー構築を通じて暗黙的に価値を最大化するというアプローチは、理論と実践の両面で優れた洞察です。

特に重要なのは、この手法が実際のロボティクスアプリケーションで重要な、事前訓練されたモデルのファインチューニングという文脈で設計されていることです。
多くの既存手法が価値関数の事前訓練を必要とする中、EXPOはポリシーのみの事前訓練で動作するため、より実用的です。

実験結果は説得力があり、特にオフラインからオンラインへの移行時にパフォーマンスが低下しない点は、実世界での展開において重要な利点です。

### 5.2 今後の展望

今後の研究方向として以下が考えられます。

- 計算効率の改善: 現在の主な制限である、TDバックアップでの多数のアクションサンプリングの計算コストを削減する方法の開発
- より困難な設定への拡張: 情報が限られた事前知識から始める設定への手法の適用
- 理論的分析: オンザフライポリシーの収束性や最適性に関する理論的保証の提供
- 他の表現力豊かなポリシークラスへの適用: 論文では主に拡散ポリシーで実験されている。フローマッチングやトランスフォーマーベースのポリシーなど、他の表現力豊かなポリシークラスでの検証が必要である。
- 実ロボットでの検証: シミュレーション環境で得られた優れた結果を実際のロボットシステムで再現する研究