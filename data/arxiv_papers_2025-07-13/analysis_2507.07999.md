# Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology

## 基本情報
- arXiv ID: 2507.07999v1 (https://arxiv.org/abs/2507.07999)
- 著者: Haochen Wang, Xiangtai Li, Zilong Huang 他9名
- 所属: NLPR MAIS CASIA, UCAS, ByteDance
- 投稿日: 2025年07月10日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると

TreeBenchは、大規模言語モデル（LLM）が「画像で考える」能力を評価するための新しいベンチマークです。
OpenAI-o3のような最新モデルは、画像内の特定領域を動的に参照しながら推論しますが、これまでそのような能力を包括的に評価するベンチマークは存在しませんでした。
TreeBenchは405の高品質な視覚的なQAペアで構成され、3つの原則に基づいて設計されています。
1つ目は複雑なシーン内の微細なターゲットの認識、2つ目はバウンディングボックスによる追跡可能な証拠の提供、3つ目は単純な物体認識を超えた二次的推論です。
さらに、TreeVGRという新しい学習手法を提案し、強化学習を用いて位置特定と推論を同時に監督することで、より正確で説明可能な視覚的推論を実現しています。
コードは https://github.com/Haochen-Wang409/TreeVGR で公開される予定です。

## 1. 研究概要
### 1.1 背景と動機

近年のLLMの発展により、OpenAI-o1やDeepSeek-R1のようなモデルはテキスト空間での推論で優れた性能を示しています。
しかし、知覚を必要とするタスクでは言語バイアスの蓄積により課題を抱えています。
OpenAI-o3は「画像で考える」という新しい方向性を開拓し、動的な領域参照を通じた視覚的根拠に基づく推論を実現しました。

しかし、既存のベンチマークには重大な制約があります。
古典的なベンチマーク（POPE、MMBench、SEED-Bench、MMMU）は細粒度の位置特定や検証可能な推論連鎖を見過ごしています。
V* Benchは単純な空間クエリに限定され、データ汚染のリスクもあります。
MME-RealWorldとHR-Benchは高解像度をサポートしますが、追跡可能な証拠と二次的推論が欠如しています。

これらの課題に対処するため、本研究では以下の問いに答えることを目指しました。
第一に、微妙な視覚的手がかりと複雑な推論を必要とする能力をどのように評価するか。
第二に、モデルに追跡可能で説明可能な推論経路を促すにはどうすればよいか。

### 1.2 主要な貢献

本研究の主要な貢献は以下の通りです。

- TreeBenchの提案：視覚的根拠に基づく推論を評価するための新しいベンチマークで、405の高品質VQAペアと専門家による詳細なアノテーションを含む
- 3つの基礎原則の確立：焦点を絞った視覚的知覚、追跡可能な証拠、視覚中心の二次的推論
- TreeVGRの開発：追跡可能な証拠を用いた強化学習による新しい学習パラダイムで、位置特定と推論を同時に改善
- 包括的な評価と分析：最先端モデルの性能評価と、追跡可能性が視覚的推論に与える影響の実証

## 2. 提案手法
### 2.1 手法の概要

本研究では、視覚的根拠に基づく推論の評価と改善のための2つの主要な貢献を提案しています。

TreeBenchは、画像で考える能力を診断的に評価するベンチマークです。
平均して画像の3.05%しか占めない微細なターゲットを複雑なシーンから識別します。
各質問にはバウンディングボックスのアノテーションが付与され、定量的な評価を可能にします。
10のカテゴリ（5つの知覚タスクと5つの推論タスク）で構成されています。

TreeVGRは、追跡可能な証拠を用いた視覚的推論の改善手法です。
2段階の学習パラダイムを採用し、まず35Kのサンプルでコールドスタート初期化を行います。
その後、37Kのサンプルを用いて追跡可能な証拠による強化学習を実施します。

### 2.2 技術的詳細

TreeBenchの構築プロセスは以下の通りです。

高品質画像の選定では、SA-1Bから密な物体を含む1K枚の画像を初期サンプリングしました。
8人のLMM専門家が各画像に対して質問、選択肢、答えを手動でアノテーションしました。
3段階の品質管理を経て、最終的に405の挑戦的なVQAペアを選定しました。

評価カテゴリは知覚タスクと推論タスクに分類されます。
知覚タスクには属性、材質、物理状態、物体検索、OCRが含まれます。
推論タスクには視点変換、順序付け、接触と遮蔽、空間的包含、比較が含まれます。

TreeVGRの学習アルゴリズムは以下の要素で構成されています。

コールドスタート初期化では、基本的な視覚推論の能力を獲得するため35KのSFTサンプルで学習します。
これにより、後続の強化学習のための安定した基盤を提供します。

デュアルIoU報酬システムは、精度と位置特定の両方を最適化します。
総報酬は正確性報酬、フォーマット報酬、IoU報酬（再現率と精度）の組み合わせです。
精度項はボックスの列挙を防ぎ、再現率項は完全な位置特定を保証します。

### 2.3 新規性

既存手法との主な違いは以下の通りです。

- 追跡可能な証拠の重視：バウンディングボックスによる中間推論ステップの明示的な監督により、説明可能性を向上
- 学習効率の向上：5エポックの学習でDeepEyes-7Bの32エポックと同等以上の性能を達成（学習時間を84%削減）
- デュアルIoU報酬の導入：精度と再現率のバランスを取る新しい報酬設計により、ボックス列挙を防ぎつつ完全な位置特定を実現
- 画像のクロッピングや再生が不要：テキスト空間での根拠付けで十分であることを実証

## 3. 実験結果
### 3.1 実験設定

評価は複数のベンチマークで実施しました。
主要な評価対象はTreeBench、V* Bench、HR-Bench-4K、MME-RealWorld-Liteです。
比較モデルにはクローズドソースの先進モデル（OpenAI-o3、Gemini-2.5-Pro、Claude-3.5-Sonnet）とオープンソースモデルを含みます。

TreeVGRの実装詳細は以下の通りです。
Qwen2.5-VL-7Bをベースモデルとして使用しました。
LLaMA-FactoryとEasyR1を用いて実装し、GRPOアルゴリズムで学習しました。
学習には8枚のH100 GPUを使用し、約10時間で完了しました。

### 3.2 主要な結果

TreeBenchでの評価結果は、最先端モデルでも困難なベンチマークであることを示しています。
OpenAI-o3は54.8%、Gemini-2.5-Proは54.1%の精度に留まりました。
TreeVGR-7Bは50.4%を達成し、すべてのオープンソースモデルを上回りました。
重要な発見として、mIoUと全体的な性能に正の相関があり、特に知覚タスクで顕著でした。

他のベンチマークでの性能向上も確認されました。
V* Benchでは91.1%を達成し、ベースモデルから16.8ポイントの向上を示しました。
HR-Bench-4Kでは77.1%（+5.0）、MME-RealWorld-Liteでは54.9%（+12.6）の改善を達成しました。
視覚中心のベンチマーク（MMVP）では75.3%を記録し、Qwen2.5-VL-72Bをも上回りました。

### 3.3 既存手法との比較

アブレーション研究により、各コンポーネントの重要性が明らかになりました。

コールドスタート初期化の効果を検証した結果、初期化なしでは性能が15ポイント以上低下することを確認しました。
デュアルIoU報酬の各項の寄与を分析した結果、精度項はボックス列挙を防ぎ、再現率項は完全な位置特定を保証することを確認しました。
視覚的根拠付けとテキストのみのRLを比較した結果、視覚的根拠付けが優れていることが実証されました。

## 4. 実用性評価
### 4.1 実装の容易性

TreeVGRは既存のLMMアーキテクチャで動作し、アーキテクチャの変更は不要です。
標準的なRL アルゴリズム（GRPO）と既存ツール（LLaMA-Factory、EasyR1）を使用します。
Qwen2.5-VL-7Bをベースとし、既存のインフラストラクチャを活用できます。
コードは https://github.com/Haochen-Wang409/TreeVGR で公開予定です。

### 4.2 計算効率

TreeVGRは競合手法と比較して学習時間を84%削減できます。
DeepEyes-7Bの32エポックに対し、わずか5エポックで同等以上の性能を達成しました。
8枚のH100 GPUで約10時間の学習時間で済み、実用的な計算コストです。
画像のクロッピングや再生が不要なため、推論時の計算効率も高いです。

### 4.3 応用可能性

本手法は幅広い応用が期待できます。

視覚的なQAシステムの改善により、より説明可能で信頼性の高いAIアシスタントを実現できます。
ロボティクスや自動運転への応用では、環境理解と意思決定の透明性向上に貢献します。
医療画像診断では、診断根拠の可視化により医師の意思決定を支援できます。
教育分野では、学習者に視覚的な説明を提供する知的チュータリングシステムに活用できます。

## 5. まとめと所感
### 5.1 論文の意義

本論文は、「画像で考える」という新しいパラダイムにおける重要な課題に取り組んでいます。
TreeBenchは、視覚的根拠に基づく推論能力を評価するための厳密な基準を提供し、現在の最先端モデルでも60%未満の精度しか達成できないことを明らかにしました。

特に重要なのは、追跡可能な証拠（バウンディングボックス）の導入により、モデルの推論過程を定量的に評価できるようになった点です。
これは単なる最終的な答えの正確性を超えて、推論の質と説明可能性を評価する新しい方向性を示しています。

TreeVGRの成功は、中間的な推論ステップの明示的な監督が性能向上に有効であることを実証しました。
デュアルIoU報酬システムは、精度と完全性のバランスを取る優れた設計であり、今後の研究の参考になるでしょう。

### 5.2 今後の展望

今後の研究方向として以下が考えられます。

モデルのスケーリングにより、32Bや72Bモデルへの適用で更なる性能向上が期待されます。
TreeBenchの拡張により、より多様なドメインとタスクタイプのカバーが必要です。
マルチターン推論への拡張により、反復的な視覚的根拠付けシナリオへの対応が可能になります。

長期的な研究機会としては、動画理解への応用が挙げられます。
時系列の視覚的根拠を追跡することで、動的なシーン理解を改善できます。
3Dシーン理解への拡張により、より豊かな空間的推論が可能になります。
人間参加型システムの構築により、インタラクティブな視覚的推論システムを実現できます。

本研究は、視覚的根拠に基づく推論の発展において重要な一歩であり、より解釈可能で信頼性の高いAIシステムの実現に向けた基盤となることが期待されます。