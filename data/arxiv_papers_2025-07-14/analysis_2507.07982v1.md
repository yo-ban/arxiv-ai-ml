# Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling

## 基本情報
- arXiv ID: 2507.07982v1 (https://arxiv.org/abs/2507.07982)
- 著者: Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, Jiang Bian
- 所属: Microsoft Research, Tsinghua University
- 投稿日: 2025年07月11日
- カテゴリ: cs.CV, cs.LG

## 簡単に説明すると
この論文は、ビデオ生成モデルに3D構造理解を組み込む「Geometry Forcing（GF）」という手法を提案しています。従来のビデオ拡散モデルは2D画像の連続として動画を生成しますが、実世界の動的な3D構造を捉えきれないという問題がありました。GFは、事前学習済みの3D基盤モデル（VGGT）の幾何学的特徴と、ビデオ拡散モデルの中間表現を整合させることで、この問題を解決します。プロジェクトページは https://GeometryForcing.github.io で公開されています。

## 1. 研究概要
### 1.1 背景と動機
現在のビデオ生成技術は、大規模なビデオデータセットを用いてリアルな視覚環境を生成することに成功していますが、重要な課題を抱えています。それは、ビデオが本質的に動的な3D世界の2D投影であるという事実を見落としていることです。既存のビデオ拡散モデルは、生のピクセル分布をモデル化することに焦点を当てているため、幾何学的な一貫性や長期的な整合性を維持することが困難でした。特に自己回帰的な設定では、小さな誤差が時間とともに蓄積し、非現実的な生成結果をもたらすことがあります。

研究チームは、事前学習済みの自己回帰ビデオ拡散モデル（DFoT）の中間特徴から深度マップを予測する実験をしました。
その結果、生のビデオデータのみで学習されたモデルが意味のある幾何学的表現を獲得できていないことを発見しました。この観察結果は、ビデオ拡散モデルと実世界の動的3D構造との間にギャップが存在することを示しています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです。
- ビデオ拡散モデルが生のビデオデータのみでは3D情報を暗黙的に学習できないことを実証
- 幾何学的表現との整合性を通じてビデオ拡散モデルに3D認識を組み込む「Geometry Forcing」手法の提案
- Angular AlignmentとScale Alignmentという2つの相補的な整合目的関数の導入
- カメラ視点条件付きビデオ生成（RealEstate10K）とアクション条件付きビデオ生成（Minecraft）での有効性の実証
- 長期的なビデオ生成における露出バイアスの軽減効果の確認

## 2. 提案手法
### 2.1 手法の概要
Geometry Forcing（GF）は、ビデオ拡散モデルの学習中に3D表現を内部化させるアプローチです。
この手法はセマンティック表現整合（REPA）の研究に着想を得ており、ビデオ拡散モデルの中間特徴を3D基盤モデル（VGGT）の幾何学的表現と整合させます。

VGGTは、1つまたは少数の2D視点から、カメラパラメータ、ポイントマップ、深度マップ、3Dポイントトラックなどの様々な3D属性を直接出力するフィードフォワードモデルです。GFは、VGGTのTransformerバックボーンから中間特徴を抽出し、これをビデオ拡散モデルの学習のための幾何学的な先行知識として活用します。

### 2.2 技術的詳細
GFの核心は、2つの相補的な整合目的関数にあります。

Angular Alignment（角度整合）は、拡散モデルの隠れ状態と目標特徴の間の方向的な対応を強制します。軽量なプロジェクタf_φを使用して拡散潜在表現をマッピングし、コサイン類似度を最大化することで整合を実現します。損失関数は、フレームレベルとパッチレベルの両方で独立に隠れ状態を整合させます。

**Scale Alignment（スケール整合）**は、幾何学的情報をエンコードする可能性のある特徴スケールを保持します。正規化された拡散隠れ状態から目標特徴のスケールを予測することで、スケール情報を保存します。具体的には、まずf_φ(h)を単位長に正規化し、別の軽量予測ヘッドg_φを使用して正規化された入力から完全な目標特徴を予測します。

全体の学習目的関数は、Flow Matching損失とこれら2つの整合損失の重み付き和として定義されます。

### 2.3 新規性
既存手法との主な違いは以下の点にあります。

第一に、GFは3D情報をビデオ拡散モデルに統合する際、外部条件として与えるのではなく、モデルの内部表現として学習させる点が新しいです。実験では、ControlNetを使用した外部ガイダンスアプローチと比較し、内部整合の方が優れた性能を示すことを確認しています。

第二に、角度とスケールの情報を分離して扱うことで、学習の安定性と表現力の両立を実現しています。直接的なMSE損失と比較して、この分離アプローチは最適化の不安定性やモデルの崩壊を防ぎます。

第三に、推論時に明示的な3D幾何を予測する能力により、ビデオと4D生成の統一的なアプローチを可能にしています。これにより、長期的なワールドモデリングのための構造化されたメモリとしての活用も期待されます。

## 3. 実験結果
### 3.1 実験設定
実験は2つの主要なタスクで実施されました。

**カメラ視点条件付きビデオ生成**では、RealEstate10Kデータセットを使用し、Diffusion Forcing Transformer（DFoT）にGFを適用しました。16フレームの256×256解像度のビデオで2,500イテレーション学習し、推論時には最初のフレームを条件として256フレームを生成しました。

**アクション条件付きビデオ生成**では、Minecraft環境でNext-Frame Diffusion（NFD）にGFを適用し、32フレームの384×224解像度のビデオで2,000ステップ学習しました。

評価指標としては、視覚品質を測るFVD、PSNR、SSIM、LPIPSに加え、幾何的整合性を評価するReprojection Error（RPE）とRevisit Error（RVE）を導入しました。

### 3.2 主要な結果
RealEstate10Kでの実験結果は、GFの優位性を明確に示しています。短期（16フレーム）および長期（256フレーム）の両方の生成設定において、GFは全てのベースライン手法を上回りました。特に注目すべきは、FVDスコアが364から243に改善された点です。

比較対象としたベースラインは、大規模画像-ビデオ拡散モデルのCosmos、DFoT、DINOv2特徴を使用したREPA、VideoMAEv2特徴を使用したVideoREPAでした。GFはこれら全ての手法に対して、FVD、LPIPS、SSIM、PSNRの全ての指標で優れた性能を示しました。

Minecraft環境でのアクション条件付きビデオ生成でも、GFは改善効果を示し、実世界とは大きく異なるデータ分布においても汎化能力があることを実証しました。

### 3.3 既存手法との比較
完全な360度回転を伴うカメラ視点条件付きビデオ生成の定性的比較では、GFの優位性がより明確になりました。ベースライン手法は時間的一貫性の維持に失敗し、開始視点に戻ることができませんでしたが、GFは一貫して開始視点を再訪しました。

ユーザースタディでは、カメラ追従、オブジェクトの一貫性、シーンの連続性の3つの観点から3D一貫性を評価しました。GFは全ての観点でベースライン手法を上回り、知覚的に一貫性のあるビデオを生成することが確認されました。

長期的なビデオ生成における露出バイアスの分析では、GFが100フレーム以降でFVDスコアを低減し、3D幾何学的表現の整合により長期的なドリフトとエラーの蓄積を効果的に軽減することが示されました。

## 4. 実用性評価
### 4.1 実装の容易性
GFは既存のビデオ拡散モデルアーキテクチャに最小限の変更で統合可能です。追加されるのは軽量なプロジェクタと予測ヘッドのみで、基本的なモデル構造は変更されません。実装では、λ_Angular = 0.5、λ_Scale = 0.05のデフォルト値を使用し、これらのハイパーパラメータは異なるタスクでも安定して機能することが確認されています。

学習時には事前学習済みのVGGTモデルから特徴を抽出しますが、VGGTの重みは固定されており、追加の3Dアノテーションは不要です。これにより、大規模なビデオデータセットへの適用が容易になります。

### 4.2 計算効率
GFの計算オーバーヘッドは限定的です。学習時には、VGGTからの特徴抽出と2つの整合損失の計算が追加されますが、これらは全体の計算コストに比べて小さいものです。8台のNVIDIA A100 GPUを使用した実験では、ベースラインと比較して学習時間の大幅な増加は観察されませんでした。

推論時には、GFで学習されたモデルは標準的なビデオ拡散モデルと同じ計算コストで動作します。
オプションで中間特徴から3D幾何を再構築できますが、必須ではありません。

### 4.3 応用可能性
GFの応用範囲は広く、以下のような分野での活用が期待されます。

**自律型エージェントのシミュレーション**：3D一貫性のあるビデオ生成により、より現実的な環境シミュレーションが可能になります。

**拡張現実（AR）/仮想現実（VR）**：幾何学的に正確なビデオ生成は、没入感のあるAR/VR体験の創出に貢献します。

**ロボティクス**：3D構造を理解したビデオ予測は、ロボットの行動計画や環境理解に活用できます。

**長期的なワールドモデリング**：明示的な3D幾何の再構築能力により、構造化されたメモリとしての活用が期待され、より長期的で一貫性のあるシミュレーションが可能になります。

## 5. まとめと所感
### 5.1 論文の意義
Geometry Forcingは、ビデオ拡散モデルと実世界の3D構造との間のギャップを埋める重要な一歩です。単純ながら効果的なアプローチにより、既存のビデオ生成技術の根本的な限界に対処し、より現実的で一貫性のあるビデオ生成を実現しています。

特に重要なのは、この手法が追加の3Dアノテーションを必要とせず、既存の大規模ビデオデータセットでの学習が可能な点です。これにより、実用的なスケーラビリティが確保されています。また、内部表現として3D構造を学習させるアプローチは、外部条件として与える方法よりも優れた性能を示し、深い幾何学的理解の重要性を示唆しています。

実験結果は、短期的および長期的なビデオ生成の両方で一貫した改善を示し、異なるドメイン（実世界とMinecraft）での汎化能力も実証されました。これは、提案手法の堅牢性と実用性を裏付けています。

### 5.2 今後の展望
論文では、現在の実装が比較的小規模なモデルとデータセットに限定されていることを制限事項として挙げています。
大規模モデルとデータセットでの学習により、GFの利点はさらに増幅される見込みです。

将来的な研究方向として、以下が考えられます。

**大規模3D一貫性ワールドシミュレータの構築**：より大きなデータセットでGFをスケールアップし、複雑な環境での長期的なシミュレーションを実現する。

**3D表現を永続的メモリとして活用**：再構築された3D幾何を構造化メモリとして使用し、超長期的なビデオ生成やインタラクティブなワールドモデリングに応用する。

**他のモダリティとの統合**：音声や触覚などの他のセンサーモダリティと組み合わせ、より豊かなマルチモーダル世界モデルを構築する。

**リアルタイムアプリケーション**：計算効率をさらに改善し、リアルタイムでの3D一貫性のあるビデオ生成を実現する。

この研究は、ビデオ生成技術の新たな方向性を示すものであり、今後の発展が大いに期待されます。