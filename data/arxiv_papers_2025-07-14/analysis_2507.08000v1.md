# Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models

## 基本情報
- arXiv ID: 2507.08000v1 (https://arxiv.org/abs/2507.08000)
- 著者: Helen Qu, Sang Michael Xie
- 所属: Flatiron Institute, Stanford University
- 投稿日: 2025年07月11日
- カテゴリ: cs.CL, cs.CV, cs.LG

## 簡単に説明すると
この論文は、CLIPやLarge Multimodal Models（LMMs）の性能が、事前学習データ内での概念の共起統計に強く依存することを明らかにしています。
単一の概念の頻度だけでなく、概念の組み合わせの出現頻度が重要であることを、相互情報量（PMI: Pointwise Mutual Information）という指標を用いて定量的に示しました。
例えば、「ほうき」という同じ概念でも、共起頻度の低い概念と組み合わせた場合は認識精度が低下します。
研究コードは https://github.com/helenqu/multimodal-pretraining-pmi で公開されています。

## 1. 研究概要
### 1.1 背景と動機
CLIPのような対照学習ベースの画像テキストエンコーダーは、大規模マルチモーダルモデル（LMMs）の重要な構成要素となっています。
これらのモデルは、ImageNet-RやObjectNetなどの挑戦的なデータセットで優れたゼロショット精度を示しています。
この結果は、大規模で多様な事前学習データセットが堅牢な汎化に十分である可能性を示唆しています。

最近の研究では、CLIP の精度が事前学習データ分布における特定の視覚概念の頻度と強く相関することが示されています。
しかし、概念の組み合わせの役割については十分に探求されていませんでした。
すべての概念の組み合わせに汎化するためには組み合わせ的に多くの事前学習例が必要となり、これは実現困難です。

既存の構成的汎化ベンチマークは、通常、事前学習中に見られなかった組み合わせのみをテストするか、事前学習データの役割を考慮しない一般的な評価にとどまっています。
本研究は、概念ペアの共起率とモデル精度の関係を詳細に調査することで、このギャップに対処しています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです。
- 概念の共起頻度と精度の相関を相互情報量（PMI）を用いて定量化する手法の提案
- 合成画像を用いた評価により、LAION-400Mで学習したCLIPモデルにおいてPMIと精度の間に強い相関（r=0.97）があることを実証
- 自然画像の編集により、低PMI概念の追加がCLIPの精度を最大10%低下させることを示す
- CLIPをベースとしたLMMs（LLaVA）でもPMIと精度の相関が持続することを確認（TextVQAでr=0.70、VQAv2でr=0.62）
- LMMsが高PMI概念を含む質問に対して「Yes」と答える傾向があることを発見

## 2. 提案手法
### 2.1 手法の概要
本研究では、CLIPとLMMsの構成汎化の能力を事前学習データの共起統計の観点から分析します。
まず、大規模画像テキストデータセットから概念を抽出し、概念ペアの共起頻度を定量化する手法を定義します。

概念集合Cは、画像キャプションのデータセットDから抽出された見出し語化された単語の集合として定義されます。
各概念の経験分布は、データセット内でその概念を含むキャプションの割合として計算されます。
同様に、概念ペア（c1, c2）の確率も定義されます。

### 2.2 技術的詳細
概念ペアの共起を測定するために、相互情報量（PMI）を使用します。
PMIは、2つの単語が共起する結合確率を、それらが独立に出現する場合の期待確率で正規化したものです。
これにより、個々の概念の頻度の影響を排除し、純粋に共起の強さを測定できます。

PMIの計算式は以下の通りです：
PMI(c1, c2) = log(P(c1, c2) / (P(c1) × P(c2)))

LAION-400Mデータセットから概念を抽出する際は、テキストキャプションを視覚概念の代理として使用します。
ストップワードを除去し、各単語を見出し語化した後、頻度が10,000以上の単語のみを保持して、最終的に21,718個のユニークな単語を概念集合としました。

### 2.3 新規性
本研究の新規性は以下の点にあります。

第一に、単一概念の頻度だけでなく、概念ペアの共起統計がモデル性能に与える影響を体系的に調査した点が新しいです。
PMIという正規化された指標を用いることで、個々の概念の頻度効果と共起効果を分離して分析できます。

第二に、合成画像生成と自然画像編集の両方のアプローチを用いて、PMIと精度の相関を実証的に検証しました。
特に、テキストから画像への拡散モデルを使用して、様々なPMI値を持つ概念ペアの制御された評価データセットを作成する手法は独創的です。

第三に、CLIPの振る舞いがLMMsに転移することを示し、視覚エンコーダーのバイアスが下流タスクに影響することを明らかにしました。
これは、CLIPベースのLMMsの限界を理解する上で重要な知見です。

## 3. 実験結果
### 3.1 実験設定
実験は3つの主要なタスクで実施されました。

**合成画像を用いたCLIP評価**では、GenPairsという新しいデータセットを作成しました。
各画像に少なくとも2つの概念を含み、そのうち1つがImageNetクラスとなるように設計されています。
Llama 3.1を使用してリアルなキャプションを生成し、Flux.1-devを使用して画像を生成しました。

**自然画像編集による評価**では、ImageNet検証セットの画像に小さな概念画像を貼り付けることで編集しました。
貼り付ける概念とターゲットクラスのPMIを制御することで、精度への影響を測定しました。

**LMM評価**では、LLaVA-1.5-7Bモデルを使用し、VQAv2とTextVQAベンチマークで評価しました。
質問と回答テキストから概念を抽出し、すべての概念ペアのPMI値の平均を計算してVQA精度との相関を分析しました。

### 3.2 主要な結果
合成画像実験では、LAION-400Mで学習したCLIP ViT-B/32において、PMIとゼロショット分類精度の間に非常に強い相関（r=0.97）が観察されました。
PMI値の下位5%と上位5%の画像間で14%の絶対精度差（30%の相対差）が確認されました。

自然画像の編集実験では、低PMI概念を含む小さな画像を貼り付けました。
その結果、CLIPのゼロショット分類精度が最大10%低下しました。
編集された自然画像でもPMIと精度の相関はr=0.75と高い値を維持しました。

LMM評価では、LLaVA-1.5においてもPMIと精度の相関の持続を確認しました。
TextVQAでr=0.70（精度差15%）、VQAv2のオープンエンド質問でr=0.62（精度差15%）の相関が観察されました。

### 3.3 既存手法との比較
モデルスケーリングの効果を調査した結果、より大きなCLIPモデル（ViT-B/32からEVA01-g/14まで）では精度差がわずかに改善するものの、PMIと精度の相関は持続することが分かりました。
最小モデルでの14.8%の精度差が最大モデルでは13.4%に減少しましたが、根本的な問題は解決されていません。

ファインチューニング実験では、編集画像を用いた拡張学習により、編集画像に対する精度差を10%から1%へと減少させました。
しかし、この改善は他のデータセット（GenPairs）には転移せず、異なる側面の構成的汎化をテストしている可能性が示唆されました。

特筆すべき発見として、LMMsは高PMI概念を含む質問に対して「Yes」と答える傾向を示しました。
VQAv2データセットで正解が「Yes」の質問において、質問内の概念のPMIが高いほど、モデルが正しく「Yes」と答える確率が高くなりました（r=0.77）。

## 4. 実用性評価
### 4.1 実装の容易性
提案手法は既存のCLIPやLMMsの評価に容易に適用できます。
PMI計算のための概念抽出パイプラインは、標準的な自然言語処理ツール（nltk）を使用して実装されており、再現性が高いです。

合成画像生成にはLlama 3.1とFlux.1-devを使用していますが、これらは公開されているモデルであり、HuggingFaceの実装を通じて簡単にアクセスできます。
自然画像の編集手法も単純な画像貼り付け操作であり、実装の複雑さは低いです。

### 4.2 計算効率
PMI計算は事前に一度だけ実行すればよく、評価時の計算オーバーヘッドは最小限です。
LAION-400Mデータセットからの概念抽出とPMI計算は計算集約的ですが、結果はキャッシュして再利用できます。

合成画像生成はFlux.1-devの推論コストがかかりますが、評価データセットは一度生成すれば繰り返し使用できます。
自然画像編集は単純な操作であり、リアルタイムで実行可能です。

### 4.3 応用可能性
本研究の知見は、以下のような応用が考えられます。

**データセット設計**：PMI分析を用いて、より均衡な概念組み合わせを持つ事前学習データセットを設計できます。

**モデル評価**：既存のベンチマークをPMIの観点から再評価し、真の構成汎化の能力を測定できます。

**敵対的攻撃**：低PMI概念の追加による精度低下を利用した、新しいタイプの敵対的攻撃の可能性があります。

**バイアス検出**：LMMsの出力バイアス（高PMI入力に対する「Yes」傾向など）を検出し、緩和する手法の開発に活用できます。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、CLIPとLMMsの構成汎化の限界を定量的に明らかにした重要な貢献です。
単に「見たことがない組み合わせ」に弱いというだけでなく、その弱さが事前学習データの共起統計によって予測可能であることを示しました。

特に重要なのは、この問題がモデルスケーリングでは根本的に解決されないという発見です。
より大きなモデルでもPMIと精度の相関は持続し、精度差の改善もわずかでした。
これは、アーキテクチャやアルゴリズムレベルでの根本的な改善が必要であることを示唆しています。

また、CLIPのバイアスがLMMsに転移するという発見は、視覚エンコーダーの品質が下流タスクに与える影響の重要性を強調しています。
LMMsの開発において、言語モデル部分だけでなく、視覚エンコーダーの改善も重要であることが示されました。

### 5.2 今後の展望
本研究は、マルチモーダルモデルの構成汎化を改善するための新しい方向性を示唆しています。

**アルゴリズム開発**：PMIベースの重み付けや正則化手法により、低頻度の概念組み合わせに対する汎化を改善する学習アルゴリズムの開発が期待されます。

**アーキテクチャ改良**：概念を明示的に分解し、組み合わせる能力を持つ新しいアーキテクチャの研究が必要です。

**データ中心アプローチ**：PMI分析を活用し、低頻度の概念組み合わせを優先的に収集するデータ拡張戦略の開発が可能です。

**評価手法の改善**：PMIを考慮した新しいベンチマークの設計により、モデルの真の汎化能力をより正確に評価できるようになるでしょう。

この研究は、事前学習モデルの限界を理解し、より堅牢で汎用的なマルチモーダルAIシステムの開発に向けた重要な一歩となっています。