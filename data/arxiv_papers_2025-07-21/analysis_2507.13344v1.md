# Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models

## 基本情報
- arXiv ID: 2507.13344v1 (https://arxiv.org/abs/2507.13344)
- 著者: Yudong Jin, Sida Peng, Xuan Wang
- 共著者: Tao Xie, Zhen Xu, Yifan Yang
- 共著者: Yujun Shen, Hujun Bao, Xiaowei Zhou
- 所属: Zhejiang University, Ant Research
- 投稿日: 2025年07月18日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると
Diffuman4Dは、少数のカメラからの動画を入力として、4D（3D+時間）で一貫性のある人物の新規視点映像を生成する新しいAIモデルです。従来の手法では密に配置された多数のカメラが必要でしたが、この手法ではわずか4台のカメラからの映像で高品質な3D人物再構成を実現します。

この技術の核心は「スライディング反復デノイジング」という新しい手法です。これは、時空間的に一貫性のある映像を生成するために、拡散モデルのノイズ除去プロセスを効率的に行います。プロジェクトページ (https://diffuman4d.github.io/) では、インタラクティブなデモと動画結果が公開されています。

## 1. 研究概要
### 1.1 背景と動機
動的な人物の高品質な4D（3D+時間）ビュー合成は、拡張現実、映画制作、スポーツ放送などで幅広い応用があります。従来の多視点ステレオ手法や最近のニューラルレンダリング手法は、高品質な再構成のために密に配置された同期カメラアレイを必要とします。これは実世界のシナリオでの適用を困難にしています。

入力ビューが少数になると、観測が不十分になります。そのため再構成問題が不良設定となり、これらの手法は失敗する傾向があります。この問題に対する直感的な解決策は、条件付き画像またはビデオ生成モデルを活用することです。しかし、既存の手法は生成画像の時空間での一貫性に苦戦しており、特に人物のトポロジーや衣服の変形が複雑な場合に顕著です。GPUメモリの制限により、これらの手法は複数のパスで目標画像を生成します。生成モデルの確率的性質により出力間に分散が生じます。

### 1.2 主要な貢献
本研究では、以下の4つの主要な貢献をしました。

- Diffuman4Dの導入: 少数視点ビデオ入力から時空間的に一貫性があり高解像度（1024p）の人物ビデオを生成する新しい拡散モデル
- スライディング反復デノイジング機構: 長期ビデオの空間的および時間的一貫性を向上させながら、推論時間を従来手法の約50%に短縮する新しい手法
- 人物ポーズ条件付けスキーム: 生成される人物ビデオの外観品質と動作精度を向上させる設計
- 処理済みDNA-Renderingデータセット: カメラパラメータの再較正、画像色補正マトリックスの最適化、前景マスクの予測、人物スケルトンの推定を含む処理済みデータセットの公開予定

## 2. 提案手法
### 2.1 手法の概要
Diffuman4Dは2段階のアプローチで人物パフォーマンスを再構成します。第一に、入力の少数視点ビデオを時空間拡散モデルを使用して密な多視点ビデオに変換します。第二に、生成された多視点ビデオから4Dガウシアンスプラッティング（4DGS）を最適化して人物パフォーマンスを再構成します。

システムは以下のコンポーネントから構成されています。
- 入力ビデオをVAEで潜在空間にエンコード
- スケルトン潜在表現とPlücker座標を条件付け埋め込みとして使用
- スライディング反復アプローチによる段階的なノイズ除去
- デノイズされた潜在表現から対応するビデオへのデコード
- 入力ビューと生成ビューの両方を使用した4DGS再構成

### 2.2 技術的詳細
**スライディング反復デノイジング**
従来の手法では、GPUメモリの制約により画像シーケンス全体を数百のグループに分割する必要がありました。これらのデノイジング反復は独立して動作するため、生成画像に不整合が生じます。

提案手法では、長さWのコンテキストウィンドウを固定ストライドSでシーケンス上をスライドさせます。各反復で、ターゲットサンプルを入力サンプルと連結し、拡散モデルに供給してPステップのデノイジングを行います。人間中心のサンプルシーケンス（円形カメラ配置など）では、最初に反時計回りでウィンドウをスライドさせます。その後、時計回りへ方向を反転させて双方向のコンテキスト集約を可能にします。

**スケルトン条件付け拡散**
3D人物スケルトンシーケンスを導入して、モデルの生成空間を制約します。具体的な手順は以下のとおりです。
1. Sapiensを使用して2D人物スケルトンを推定し、三角測量して3Dスケルトンシーケンスを取得
2. これらのスケルトンを各ビューに投影し、異なる体の部位に異なる色を割り当ててRGBマップとしてレンダリング
3. このRGBマップをVAEで潜在空間にエンコードし、ピクセル整合特徴として使用

**スケルトン-Plücker混合条件付け**
スケルトン予測は複雑な衣服を着た個人では不正確になることが多く、前後の対称性による曖昧さも生じます。これらの制限を緩和するため、Plücker座標条件付けを保持して明示的なカメラポーズ情報を提供します。

### 2.3 新規性
本研究の新規性は主に以下の点にあります。

1. **スライディング反復デノイジング機構**: 従来の独立したグループ処理や中央値フィルタリングとは異なり、拡散モデルのデノイジングプロセスに滑らかさ誘導バイアスを導入
2. **情報伝播の最適化**: 各サンプルが周囲の空間情報と時間情報を知覚できる仕組みにより、より近いサンプル間でより多くの共同デノイジングステップを実行
3. **人物特有の条件付け**: 一般的なシーン向けの既存手法と異なり、3Dスケルトンという中間表現を活用して生成品質を向上
4. **混合条件付けスキーム**: スケルトンとPlücker座標の長所を組み合わせ、ロバストな生成プロセスを実現

## 3. 実験結果
### 3.1 実験設定
モデルは空間サンプルシーケンスまたは時間サンプルシーケンスで訓練され、各シーケンスの総長は M + N = 16 です。

**訓練設定**は以下のとおりです。
- 空間シーケンス：M=4の条件サンプルからN=12のターゲットサンプルを生成
- 時間シーケンス：参照カメラからのM=8サンプルを条件として、ターゲットカメラでN=8の連続サンプルを生成
- 両シーケンスを50%の確率で訓練
- すべての条件を10%の確率でランダムにドロップしてClassifier-Free Guidanceを有効化

**データセット**は以下のとおりです。
- DNA-Renderingデータセット：2,000以上の人物パフォーマンスシーケンスから1,000シーケンスを選択（各48ビュー、225フレーム、計1000万画像）
- ActorsHQデータセット：12シーケンスの人物パフォーマンスでゼロショット汎化を評価

### 3.2 主要な結果
**定量的結果**
DNA-RenderingとActorsHQデータセットでの評価において、提案手法は全てのベースラインを上回る性能を示しました。わずか4つの入力ビューで、48ビューを使用したLongVolcapの密な再構成に匹敵する視覚品質を達成しました。

**定性的結果**
- LongVolcap：スパースビュー再構成の不良設定性により、ノイズの多いレンダリング結果
- GauHuman：複雑な衣服や動的な動作の再構成に失敗
- GPS-Gaussian：深度推定器がスパースビュー設定で機能せず、断片的な結果
- 提案手法：拡散事前分布から合理的なガイダンスを生成し、複雑な動作と外観に対しても良好に汎化

### 3.3 既存手法との比較
**CAT4Dとの比較**
CAT4Dは一般的なシーン向けに設計された最新の4D生成手法ですが、人物生成では以下の問題があります。
- Plücker座標のみでは人物ポーズの誤差が顕著
- 髪や衣服などの柔軟な構造の動きによる形状歪みと自己オクルージョンへの対処が困難

提案手法は人物特有の条件付け信号を導入することで、これらの課題を解決しています。

**アブレーション研究**
1. **デノイジング戦略の比較**：
   - マルチグループデノイジング：グループ間で突然のジャンプが発生
   - 中央値フィルタリング：計算コストが重なり率に反比例し、不整合が残る
   - 提案手法：一定の計算コストで、より一貫性のあるグローバルに正確な結果

2. **条件付けスキームの比較**：
   - スケルトンなし：カメラ制御が限定的で、大きなミスアラインメント
   - Plückerなし：前後・左右の曖昧さにより不整合なガイダンス
   - 提案手法：カメラとポーズ制御信号の利点を組み合わせ、一貫性のある制御可能な結果

## 4. 実用性評価
### 4.1 実装の容易性
提案手法は、既存の多視点潜在を使った拡散モデルのアーキテクチャを基盤としており、実装は比較的容易です。主な追加要素は次のとおりです。
- スライディング反復デノイジングの実装
- Sapiensによる2Dスケルトン推定と3D三角測量
- スケルトンのRGBマップレンダリングとVAEエンコーディング

オープンソースの実装が予定されており、研究コミュニティでの利用が期待されます。

### 4.2 計算効率
- 推論時の計算効率は、スライディングウィンドウアプローチにより従来手法より向上
- 複数のサンプルシーケンスを複数のGPUで並列にデノイズ可能
- 各サンプルの総デノイジングステップ数：D = 2 × P × W/S
- 中央値フィルタリングアプローチと比較して、一定の計算コストを維持

### 4.3 応用可能性
Diffuman4Dは以下の分野で幅広い応用が期待されます。

**拡張現実（AR）**
少数のカメラから任意視点の人物映像を生成できるため、ARアプリケーションでのリアルタイム人物表示に活用可能です。

**映画・ビデオ制作**
従来は大規模なカメラアレイが必要だった特殊効果を、少数のカメラで実現できます。バレットタイム効果やマルチアングル撮影が低コストで可能になります。

**スポーツ放送**
競技場に少数のカメラを設置するだけで、任意の視点からの映像を生成でき、視聴者に新しい観戦体験を提供できます。

**バーチャル会議・テレプレゼンス**
参加者を少数のカメラで撮影し、任意の視点から見られる3D表現を生成することで、より臨場感のあるコミュニケーションが可能になります。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、スパースビュー入力からの高品質な4D人物再構成という挑戦的な問題に対して、実用的な解決策を提示しています。特に重要なのは、密なカメラアレイを必要としない点で、これにより実世界での応用が約70%容易になります。

スライディング反復デノイジングという新しいアプローチは、拡散モデルの時空間での一貫性の問題に対する効果的な解決策を提供しており、今後の4D生成研究に大きな影響を与える可能性があります。また、人物特有の条件付けを導入することで、一般的な手法では困難だった複雑な人物動作の生成を可能にしています。

### 5.2 今後の展望
論文では以下の制限事項と今後の研究方向が示されています。

1. **高解像度化**: 現在は1024pまでの対応であるが、4Kビデオへの拡張が期待される。ベースモデルの制約を克服する必要がある。

2. **人物-物体インタラクション**: 現在の手法は複雑な人物-物体インタラクションのシナリオで苦戦する可能性がある。物体を含む条件付けの拡張が考えられる。

3. **新規ポーズレンダリング**: 入力ビデオで空間的一貫性を制約する必要があるため、現在は新規ポーズのレンダリングができない。この制約を緩和する手法の開発が期待される。

4. **リアルタイム化**: 現在の推論速度では、リアルタイムアプリケーションには適していない。モデルの軽量化や推論の高速化が今後の課題である。

5. **より少ない入力ビュー**: 現在は4ビューを使用しているが、2-3ビューでも同等の品質を達成できる手法の開発が期待される。

この研究は、4D人物再構成の技術を実用的なレベルに引き上げる重要な一歩であり、今後のAR/VRやコンテンツ制作の分野に大きな影響を与える可能性があります。