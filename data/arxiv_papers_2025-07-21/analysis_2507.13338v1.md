# Training Transformers with Enforced Lipschitz Constants

## 基本情報
- arXiv ID: 2507.13338v1 (https://arxiv.org/abs/2507.13338)
- 著者: Laker Newhouse, R. Preston Hess, Franz Cesista, Andrii Zahorodnii, Jeremy Bernstein, Phillip Isola
- 所属: MIT CSAIL, MIT BCS, Independent
- 投稿日: 2025年07月18日
- カテゴリ: cs.LG, cs.AI

## 簡単に説明すると
この論文は、Transformerモデルに対して、訓練中にLipschitz定数を強制的に制限する新しい手法を提案しています。
Lipschitz定数とは、入力の微小な変化に対する出力の変化の上限を表す指標です。
通常のTransformerは入力の摂動に非常に敏感で、敵対的攻撃への脆弱性や訓練の不安定性などの問題を抱えています。
この研究では重み行列のノルムを効率的に制約することで、これらの問題を解決しようとしています。

特に注目すべきは、新しいオプティマイザーであるMuonを使用することで、従来のAdamWよりも優れたLipschitz制約と性能のトレードオフを実現できることを発見した点です。
また、「spectral soft cap」と「spectral hammer」という2つの新しい重み制約手法を提案し、最大145Mパラメータのモデルで安定した訓練を実現しています。
実装コードはGitHub (https://github.com/Arongil/lipschitz-transformers) で公開されています。
データはHugging Face (https://huggingface.co/phess2/lipschitz-transformers) で利用可能です。

## 1. 研究概要
### 1.1 背景と動機
深層学習モデル、特にTransformerは、入力の微小な摂動に対して非常に敏感であることが知られています。この敏感性は、敵対的攻撃への脆弱性、訓練の発散、過学習などの病理的な問題につながります。Lipschitz定数は、モデルの入力に対する感度の上限を示す指標であり、これを制御することで、より頑健で安定したモデルを構築できる可能性があります。

過去の研究では、MLP、RNN、GANなどのアーキテクチャに対してLipschitz制約を適用する手法が提案されてきました。
しかし、Transformerのような現代的なアーキテクチャに対しては、初期化段階を超えてLipschitz証明書を強制する成熟した技術がありませんでした。
特に、大規模なTransformerの訓練では、アテンションや出力ロジットが過度に大きくなることによる不安定性が問題となっています。
これまでは、QKノームやlogit tanh softcappingなどの対症療法的な手法が使われてきました。

本研究は、2つの問いに答えることを目的としています。
第一に「小さな強制Lipschitz境界を持つTransformerは良好な性能を発揮できるか」です。
第二に「重み制約手法はLipschitzと性能のトレードオフにどのように影響するか」です。

### 1.2 主要な貢献
この研究の主要な貢献は以下の3点にまとめられます。

第一に、145Mパラメータまでのスケールで、強制Lipschitz制約を持つTransformerの訓練を実現しました。
具体的には、10未満のLipschitz定数を持つTransformerがFineWeb10Bインターネットテキストで21%の精度を達成しました。
また、2未満のLipschitz定数を持つTransformerがシェイクスピアテキストで60%の精度を達成しています。
これは、Transformer訓練における完全な重み行列制約の実現可能性を示す重要な成果です。

第二に、Muonオプティマイザーを使用することで、AdamWと比較して重み減衰とスペクトル正規化がより大きな利益をもたらすことを実証しました。Muonを使用することで、より小さなLipschitz境界で同等の精度を達成でき、標準的な頑健性特性も保持されることを確認しています。

第三に、「spectral soft cap」と「spectral hammer」という2つの新しい重みノルム制約技術を導入しました。
AdamWのための重み正規化手法の中では、spectral hammerが最も競争力のあるLipschitz制約性能を引き出します。
Muonに対しては、spectral soft capが重みノルムを境界づけることを証明しました。
また、スペクトル正規化と同等またはわずかに優れた性能を示すことを発見しました。

## 2. 提案手法
### 2.1 手法の概要
提案手法の核心は、Transformerの各層における重み行列のスペクトルノルム（最大特異値）を効率的に制約することです。これにより、モデル全体のLipschitz定数を所望の範囲内に保ちながら、高い性能を維持することを目指しています。

手法は大きく3つの要素から構成されています。
第一に、残差接続とself-attentionメカニズムの再パラメータ化です。
従来の残差接続 x + block(x) の代わりに、(N-1)/N · x + 1/N · block(x) という凸結合を使用します。
ここで、Nは層数を表します。
また、attentionの計算では、通常の1/√d スケーリングの代わりに1/dスケーリングを使用します。
具体的には、softmax(QK^T/d)V という形式を採用しています。

第二に、重みノルム制約手法の選択です。
本研究では、重み減衰、スペクトル正規化、Stiefel多様体射影などの既存手法を検討しています。
これらに加えて、新たに提案するspectral soft capとspectral hammerを含む7つの手法を比較検討しています。

第三に、オプティマイザーの選択です。特に、Muonオプティマイザーは重み更新のスペクトルノルムが学習率によって制限されるという特性を持ち、これが厳密な重み制約の実現に有利に働くことを発見しました。

### 2.2 技術的詳細
提案手法の技術的な核心は、特異値分解（SVD）を使用せずに効率的に重みのスペクトルノルムを制約する点にあります。
これは、奇数次多項式の反復を使用して実現されます。

Spectral Soft Capは、Muonの高い安定ランク更新に合わせて設計された手法です。
この手法は、すべての特異値に対して並列に σ → min(σ_max, σ) という写像を滑らかに近似します。
具体的には、p₂(p₁(x)) という合成を使用します。
ここで p₁(x) = x - αx³、p₂(x) = x + αx³ であり、α ≥ 0は強度パラメータです。
この手法の重要な特性は、σ << σ_max のときには特異値をほとんど減衰させません。
一方、σ = σ_max のときは、Muonの既知の更新ノルムに対抗できる強度で減衰させます。

論文では、Spectral Soft Capがスペクトルノルムを境界づけることを定理として証明しています（定理3.1）。
所望の最大スペクトルノルム σ_max、学習率 η、重み減衰 λ、境界ノルム ||W||* ≤ σ_max を持つ重み行列が与えられたとします。
このとき、Muonの更新ステップに続くspectral soft capが ||W||* ≤ σ_max を保持するような最小の α ≥ 0 が存在します。

Spectral Hammerは、AdamWの低い安定ランク重み更新により適した手法です。
この手法は、最大の特異値を σ_max に設定することで動作します。
具体的には、W → W + (σ_max - σ₁)u₁v₁^T という写像を適用します。
ただし、複数の特異ベクトルが更新ごとに増加する可能性があります。
そのため、スペクトルノルムが σ_max 以下に留まることは保証されません。

### 2.3 新規性
本研究の新規性は、主に以下の3点に集約されます。

第一に、Transformerアーキテクチャに対して、訓練全体を通じてLipschitz制約を強制する初めての実用的な手法を提案したことです。既存のLipsFormerは初期化時のみの制約に留まっていましたが、本手法は訓練中継続的に制約を維持します。

第二に、Muonオプティマイザーの特性（重み更新が小さく既知のスペクトルノルムを持つ）を活用した新しい重み制約設計パラダイムを提示しました。これにより、AdamWでは実現困難だった厳密な重み制約が可能になりました。

第三に、層正規化、QKノーム、logit tanh softcappingなどの安定化手法を一切使用せずに、大規模Transformerの安定した訓練を実現しました。これは、Lipschitz制約が訓練の安定性に対する根本的な解決策となる可能性を示唆しています。

## 3. 実験結果
### 3.1 実験設定
実験は3つの段階で構成されています。
初期段階では、CIFAR-10データセットを用いた3層MLP（隠れ次元256）で各重み制約手法の基本的な特性を評価しました。
次に、シェイクスピアテキストデータセット上で2Mパラメータの小規模Transformerを訓練し、手法の有効性を検証しました。
最終段階では、NanoGPT speedrunベンチマークを使用しました。
145MパラメータのTransformerをFineWeb10Bインターネットテキストデータセット上で訓練しました。

すべての実験において、RMS→RMS演算子ノルムに関してLipschitz境界を計算しています。
これはスペクトルノルムの再スケール版です。
MLPの場合、Lipschitz境界は重みのRMS→RMSノルムの積として計算されます。
Transformerの場合は、各層の活性化ノルムの境界を考慮したより複雑なアルゴリズムを使用しています。

オプティマイザーとしては、AdamWとMuonの両方を比較検討しました。
7つの異なる重み制約手法を評価しました。
これらは、重み減衰、スペクトル重み減衰、スペクトル正規化、Stiefel多様体射影です。
さらに、spectral soft cap、spectral hard cap、spectral hammerも評価しました。

### 3.2 主要な結果
CIFAR-10での実験結果は、MuonがAdamWよりも一貫して優れたLipschitz対性能のトレードオフを達成することを示しています。
具体的には、同じ検証精度45%を達成するのに、Muon + spectral soft capではLipschitz境界が15.2でした。
一方、AdamW + 重み減衰では7618.8という非常に高い値となりました。

シェイクスピアテキストでの実験では、2未満のLipschitz境界を持つTransformerが検証損失1.29を達成しました。
これは、ベースライン（AdamW、層正規化あり）の1.47を上回る結果です。
このモデルは次元256、深さ3で、2000ステップの訓練で達成されました。
最良の検証損失は、6.02未満のLipschitz境界を持つTransformerで1.20でした。

NanoGPTスケールでの実験では、σ_max=1の設定で10-Lipschitz Transformerが21.2%の検証精度を達成しました。
このモデルは層正規化、QKノーム、logit tanhを使用せずに安定して訓練されました。
σ_max=16の設定では、ベースラインと同等の39.4%の精度を達成しました。
しかし、Lipschitz境界は10^264という天文学的な値になりました。

重要な観察として、Lipschitz制約されたモデルは、ベースラインと比較して活性化の最大値が顕著に小さいことが判明しました。
393Kトークンのバッチで、非Lipschitzベースラインの最大活性化エントリが148,480でした。
一方、10^264-Lipschitz Transformerでは160でした。

### 3.3 既存手法との比較
LipsFormerとの主要な違いは、本手法が訓練全体を通じて重み制約を強制する点です。
LipsFormerは初期化時のみの制約に留まり、訓練中の重み行列は制約されません。
実験では、LipsFormerが10^130のLipschitz境界で30.1%の精度を達成しました。
一方、本手法は10のLipschitz境界で21.2%、10^134の境界で36.2%の精度を達成しました。

敵対的頑健性の評価では、Lipschitz制約されたネットワークがより大きな摂動に対して耐性を示すことを確認しました。
CIFAR-10テストセット2000画像に対して評価しました。
ℓ2摂動バジェットεが増加してもLipschitz制約モデルはより高い精度を維持しました。
また、正解クラスの確率もより緩やかに低下しました。

重み制約手法の比較では、Muonと組み合わせた場合の結果が注目されます。
spectral normalization、spectral soft cap、spectral hard capがLipschitz対損失のトレードオフのフロンティアを定義しました。
特に、これらの手法はベースライン（Muon + 重み減衰）の1%以内の検証精度に到達しました。
同時に、より低いLipschitz境界を実現しました。

## 4. 実用性評価
### 4.1 実装の容易性
提案手法の実装は比較的容易です。
主要なコンポーネントは、パワー反復法による最大特異値の計算と、奇数多項式の適用という標準的な線形代数演算に基づいています。
GitHubで公開されているコードは、PyTorchを使用した明快な実装を提供しています。
コードは https://github.com/Arongil/lipschitz-transformers で入手できます。

特に、spectral soft capとspectral hammerの実装は、既存の深層学習フレームワークの標準的な演算のみを使用しています。
特殊なハードウェアや複雑な最適化は不要です。
ただし、各訓練ステップで追加の計算が必要となります。
そのため、標準的な訓練と比較して若干のオーバーヘッドが発生します。

既存のTransformerコードベースへの統合も比較的簡単です。
主な変更点は3つあります。
残差接続の再パラメータ化、attention計算のスケーリング変更、各訓練ステップ後の重み制約の適用です。
層正規化の除去により、実装はむしろシンプルになる側面もあります。

### 4.2 計算効率
計算効率の観点では、提案手法にはトレードオフが存在します。
各訓練ステップで重み制約を適用するための追加計算が必要となります。
特にパワー反復法による最大特異値の計算がオーバーヘッドとなります。

実験では、小規模なモデルでは訓練時間の増加は限定的でした。
NanoGPTスケールの実験では、ベースラインが0.7Bトークンで訓練を完了しました。
一方、同等の性能を達成するために2.8Bトークンが必要でした。
ただし、これは主に収束速度の違いによるもので、単純な計算オーバーヘッドだけではありません。

一方で、Lipschitz制約されたモデルは活性化値が小さいという特性を持ちます。
そのため、低精度演算での訓練や推論に適している可能性があります。
論文では、線形層の行列乗算にfp8精度を使用しても安定した訓練が可能であることを示しています。

### 4.3 応用可能性
提案手法は幅広い応用可能性を持っています。
最も直接的な応用は、敵対的攻撃に対する頑健性が要求される分野です。
セキュリティクリティカルなアプリケーションや、信頼性が重要な医療診断システムなどで有用です。

差分プライバシーの文脈でも重要な応用があります。
重みに慎重に調整されたノイズを加える必要がある差分プライベート訓練アルゴリズムの設計があります。
この設計において、Lipschitz制約は安定性を保証する重要な要素となります。

大規模言語モデルの訓練における損失スパイクの問題に対しても、根本的な解決策を提供する可能性があります。
現在の1Tパラメータスケールの訓練では、MuonClipオプティマイザーのような対症療法的アプローチが使用されています。
これらの代わりに、Lipschitz制約による原理的なアプローチが有効となる可能性があります。

さらに、低精度訓練と推論への応用も期待されます。
活性化値が小さく保たれるという特性は、量子化やモデル圧縮との相性が良いです。
これは、エッジデバイスでの展開に有利です。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、Transformerの訓練における根本的な問題に対して、理論的に裏付けられた実用的な解決策を提示している点で非常に意義深いものです。
Lipschitz制約という数学的に明確な概念を用いています。
これにより、訓練の安定性、敵対的頑健性、一般化性能といった複数の問題に同時にアプローチしています。

特に印象的なのは、層正規化やQKノームといった経験的な安定化手法を一切使用せずに、大規模Transformerの安定した訓練を実現した点です。
これは、現在の深層学習で広く使われている多くのヒューリスティックな手法が、より原理的なアプローチで置き換えられる可能性を示唆しています。

Muonオプティマイザーとの相乗効果の発見も重要な貢献です。
オプティマイザーの選択が重み制約手法の効果に大きく影響することを示しました。
これはアルゴリズム設計における新しい視点を提供しています。
spectral soft capのような、オプティマイザーの特性を考慮した制約手法の設計は、今後の研究の新しい方向性を示しています。

ただし、課題も明確です。
競争力のある性能を達成するためには依然として非常に大きなLipschitz境界（10^264）が必要です。
理論的な境界と実際の挙動の間には大きなギャップが存在します。
このギャップを埋めることが、今後の重要な研究課題となるでしょう。

### 5.2 今後の展望
今後の研究方向として、いくつかの興味深い可能性が考えられます。
第一に、より洗練されたLipschitz境界の計算手法の開発です。
現在の大域的な境界計算は非常に保守的です。
実際のモデルの挙動をより正確に反映する局所的または条件付きLipschitz境界の研究が期待されます。

第二に、アーキテクチャレベルでの改良です。
深さに依存しないLipschitz境界を持つTransformerアーキテクチャの開発が考えられます。
また、Lipschitz制約と相性の良い新しい注意機構の設計なども考えられます。

第三に、より大規模なモデルへの適用です。
現在の145Mパラメータから、数十億、数百億パラメータのモデルへとスケールアップする際の課題があります。
これらの課題と解決策の探求が必要です。
特に、計算効率を改善しながらLipschitz制約を維持する手法の開発が重要になるでしょう。

最後に、他のドメインへの応用も期待されます。
ビジョンTransformer、マルチモーダルモデル、強化学習など、様々な分野があります。
これらの分野でLipschitz制約の有効性を検証することで、より汎用的な手法へと発展する可能性があります。

この研究は、深層学習における理論と実践のギャップを埋める重要な一歩です。
より原理的で信頼性の高いAIシステムの構築に向けた道筋を示しています。