# Phi-Ground Tech Report: Advancing Perception in GUI Grounding

## 基本情報
- arXiv ID: 2507.23779v1 (https://arxiv.org/abs/2507.23779)
- 著者: Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, Baining Guo
- 所属: Microsoft
- 投稿日: 2025年08月01日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると
この論文は、Computer Use Agents（CUAs）のコア技術であるGUI groundingを改善したPhi-Groundモデルファミリーについて報告しています。GUI groundingは画面上の要素を正確に認識し操作する技術です。マウスクリックやキーボード入力などの操作を画面座標と結び付ける重要な役割を持ちます。

この技術により、チャットボットがデスクトップアプリケーションを人間のように操作できます。Phi-Groundは10B未満のパラメータで高性能を実現しました。プロジェクトページ：https://zhangmiaosen2000.github.io/Phi-Ground/

## 1. 研究概要
### 1.1 背景と動機
近年、大規模言語モデルの推論能力の向上により、コンピュータを人間のように操作できるComputer Use Agents（CUAs）が現実のものとなりつつあります。CUAsは、APIベースのアプローチとは異なり、GUIを通じて人間と同じようにマウスクリックやキーボード入力で操作します。これにより、プラットフォームや環境に依存せず、理論的には人間が実行できるあらゆる操作を可能にします。

しかし、現在のGUI groundingモデルは、ScreenSpot-proやUI-Visionなどの難しいベンチマークで65%未満の精度しか達成できておらず、実用化にはまだ課題があります。単一のミスクリックが取り返しのつかない結果をもたらす可能性があるため、高精度なgrounding技術の開発が急務となっています。

### 1.2 主要な貢献
本研究では、以下のような重要な貢献をしました。

- 40M以上の大規模データセットを構築し、データ収集からモデル訓練まで詳細な実証研究を実施しました。
- 入力モダリティの順序、データ拡張、計算効率などの重要な要因を特定し、最適化しました。
- 10Bパラメータ未満のモデルで、エージェント設定における5つのベンチマーク全てで業界トップレベルの性能を達成しました。
- ScreenSpot-proで55.0%、UI-Visionで36.2%という高い精度を実現しました。
- 実用性を考慮した計算効率とパラメータ数のトレードオフを詳細に分析しました。

## 2. 提案手法
### 2.1 手法の概要
Phi-Groundは2段階のアプローチを採用しています。第1段階では、GPT-4OなどのLarge Multimodal Model（LMM）が画面の要素に対する詳細な参照表現（Reference Expression, RE）を生成します。第2段階では、訓練されたPhi-Groundモデルがこの参照表現を基に正確なクリック座標を出力します。

この分離により、空間的計画（どの要素を操作するか）と位置特定（正確な座標を特定）という2つの異なるタスクを効率的に処理できます。モデルは相対座標を1000倍にスケールしたテキスト形式で出力し、シンプルながら効果的な実装を実現しています。

### 2.2 技術的詳細
入力フォーマットに関して、テキスト（参照表現）を画像より先に入力することで、画像トークンがinstruction-awareになり、性能が2～4ポイント向上することを発見しました。これはTransformerのcausal maskの特性により、早期のトークンが後期のトークンの情報を使えないことに起因します。

データ拡張技術として、Random CropとRandom Resizeを導入しました。Random Cropは画面の一部が表示されない状況を、Random Resizeは高解像度画面での小さな要素を模擬します。特にRandom Resizeは、ScreenSpot-proのような高解像度ベンチマークで8.0ポイントの性能向上をもたらしました。

訓練には多様なソースからのデータを使用しました。CommonCrawlの10.5Mウェブデータ、BingSearchの158K高解像度スクリーンショット、人間がラベル付けした80KのWindowsアプリケーションデータなどです。

### 2.3 新規性
既存手法との主な違いには、次のような点があります。

- モダリティ入力順序の重要性を初めて体系的に検証し、instruction-aware画像モデリングを実現しました。
- 大規模訓練において有効なデータ拡張技術を特定しました（多くの一般的な技術は効果がなかった）。
- パラメータ数だけでなく、画像トークン数を含む計算効率を考慮したスケーリング法則を分析しました。
- In-domain post-trainingにおけるDPOの有効性を純粋な知覚タスクで実証しました。

## 3. 実験結果
### 3.1 実験設定
5つの公開ベンチマークで評価を実施しました。ScreenSpot-V2、ScreenSpot-Pro、UI-Vision、Showdown-click-dev、独自のGoldデータセットです。評価にはShort REとLong REの両方を使用しました。Short REはベンチマーク提供の簡潔な指示で、Long REはGPT-4Oが生成した詳細な参照表現です。両設定で性能を測定しました。

### 3.2 主要な結果
Agent設定（Long RE使用）において、Phi-Ground-7B-16C-DPOは次のような結果を達成しました。

- ScreenSpot-V2: 93.4%（既存最高は93.0%）
- ScreenSpot-Pro: 55.0%（既存最高は48.8%）
- UI-Vision: 36.2%（既存最高は29.4%）
- Showdown: 73.9%（既存最高は71.6%）
- Gold: 88.2%（既存最高は81.5%）

End-to-end設定でも、ScreenSpot-Pro（43.2%）、UI-Vision（27.2%）、Gold（79.6%）で最高性能を達成しました。

### 3.3 既存手法との比較
UI-TARS-1.5-7Bと比較して、ScreenSpot-Proで6.2ポイント、UI-Visionで6.8ポイントの改善を達成しました。また、商用モデルであるClaude Computer UseやOpenAI Operatorよりも高い性能を示しました。計算効率の観点では、同等の性能を達成するのに必要な計算量を約30%削減し、Pareto最適フロンティアを更新しました。

## 4. 実用性評価
### 4.1 実装の容易性
Phi-Groundは、Phi-3.5-Vision-InstructとPhi-4-MMを基盤モデルとして使用し、標準的なファインチューニング手法で訓練可能です。座標をテキスト形式で出力するシンプルな設計により、特殊なアーキテクチャの変更は不要で、既存のMLLMパイプラインに容易に統合できます。

### 4.2 計算効率
画像トークン数を調整可能な設計により、用途に応じて性能と計算コストのバランスを取ることができます。3つの設定を提供しています。7クロップ（1045トークン）、16クロップ（2353-4161トークン）、29クロップ（4237-7505トークン）です。高解像度が不要なタスクでは計算量を最大80%削減できます。

### 4.3 応用可能性
GUI自動化、RPAツール、アクセシビリティ支援、自動テストなど、幅広い応用が期待できます。特に、Windowsアプリケーション向けのin-domain post-trainingの成功は、特定の業務システムやソフトウェアに特化したカスタマイズの可能性を示しています。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、GUI groundingという実用的な問題に対して、体系的な実証研究を通じて大幅な性能向上を達成しました。特に、一見些細に思える入力順序の変更が大きな影響を与えることや、大規模訓練では多くの一般的な技術が無効になることなど、実践的に重要な知見を提供しています。また、純粋な知覚タスクにおけるDPOの有効性を示したことは、強化学習の応用範囲を広げる重要な貢献です。

エラー分析により、現在の限界も明確に示されています。計画の誤り（24.8%）、言語サポートの不足（12.3%）、極端な画面サイズへの対応（22.9%）など、今後の改善点が具体的に特定されています。

### 5.2 今後の展望
プライバシー保護とエラーの責任所在という2つの社会的課題が明確に認識されています。今後は、ローカルでの処理や差分プライバシーなどの技術を組み合わせた安全なCUAシステムの開発が期待されます。また、誤操作の影響を最小化するための人間-AI協調インターフェースの設計も重要な研究課題となるでしょう。

技術的には、マルチ言語対応、より極端な画面構成への対応、リアルタイム性の向上などが今後の発展方向として考えられます。また、本研究で得られた知見は、他のマルチモーダル知覚タスクにも応用可能であり、広範な影響が期待されます。