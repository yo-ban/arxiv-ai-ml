# SpikingBrain Technical Report: Spiking Brain-inspired Large Models

## 基本情報
- arXiv ID: 2509.05276v1 (https://arxiv.org/abs/2509.05276)
- 著者: Yuqi Pan, Yupeng Feng, Jinghao Zhuang, Siyu Ding, Zehao Liu, Bohan Sun, Yuhong Chou, Han Xu, Xuerui Qiu, Anlin Deng, Anjie Hu, Peng Zhou, Man Yao, Jibin Wu, Jian Yang, Guoliang Sun, Bo Xu, Guoqi Li（全18名）
- 所属: 中国科学院自動化研究所、香港理工大学、北京人工智能研究院、MetaX集成電路株式会社など
- 投稿日: 2025年09月08日
- カテゴリ: cs.NE, cs.AI

## 簡単に説明すると
従来のTransformerベースの大規模言語モデルは計算量が系列長の二乗に比例し、
長文処理で大きなボトルネックとなっています。
この研究はスパイキングニューラルネットワークの仕組みにヒントを得て、
線形注意機構とMixture-of-Experts（MoE）、適応的スパイキングニューロンを組み合わせた
脳型大規模言語モデル「SpikingBrain」を提案しています。
特筆すべきは、NVIDIA以外のMetaX GPUクラスタで実際に学習・推論を行い、
7Bパラメータモデルと76B MoEモデルの開発に成功している点です。
コードはGitHubで公開されています：https://github.com/BICLab/SpikingBrain-7B

## 1. 研究概要
### 1.1 背景と動機
現代の大規模言語モデルはTransformerアーキテクチャに基づき、
スケーリング法則により性能向上を実現してきました。
しかし、このスケール駆動型アプローチには深刻な課題があります。

第一に計算量とメモリ使用量の爆発的増加です。
学習時の計算量は系列長の二乗に比例し、
推論時のメモリ使用量は系列長に比例して増加します。
これにより長文処理能力が著しく制限されています。

第二にエネルギー効率の問題です。
高い学習コストと消費電力、複雑な展開パイプラインが必要となり、
限られた資源下での高性能・省エネルギー化が重要な研究目標となっています。

第三に計算プラットフォームの多様化への対応です。
NVIDIA以外の計算クラスタでの大規模モデル開発は、
安定かつ効率的な学習・展開において大きな技術的挑戦となっています。

### 1.2 主要な貢献
この研究は脳の仕組みにヒントを得て、従来のTransformerフレームワークを超える
効率的な脳型大規模言語モデルの開発に成功しました。

- ハイブリッド線形アーキテクチャによる二次的自己注意からの脱却を実現し、
  線形・局所・標準注意モジュールを組み合わせた新しい設計を提案
- 効率的なモデル変換パイプラインにより既存LLMとの互換性を保ちながら、
  学習・推論コストを従来の2%以下に削減
- 生物学的ニューロンにヒントを得た適応的閾値スパイキング手法を開発し、
  69.15%のスパイク疎性を実現して低電力動作を可能にした
- MetaX GPU クラスタでの大規模学習・推論の実現により、
  非NVIDIA プラットフォームでの脳型LLM開発の実現可能性を実証

## 2. 提案手法
### 2.1 手法の概要
SpikingBrainは脳の情報処理メカニズムにヒントを得た
大規模言語モデルファミリーです。
中核となる3つの技術要素で構成されています。

第一にハイブリッド線形アーキテクチャです。
線形注意、スライディングウィンドウ注意、標準ソフトマックス注意を
組み合わせることで計算複雑度を線形に削減しつつ、
各注意メカニズムの長所を活かした設計を実現しています。

第二に効率的なモデル変換フレームワークです。
既存のTransformerモデルの重みを再マッピングすることで、
注意マップの対応関係を利用した軽量な学習パラダイムを確立しています。

第三にスパイキング符号化スキームです。
生物学的ニューロンの活動にヒントを得て、
活性化値をスパイクカウントに変換し、
さらにスパースなスパイクトレインに展開することで
加算ベースのイベント駆動計算を実現しています。

### 2.2 技術的詳細
SpikingBrainは2つのモデルを開発しました。

SpikingBrain-7Bは純粋線形モデルで、
線形注意とスライディングウィンドウ注意（4Kウィンドウ）を
1:1の比率で層間ハイブリッド化しています。
ゲート線形注意モジュール（GLA）を採用し、
学習時は線形計算複雑度、推論時は一定メモリ使用量を実現します。

SpikingBrain-76Bはハイブリッド線形MoEモデルで、
層内並列ハイブリッド化により線形・局所・標準注意を組み合わせています。
16個のルーテッドエキスパート（top-1活性化）と1個の共有エキスパートを持ち、
トークン当たり約15%（12B）のパラメータのみが活性化されます。

適応的閾値スパイキングニューロンは従来のLIFモデルを簡素化し、
膜電位と相関する動的閾値により適度な活性状態を維持します。
減衰係数を除去し、ソフトリセット機構を採用することで
連続値から整数スパイクカウントへの一段階変換を可能にしています。

### 2.3 新規性
既存研究との主な違いは包括的なシステム設計にあります。
従来の線形注意研究は単一の注意機構に焦点を当てていましたが、
SpikingBrainは複数の注意メカニズムを統合したハイブリッドアプローチを採用しています。

既存のMoEモデルは主にモデル容量の拡張に焦点を当てていましたが、
SprikingBrainはネットワークレベルとニューロンレベルの
二階層疎性戦略により効率性と性能のバランスを実現しています。

特に重要な革新は実用的な変換パイプラインです。
注意マップの対応関係分析により既存Transformerの重みを直接活用し、
約150Bトークンという少ないデータで学習を完了できます。

## 3. 実験結果
### 3.1 実験設定
実験はMetaX C550 GPU数百台のクラスタで実施されました。
ベースモデルとしてQwen2.5-7Bチェックポイントを使用し、
継続的事前学習、長文脈拡張（最大128kトークン）、
教師あり微調整の全プロセスを検証しています。

評価指標として一般言語モデリング性能、長系列処理効率、
モデル効率指標（MFU）、推論速度（TTFT）、
スパイク疎性率などを使用しています。

分散学習戦略として7BモデルではColossal-AIフレームワークで
32ウェイデータ並列・8ウェイ系列並列を採用し、
76BモデルではMegatronフレームワークで
128ウェイデータ並列・8ウェイエキスパート並列・4ウェイパイプライン並列を採用しています。

### 3.2 主要な結果
SpikingBrainは主流の大規模言語モデルの約2%のデータ（150Bトークン）で
オープンソースTransformerモデルに匹敵する一般言語モデリング性能を達成しました。

長系列処理において顕著な効率向上を実現しています。
SpikingBrain-7Bは4Mトークン系列で100倍以上のTTFT高速化を達成し、
1Bパラメータの圧縮モデルでは256k系列長で15.39倍の高速化を実現しています。

学習安定性の面では、SprikingBrain-7BでModel FLOPs Utilization（MFU）23.4%を達成し、
数週間にわたる安定した大規模学習を維持しています。
Tokens-per-GPU-per-Second（TGS）は1558（8k系列長、DP8PP4設定）を記録しています。

スパイク疎性は69.15%を達成し、
長系列での動的スパイク符号化は約5%のスパイク率を実現して
低電力動作への強力なサポートを提供しています。

### 3.3 既存手法との比較
従来のTransformerモデルと比較して、
SpikingBrainは計算複雑度と推論メモリ使用量を大幅に削減しています。
線形注意により学習時O(n)、推論時O(1)の複雑度を実現し、
標準的な二次複雑度O(n²)からの大幅な改善を達成しています。

既存の線形注意モデル（RetNet、Mambaなど）と比較して、
ハイブリッド設計により単一メカニズムの限界を克服し、
より柔軟な性能・効率トレードオフを実現しています。

MoEモデルとの比較では、脳型設計による二階層疎性戦略により
計算効率とモデル表現力のバランスを向上させています。

## 4. 実用性評価
### 4.1 実装の容易性
HuggingFaceとvLLM推論フレームワークとの統合により
単一・複数GPU設定での展開をサポートしています。
既存のTransformerチェックポイントからの変換パイプラインにより
実装コストを大幅に削減しています。

MetaX GPUクラスタでの完全なソフトウェアスタック適応により、
TritonとCUDA演算子の移植、並列戦略の最適化、
通信プリミティブの調整を含む包括的な実装支援を提供しています。

オープンソースコード公開により
研究コミュニティでの再現・拡張が容易になっています。

### 4.2 計算効率
学習効率では従来の2%以下のデータで同等性能を実現し、
大幅な学習コスト削減を達成しています。
線形・近線形複雑度により長系列学習効率を大幅に向上させています。

推論効率では4Mトークン系列で100倍以上のTTFT高速化、
定数または部分定数のメモリオーバーヘッドを実現しています。
スパイク符号化による69%以上の疎性により
低電力動作への道筋を提示しています。

ハードウェア効率ではMetaX GPUでのMFU 23.4%達成により、
非NVIDIAプラットフォームでの効率的な学習実現可能性を実証しています。

### 4.3 応用可能性
産業制御・モバイルデバイスなどのエッジシナリオでの
低電力脳型LLM展開への実用的経路を提供しています。
特にニューロモーフィックチップとの親和性が高く、
次世代省エネルギーハードウェア開発への技術的指針となっています。

多様なGPUプラットフォームでの効率的モデルスケーリングへの示唆を提供し、
NVIDIA以外の計算プラットフォームでの大規模モデル開発の実現可能性を実証しています。

E-コマース・動画ストリーミング・ソーシャルメディアなど
長文脈処理が重要な応用分野での性能向上が期待されます。

## 5. まとめと所感
### 5.1 論文の意義
この研究は理論と実践の両面で重要な意義を持ちます。
学術的には脳型計算パラダイムを大規模言語モデルに適用した
包括的なアプローチを提示し、ニューロモーフィック計算の新しい方向性を示しています。

産業的にはMetaX GPUクラスタでの実際の大規模モデル学習・展開により
非NVIDIAプラットフォームでの実現可能性を実証した点が画期的です。
これは計算プラットフォームの多様化への重要な技術的基盤を提供しています。

技術的には線形注意とMoE、スパイキングニューロンの
統合設計により効率性と性能のバランスを実現し、
従来のTransformerアーキテクチャの限界を克服する
実用的なソリューションを提示しています。

### 5.2 今後の展望
技術的発展として更なる長文脈処理能力の向上と
より高度なスパイキング符号化スキームの開発が期待されます。
特にニューロモーフィックハードウェアとの協調設計により
真の低電力動作実現への道筋が重要になります。

プラットフォーム拡張として他の非NVIDIAプラットフォームへの適用と
異なるハードウェアアーキテクチャでの最適化が課題となります。
また、クラウドからエッジまでの幅広い展開シナリオへの対応も重要です。

応用拡大として長文書理解・対話システム・コード生成など
具体的なタスクでの性能検証と実用化が期待されます。
特に産業応用での省エネルギー効果の実証が
今後の発展において重要な鍵となるでしょう。
