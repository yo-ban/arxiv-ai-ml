# Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest

## 基本情報
- arXiv ID: 2509.05292v1 (https://arxiv.org/abs/2509.05292)
- 著者: Xiao Yang, Mehdi Ben Ayed, Longyu Zhao, Fan Zhou, Yuchen Shen, Abe Engle, Jinfeng Zhuang, Ling Leng, Jiajing Xu, Charles Rosenberg, Prathiba Deshikachar（全12名）
- 所属: Pinterest Inc.（Yuchen ShenはCarnegie Mellon Universityからインターンとして参加）
- 投稿日: 2025年09月08日
- カテゴリ: cs.LG, cs.AI

## 簡単に説明すると
広告推薦システムにおいて、プラットフォーム・広告主・ユーザーの価値バランスを取る
「ランキング効用関数」のハイパーパラメータを従来の手動調整ではなく
深層強化学習で自動的かつ個人化して調整するフレームワーク「DRL-PUT」を提案した論文です。
Pinterest社の本番環境で実際に運用し、
クリック率9.7%、長時間クリック率7.7%の改善を実現しています。
強化学習の状態としてユーザープロファイル・行動履歴・コンテキスト情報を用い、
行動空間として効用関数のウェイト組み合わせを離散化し、
REINFORCEアルゴリズムによりポリシーを学習します。

## 1. 研究概要
### 1.1 背景と動機
現代の広告推薦システムでは、プラットフォーム・広告主・ユーザーの
三者の価値バランスを取るランキング効用関数が中核的な役割を果たしています。
従来はこの関数のハイパーパラメータを部門横断チームが手動で調整していましたが、
この手法には根本的な課題がありました。

第一に調整方法論が非体系的であることです。
現代の推薦システムは複数の目標を同時最適化する必要がありますが、
手動調整時の数学的定式化が曖昧であることが多く、
最適化の方向性が不明確になりがちです。

第二にハイパーパラメータ組み合わせの爆発的増加です。
推薦する項目は多様で、それぞれが固有の目標を持ち、
様々なエンゲージメントスコアの予測が必要となり、
さらに多数のビジネスルールがそれぞれハイパーパラメータを持つため、
組み合わせ数は組み合わせ論的に爆発します。

第三に個人化と季節調整の欠如です。
手動調整では通常、全ユーザーに同一のハイパーパラメータが適用され、
時期や季節の変動への適応も困難です。

### 1.2 主要な貢献
この研究の主要な貢献は多目標最適化問題に対する
自動的・適応的・個人化されたソリューションの提案です。
具体的には以下の点で革新的です。

- 広告ランキングにおけるユーティリティ調整問題を
  強化学習問題として定式化した初の研究
- Pinterest社の本番システムで実証され、
  実際の広告推薦システムで大幅な性能向上を達成
- REINFORCEアルゴリズムベースのポリシー勾配手法により、
  価値関数推定の困難を回避した実用的な解法を提案

## 2. 提案手法
### 2.1 手法の概要
DRL-PUT（Deep Reinforcement Learning for Personalized Utility Tuning）は、
広告リクエストを受信した際に最適なハイパーパラメータを予測する強化学習エージェントです。

ユーザーの特徴量を状態表現に変換し、
学習済みポリシーによってカスタマイズされた報酬の最大化を目指して
最適なハイパーパラメータセットを行動として決定します。

ランキング効用関数は以下のように定式化されます：
U = 1{推定収益 ≥ b} × (推定収益 + 推定ユーザー価値)
ここで推定収益は p(最適化行動) × 入札額、
推定ユーザー価値は Σ p(エンゲージメント行動_i) × w_i として表現されます。

### 2.2 技術的詳細
行動空間は本来連続空間ℝ_+^nですが、
十分な学習データが得られないため離散化を採用しました。
各重み w_i を事前情報に基づく範囲 [w_{i,min}, w_{i,max}] 内で
m=10個の等間隔値に分割し、
さらに意味的に関連する重みをグループ化することで
行動空間を g^n = 10^3 に削減しました。

状態表現には以下の特徴量カテゴリを使用します：
- ユーザープロファイル（年齢・性別・居住地域など）
- ユーザー行動（サイト内外でのクリック・購入履歴など）
- コンテキスト情報（時刻・曜日・IP国など）

報酬関数は推定収益と推定ユーザー価値の和として定義し、
キャンペーンタイプ（クリック・コンバージョン・インプレッション）に応じて
異なる収益定義を採用しています。

### 2.3 新規性
既存のマルチエージェント強化学習アプローチとは異なり、
DRL-PUTは単一エージェントが効用関数の重みのみを調整するため
開発・保守が簡潔です。
特に大規模産業オンライン広告プラットフォームでは
多数のモデル変更が複雑化しやすい中、
この独立性は重要な利点となります。

ポリシーベース手法により、
高分散かつ不均衡な即時報酬分布のため困難な価値関数推定を回避し、
オンライン学習環境での計算効率を実現しています。
サービング時にはポリシーモデルの推論のみで済むため、
リアルタイム性も確保されています。

## 3. 実験結果
### 3.1 実験設定
Pinterest社の数億人の月間アクティブユーザーを持つ
実世界データセットでオフライン・オンライン実験を実施しました。

オフライン評価では新たな指標Diversity（最頻出行動の支配度）と
Relative_Gain（行動ポリシーに対する候補ポリシーの優位性）を提案し、
様々な設計選択の組み合わせを評価しました。

オンラインA/B実験では収益・インプレッション・CTR・CTR30・CVRを指標とし、
本番トラフィックで効果を検証しました。

### 3.2 主要な結果
オンラインA/B実験において顕著な改善を達成しました。
プラットフォーム全体では収益+0.27%、CTR+1.62%、CTR30+1.03%、CVR+0.67%、
処理セグメントではCTR+9.71%、CTR30+7.73%、CVR+1.26%の向上を実現しています。

特に注目すべきは、0.5%のCTR向上が実質的な改善とされる中で
9.71%という大幅な改善を達成した点です。

報酬関数の異なる定義（R0-R4）による消融研究では、
収益重視（R1）ではユーザーエンゲージメントが犠牲になり、
エンゲージメント重視（R2）では収益が減少することを確認し、
キャンペーンタイプ別の報酬カスタマイゼーション（R3,R4）が
バランスの取れた改善をもたらすことを実証しました。

### 3.3 既存手法との比較
従来の手動調整と比較して、
DRL-PUTは数学的に明確な最適化目標を持ち、
個人化と季節適応を自動で実現します。

既存のマルチエージェント強化学習手法と比較すると、
単一エージェント設計により開発・運用コストを大幅に削減し、
ポリシー勾配手法により価値関数推定の困難を回避しています。

## 4. 実用性評価
### 4.1 実装の容易性
Pinterest社の本番環境での運用実績により実装可能性が実証されています。
MLP（多層パーセプトロン）ベースの比較的シンプルなアーキテクチャを採用し、
カテゴリカル特徴量の埋め込み・バッチ正規化・ReLU活性化・ソフトマックス出力という
標準的な深層学習技術で構築されています。

行動空間の離散化により、連続空間での複雑な最適化を避け、
実装とデバッグを簡素化しています。

### 4.2 計算効率
サービング時にはポリシーモデルの1回の推論のみで済むため
リアルタイム性が確保されています。
REINFORCEアルゴリズムによる学習は比較的軽量で、
バッチサイズ24567でAdamW最適化器を使用し
学習率10^-4で安定した収束を実現しています。

行動ログ収集はリクエストレベルで0.5%以下に制限し、
本番システムへのリスクを最小化しています。

### 4.3 応用可能性
本手法は広告推薦システム以外の多目標最適化問題にも適用可能です。
Eコマース・動画ストリーミング・ソーシャルメディアなど、
複数のステークホルダー間のバランス調整が必要な推薦システム全般に
拡張できる汎用性を持っています。

特に個人化と季節適応が重要な領域では、
従来の静的な手動調整に比べて大幅な改善が期待できます。

## 5. まとめと所感
### 5.1 論文の意義
この研究は理論と実践の両面で重要な意義を持ちます。
学術的には多目標最適化問題への強化学習適用の新しいパラダイムを提示し、
産業的には大規模広告システムでの実証により実用性を証明しています。

特に価値関数推定を回避するポリシーベース手法の採用と
行動空間の効果的な離散化は、
実環境での強化学習適用における重要な知見を提供しています。

Pinterest社という実際の大規模プラットフォームでの
長期間の運用実績は研究の信頼性を大きく高めています。

### 5.2 今後の展望
著者らが指摘する通り、オンポリシー学習への移行と
モデル安定性の確保が重要な課題です。
現在はオフポリシー学習ですが、本番環境では
モデル自身が生成するデータからの学習が必要となります。

長期報酬の組み込みも重要な研究方向です。
現在の即時報酬重視から、
時間割引や新しい報酬構造による遅延利益の捕捉が
より包括的な最適化を可能にするでしょう。

需要・供給情報の活用や、
より高度なコンテキスト情報の統合も性能向上の余地があります。
