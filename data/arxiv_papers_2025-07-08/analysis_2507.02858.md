# Requirements Elicitation Follow-Up Question Generation

## 基本情報
- arXiv ID: 2507.02858v1 (https://arxiv.org/abs/2507.02858)
- 著者: Yuchen Shen, Anmol Singhal, Travis Breaux
- 所属: Carnegie Mellon University, Pittsburgh, USA
- 投稿日: 2024年07月03日
- カテゴリ: cs.AI

## 簡単に説明すると
この論文は、ソフトウェア開発における要求抽出インタビューで、インタビュアーが明確で関連性の高いフォローアップ質問を生成することを支援するために、GPT-4oを活用する手法を提案しています。要求抽出インタビューは、ソフトウェアシステムに対するステークホルダーのニーズや期待を理解するための重要な手法です。しかし、インタビュアーは認知的負荷や情報過多により、リアルタイムで効果的な質問を考えることが困難です。

本研究では、インタビュアーがよく犯す14種類のミスを避けるように GPT-4o を誘導することで、人間が作成した質問よりも明確で関連性が高く、情報量の多い質問を生成できることを実証しています。データセットとコードは Zenodo (https://zenodo.org/records/15793870) で公開されています。また、関連研究のリストは GitHub (https://github.com/anmolsinghal98/Requirements-Elicitation-Follow-Up-Question-Generation) で公開されています。

## 1. 研究概要
### 1.1 背景と動機
要求抽出インタビューは、ソフトウェア開発においてステークホルダーのニーズ、好み、期待を理解するための最も伝統的で一般的な手法です。インタビュアーはステークホルダーと直接対話し、リアルタイムで深く探求できます。これにより曖昧さを解消し、暗黙知を引き出すことが可能になります。

しかし、インタビューの実施には多くの課題があります。インタビュアーは、ステークホルダーのスピーチを聞きながらフォローアップ質問を考える必要があり、過度の認知的負荷を経験します。また、ドメイン知識の不足、コミュニケーションスキルの差、文化的違いなども影響します。ステークホルダーが長く複雑な回答をした場合、情報過多により重要なポイントを整理することが困難になります。

### 1.2 主要な貢献
本研究は、要求抽出インタビューにおけるフォローアップ質問生成に大規模言語モデル（LLM）を活用する初めての詳細な研究で、以下の4つの主要な貢献をしています。
- 14件のインタビュー記録を分析し、フォローアップ質問に必要な会話コンテキストの量を7つの質問タイプを特定。
- 最小限のガイダンスでGPT-4oが生成した質問と人間が作成した質問を比較し、関連性、明確性、情報性において同等の性能を実証。
- 先行研究からインタビュアーが避けるべき14種類の一般的なミスを体系化したフレームワークを作成。
- ミスタイプに基づいてガイドされた場合、GPT-4o生成質問が人間作成質問よりも優れた性能を示すことを実証。

## 2. 提案手法
### 2.1 手法の概要
本研究では、要求抽出インタビューにおけるフォローアップ質問生成を支援するために、GPT-4oを用いた3つの実験を実施しました。第1に、最小限のガイダンスでGPT-4oが生成した質問と訓練されたインタビュアーの質問を比較しました。第2に、GPT-4oが人間の質問が14種類の一般的なミスを犯しているかを判定する能力を評価しました。第3に、特定のミスを回避するようガイドされた場合のGPT-4o生成質問と人間作成質問を比較しました。

実験では、4種類のディレクトリサービスに関する14件の実際のインタビューから収集したトランスクリプトを使用しました。対象となったサービスはアパート探し、レストラン探し、ハイキングコース探し、クリニック探しです。各インタビューは20分間で、2年以上のソフトウェアエンジニアリング経験を持つ訓練されたインタビュアーが実施しました。

### 2.2 技術的詳細
本研究では、3つの異なるプロンプトテンプレートを使用してGPT-4oを活用しました。第1のプロンプトは最小限のガイダンスで質問を生成するもので、会話コンテキストのみを提供します。第2のプロンプトは、インタビュアーの質問が特定のミスタイプを示しているかを分類するためのものです。第3のプロンプトは、特定のミスを回避するように指示して質問を生成するものです。

プロンプト設計では、ペルソナベースのプロンピング技術を使用し、「あなたは要求抽出インタビューを実施できるAIエージェントです」というシステムプロンプトを採用しました。また、否定表現を避け、肯定的なフレーミングに変更しました。例えば「代替案の検討に失敗してはいけない」ではなく「良いフォローアップ質問は代替案を検討すべきである」としました。

温度パラメータは1.0に設定し、創造的なタスクに適した非決定論的な出力を促しました。14種類のミスタイプのうち4つでは、性能向上のためにワンショット例を追加しました。

### 2.3 新規性
本研究の新規性は以下の点にあります。第1に、要求抽出インタビューのリアルタイムサポートに焦点を当てた初めての研究です。既存研究の多くはインタビュー前の準備や事後処理に注目していましたが、本研究はインタビュー中の質問生成を支援します。

第2に、インタビュアーのミスに基づく体系的なアプローチを採用しています。14種類の一般的なミスを文献から統合し、これらを回避するようにLLMをガイドすることで、質問の品質を向上させています。

第3に、実際のインタビューデータを用いた包括的な評価を実施しています。146個のフォローアップ質問コンテキストを分析し、7つの質問タイプを特定しました。また、64名の評価者による大規模な人間評価実験を通じて、LLM生成質問の有効性を実証しています。

## 3. 実験結果
### 3.1 実験設定
データ収集のために、4名の訓練されたインタビュアーを採用しました。全員が2年以上のフルタイムソフトウェアエンジニアリング経験を持ち、要求工学のコースを履修し、Ferrariらのトレーニングを受けました。インタビュー対象者は18歳以上で、ウェブ・モバイルアプリの使用経験があり、英語が流暢であることを条件としました。

評価指標としては、Paul Griceの実質的な会話理論に基づき、以下の3つを採用しました。関連性（relevancy）は、質問がトピックやシステムに関連し、会話の流れに沿っているかを評価します。明確性（clarity）は、質問がステークホルダーにとって理解しやすいかを評価します。情報性（informativeness）は、質問がシステムに関する情報量を増やす回答を引き出すかを評価します。

評価は6段階および5段階のセマンティックスケールを使用し、Prolificプラットフォームを通じて募集した参加者が実施しました。合計128名の評価者が参加し、各質問のソース（GPT-4oまたは人間）は盲検化されました。

### 3.2 主要な結果
研究質問1（フォローアップ質問のタイプと必要なコンテキスト）に対し、146個のフォローアップ質問を分析した結果、必要なコンテキストは平均1ターンであることが判明しました。71個の質問はコンテキスト不要、104個は最大1ターン、98%は最大4ターンで十分でした。また、7つの質問タイプを特定しました。トピック変更、回答探求、確認、質問探求、代替案探求、好み探求、明確化です。

研究質問2（最小限ガイドGPT-4oと人間の比較）では、両群間に統計的に有意な差は見られませんでした。情報性ではGPT-4oが4.8、人間が4.6（p=0.17）、関連性ではGPT-4oが5.0、人間が4.8（p=0.08）、明確性ではGPT-4oが5.1、人間が4.9（p=0.10）でした。

研究質問3（ミス分類能力）では、GPT-4oと人間アナリストの分類一致率は81.0%（340/420）でした。人間は177個、GPT-4oは163個をミスありと分類しました。

研究質問4（ミスガイド質問生成）では、GPT-4o生成質問が68.0%の確率で選ばれました。混合効果Bradley-Terryモデルは有意差（$4.23 \times 10^{-8}$）を示し、オッズ比2.662からGPT-4oが良い質問を生成する確率は93.5%と推定されました。

### 3.3 既存手法との比較
本研究は要求抽出インタビューのリアルタイム支援にLLMを使用した初めての研究であるため、直接比較できる既存手法は限られています。関連研究として以下があります。LLMを用いたインタビュースクリプト生成（GörerとAydemir, 2023）。シミュレートされたユーザー生成（Ataeiら, 2024）。トランスクリプト分析（Singhら, 2024）。

しかし、これらは主にインタビュー前後の処理に焦点を当てています。インタビュー中のチャットボット研究（Hanら, 2021）では、生成された質問の明確性の欠如やユーザー入力の認識問題が報告されています。

本研究の特徴は、インタビュアーのミスに基づく体系的なアプローチと、実際のインタビューデータを用いた包括的な評価です。ミスガイドアプローチにより、LLM生成質問が人間を上回る性能を示したことは、今後の要求工学実践に重要な示唆を与えます。

## 4. 実用性評価
### 4.1 実装の容易性
本手法の実装は比較的容易です。GPT-4o APIを使用し、3つのプロンプトテンプレートを適用するだけです。インタビュートランスクリプトのリアルタイム処理には、音声認識機能の統合が必要ですが、現在の音声テキスト変換技術（Whisperなど）を使用すれば実現可能です。

14種類のミスタイプはフレームワークとして明確に定義されており、新しいドメインへの適用も可能です。プロンプト設計の注意点（役割の明確化、否定表現の回避、ワンショット例の活用）も論文で詳細に説明されています。

### 4.2 計算効率
GPT-4oの推論速度はリアルタイムアプリケーションに十分です。論文の実験では、最大で4ターンのコンテキスト（約500-1000トークン）で質問を生成しており、API呼び出しの応答時間は数秒以内です。

ミス分類タスクでは、14種類のミスタイプを並列に判定することで効率化が可能です。コスト面では、GPT-4oのAPI料金が考慶すべき点ですが、人間のインタビュアーのトレーニングコストと比較するとコスト効果が高いと考えられます。

### 4.3 応用可能性
本手法は様々な領域への応用が可能です。第1に、ソフトウェア要求抽出以外のインタビュー（市場調査、ユーザーリサーチ、医療問診など）にも適用できます。第2に、リアルタイム支援ツールとしての実装が可能で、インタビュアーにARグラスやスマートフォンアプリで質問候補を提示できます。

第3に、インタビュアートレーニングツールとしての活用が考えられます。ミスタイプに基づくフィードバックを提供できるため、初心者のスキル向上に有効です。第4に、マルチエージェントシステムへの拡張も可能で、1つのLLMが質問を生成し、別のLLMが評価することでより高品質な質問生成が期待できます。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、要求工学分野におけるLLM活用の新たな可能性を示しました。特に重要なのは、インタビュアーのミスに着目したアプローチです。単に「良い質問」を生成するのではなく、「ミスを避ける」という否定的指示がLLMの性能向上につながったことは、プロンプトエンジニアリングの重要な知見です。

また、実際のインタビューデータに基づく实証的研究である点も価値があります。4ターン以内のコンテキストが98%のフォローアップ質問に十分であるという発見は、実装上の重要な指針を提供します。これにより、計算資源を効率的に使用しながら高品質な支援が可能になります。

### 5.2 今後の展望
今後の研究方向として以下が考えられます。第1に、ファインチューニングによるドメイン特化モデルの開発です。特定の業界やドメインに特化したモデルにより、より高精度な質問生成が期待できます。第2に、強化学習の活用（RLHF）です。人間のフィードバックを取り入れることで、モデルの性能を継続的に改善できます。

第3に、マルチモーダルアプローチの採用です。音声、表情、ジェスチャーなどの非言語情報を統合することで、文脈により合致したフォローアップ質問の生成が可能になります。第4に、リアルタイム実装と実証実験です。実際のインタビュー現場での有効性検証が必要です。

最後に、安全性や倫理面の考慮も重要です。LLMのハルシネーションやバイアスの問題に対処し、インタビューの公平性を保つ仕組みが必要です。これらの課題に取り組むことで、要求抽出プロセスの大幅な改善が期待できます。