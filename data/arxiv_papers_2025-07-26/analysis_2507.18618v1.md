# TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards

## 基本情報
- arXiv ID：2507.18618v1 (https://arxiv.org/abs/2507.18618)
- 著者：Andreea Nica、Ivan Zakazov、Nicolas Mario Baldwin、Saibo Geng、Robert West
- 所属：EPFL、Microsoft Research
- 投稿日：2025年7月26日
- カテゴリ：cs.CL, cs.AI, cs.LG

## 簡単に説明すると
この論文は、大規模言語モデル（LLM）の数学的な推論能力を向上させる新しいプロンプト最適化手法を提案しています。従来の手法は数値的な報酬に依存していました。しかし本研究では、自然言語で表現されたフィードバックを直接学習に活用します。これにより、より豊かで表現力の高い学習信号を提供できます。

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデルは優れた能力を持ちますが、数学的・論理的推論タスクでは課題があります。プロンプトエンジニアリングは、モデルのパラメータを変更せずに推論能力を向上させる軽量な解決策として登場しました。

既存のプロンプト最適化手法には主に次の2つの問題があります。
1. クエリ依存型の問題では、正解/不正解の二値的な性質のため、数値的報酬の設計が困難。
2. テキストベースの手法は学習フリーで、実際の学習には活用されていない。

### 1.2 主要な貢献
この論文の主要な貢献は次の4点です。
- テキスト報酬を学習時の監督信号として直接組み込む初めての手法。
- クエリ依存型プロンプト最適化のための新しい方法論「TRPrompt」の提案。
- 様々なデータセットでの有効性の検証と包括的な分析。
- GSMHardとMATHデータセットでの最先端性能の達成。

## 2. 提案手法
### 2.1 手法の概要
TRPromptは次の3つの主要な段階から構成される反復的なパイプラインです。

1. **合成訓練データセット作成**：プロンプトモデルが最適なテキスト報酬に基づいてクエリ特化型プロンプトを生成。ターゲットLLMがそのプロンプトで回答を生成し、報酬モデルがテキスト報酬を作成。

2. **プロンプトモデルの微調整**：生成されたプロンプトとテキスト報酬のペアを使用して、教師あり学習でプロンプトモデルを微調整。

3. **最適報酬の更新**：Textgradを使用して、微調整されたモデルに対する最適なテキスト報酬を探索・更新。

### 2.2 技術的詳細
**クエリ依存型プロンプト問題の定式化**
- 質問 q ∈ Q に対して、プロンプト p ∈ P を生成
- ターゲットLLM：M_target(q, p) → y
- テキスト報酬モデル：R_textual(p, q, y, y*) → t
- プロンプトモデル：Π_query(q, t) → p

**二重目的の最適化**
1. プロンプトモデルの最適化：最適なテキスト報酬 t* に対応するプロンプトを生成
2. 最適テキスト報酬の探索：プロンプトモデルの性能を最大化する報酬の特定

### 2.3 新規性
既存手法と比較した新規性は次の点にあります。
- テキスト報酬を学習に直接活用する初めての手法。
- 初期の専門家作成プロンプトへの依存を排除。
- 反復的な自己改善プロセスにより、継続的な性能向上を実現。

## 3. 実験結果
### 3.1 実験設定
- データセット：GSM8K、GSMHard、MATH（数学的推論タスク）
- モデル：Meta-Llama-3-8B-Instructを全コンポーネントで使用
- 評価指標：正答率（Accuracy）
- 実装詳細：LoRA（r=256、α=256）、4イテレーション、800サンプルの合成データセット

### 3.2 主要な結果
**定量的結果**
- GSM8K：84.53%（QPO 500プロンプトの86.05%には及ばず）
- GSMHard：31.76%（全手法中最高、+1%改善）
- MATH：41.37%（全手法中最高、+2%改善）

**反復的改善**
- 各イテレーションで性能が向上し、ベースモデルから最大7.5%の改善
- 困難なデータセットでより顕著な改善効果

### 3.3 既存手法との比較
- CoT、Prompt-OIRL、QPOなどの既存手法と比較
- 困難なデータセット（GSMHard、MATH）で一貫して最高性能
- 初期プロンプトへの依存がなく、より自由なプロンプト空間の探索が可能

## 4. 実用性評価
### 4.1 実装の容易性
本手法は既存のLLMを活用するため、実装が比較的容易です。必要なのは次の要素です。
- Meta-Llama-3-8B-Instructなどの事前学習済みLLM。
- GPT-4o-miniへのアクセス（最適報酬の探索用）。
- 標準的なGPU（NVIDIA A100 80GB）。

### 4.2 計算効率
- 訓練時間：48〜72時間（データセットによる）
- ボトルネック：Textgradステップが全体の70%を占める
- 並列化が困難なため、スケーラビリティに課題

### 4.3 応用可能性
TRPromptは次のような分野で特に有用です。
- **教育分野**：数学的な推論問題の自動解答生成。
- **研究支援**：複雑な論理的推論が必要なタスクの支援。
- **創造的タスク**：数値報酬では評価困難な詩や創作文章の生成。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、プロンプト最適化の分野に重要な貢献をしています。特に次の点が挙げられます。
- テキスト報酬の表現力を活用した新しいパラダイムの提案。
- 自己改善型の学習プロセスによる継続的な性能向上。
- 初期プロンプトへの依存を排除した、より汎用的なアプローチ。

査読前の論文ですが、実験結果は説得力があり、特に困難なデータセットでの改善は実用上の価値が高いと評価できます。

### 5.2 今後の展望
**改善の余地**
- 現在のTextgradより70%高速な最適報酬の探索手法の開発
- 簡単なデータセットでの性能向上手法の検討
- より大規模なモデルでの検証

**潜在的な課題**
- 計算コストの削減とスケーラビリティの改善
- テキスト報酬の品質がモデル性能に与える影響の詳細な分析
- クロスドメイン汎化性能のさらなる向上

この研究は、数値報酬では捉えきれない豊かな情報をテキスト報酬として活用することで、LLMの能力を最大限に引き出す新しい可能性を示しています。