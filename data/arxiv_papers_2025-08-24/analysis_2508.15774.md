# CineScale: Free Lunch in High-Resolution Cinematic Visual Generation

## 基本情報
arXiv IDは2508.15774v1で、URLはhttps://arxiv.org/abs/2508.15774です。
著者はHaonan Qiu、Ning Yu、Ziqi Huang、Paul Debevec、Ziwei Liuです。
所属はNetflix Eyeline Studiosとnanyang Technological Universityです。
投稿日は2025年8月24日で、カテゴリはcs.CVおよびcs.AIです。

## 簡単に説明すると
この論文は、既存の拡散モデルを高解像度の画像・動画生成に対応させる推論手法CineScaleを提案しています。従来の拡散モデルは512×512や1024×1024など限られた解像度で学習されているため、より高い解像度での生成が困難でした。
CineScaleは追加の学習なしで8K画像生成を可能にし、最小限のLoRA微調整で4K動画生成を実現します。
UNet系とDiT系の両方の拡散モデルアーキテクチャに対応しています。
Text-to-Image、Text-to-Video、Image-to-Video、Video-to-Videoタスクをサポートします。
生成サンプルは公式ウェブサイト https://eyeline-labs.github.io/CineScale/ で確認できます。

## 1. 研究概要
### 1.1 背景と動機
視覚的拡散モデルは画像や動画生成において著しい進歩を遂げていますが、高解像度データの不足と計算資源の制約により、通常は限られた解像度で学習されています。
SD 1.5は512×512、SDXLは1024×1024、VideoCrafter2は320×512といった具合に、既存モデルの学習解像度は限定的でした。
高解像度データの希少性と相当なモデル容量増大の要求を考慮すると、既存の事前学習済み拡散モデルの強力な生成能力を継承する調整不要戦略が有望なアプローチとなります。

### 1.2 主要な貢献
本研究では3つの重要な貢献を提供しています。
第一に、FreeScaleを拡張したCineScaleという新しい推論パラダイムを提案しました。
これにUNetベースとDiTベースの両方の拡散モデルで高解像度の視覚生成を可能にしました。
第二に、Text-to-Image（T2I）、Text-to-Video（T2V）に加えて、Image-to-Video（I2V）、Video-to-Video（V2V）の高解像度生成に拡張しました。
第三に、調整不要手法としては初めて8K解像度（64倍）のtext-to-image生成を実現し、最小限のLoRA微調整で4K解像度（9倍）の動画生成を可能にしました。

## 2. 提案手法
### 2.1 手法の概要
CineScaleは、FreeScaleフレームワークを基盤として、UNetベースとDiTベースの拡散モデル両方に対応する高解像度生成の手法です。
主要コンポーネントは3つで構成されています。
Tailored Self-Cascade Upscaling、Restrained Dilated Convolution、Scale Fusionが主な要素です。
DiTベースモデルには追加でNTK-RoPE、Attentional Scaling、Minimal LoRA Fine-Tuningが適用されます。

### 2.2 技術的詳細
Latent Diffusion Modelの数学的定式化に基づいています。

**Tailored Self-Cascade Upscaling:**
低解像度で可理な視覚構造を生成し、それを保持しながら段階的に解像度を向上させる手法です。
中間結果をアップサンプリングし、タイムステップKのノイズを追加してデノイジングを行います。
詳細レベルの柔軟な制御のため、コサイン減衰因子を用いた混合処理を実装しています。

**Restrained Dilated Convolution:**
ScaleCrafterの提案に基づき、畳み込みの受容野を拡張してオブジェクトの繰り返しを抑制します。
ただし、up-blockに適用すると混乱したテクスチャが生じるため、down-blockとmid-blockのみに適用しています。

**Scale Fusion:**
グローバル情報とローカル情報を融合して、高品質な結果を達成します。
数式で表現すると以下のようになります。
（融合式省略）
ここでGはGaussian blurのローパスフィルタです。
高周波数と低周波数情報を適切に組み合わせています。

DiTベースモデルには、NTK-RoPE、Attentional Scaling、Noise Shifting、Minimal LoRA Fine-Tuningを組み合わせて適用します。

### 2.3 新規性
従来手法との主な違いは、UNetとDiTの両アーキテクチャに対応する統一的なフレームワークの提供です。
MultiDiffusionやScaleCrafterは部分的な繰り返し問題を解決しました。
CineScaleは高周波数情報のScale Fusion技術により全ての種類の繰り返しを排除します。
また、T2I、T2Vに加えてI2V、V2Vタスクまで対応範囲を拡張した点も新規性です。

## 3. 実験結果
### 3.1 実験設定
実験では複数のベースラインモデルとの比較を実施しました。
UNetベースモデルとしてSDXLを画像生成、VideoCrafter2を動画生成に使用しています。
DiTベースモデルとしてはWanモデルを採用し、各手法の効果を検証しました。

### 3.2 主要な結果
実験結果は、CineScaleが既存手法を上回る性能を示しています。
8K解像度でのtext-to-image生成において、オブジェクトの繰り返しや不自然な色彩・テクスチャの問題を解決しました。
4K動画生成では最小限のLoRA微調整により高品質な結果を達成しています。

### 3.3 既存手法との比較
ScaleCrafter、DemoFusion、FouriScaleとの比較において、CineScaleは全ての種類の繰り返し問題を解決しながら自然な色彩とテクスチャを維持しています。
特に、FouriScaleの激しい周波数領域の編集による不自然な結果を避けつつ、繰り返しのない生成を実現している点で優位性を示しています。

## 4. 実用性評価
### 4.1 実装の容易性
CineScaleの実装は既存の拡散モデルフレームワークに比較的容易に統合可能です。
UNetベースのモデルではSDXLやVideoCrafter2といった標準的なアーキテクチャに直接適用でき、追加の大規模な変更は不要です。
DiTベースのモデルでも、NTK-RoPEとAttentional Scalingの実装により対応可能です。

### 4.2 計算効率
調整不要の手法として、CineScaleは追加の学習コストを最小限に抑えています。
4K動画生成では最小限のLoRA微調整のみで実現可能で、従来の大規模な再学習と比較してコスト削減を達成しています。
推論時の計算オーバーヘッドも比較的小さく抑えられており、実用的な効率性を示します。

### 4.3 応用可能性
CineScaleの応用可能性は映像制作、デジタルアート、広告業界など幅広い分野にわたります。
8K画像生成と4K動画生成の能力は、プロフェッショナルなコンテンツ制作の要求水準を満たします。
また、I2VとV2V機能により、既存コンテンツの高解像度化や編集作業の効率化も期待されます。

## 5. まとめと所感
### 5.1 論文の意義
本論文は、高解像度の視覚生成における重要な技術的ブレークスルーを達成した価値の高い研究です。
従来の調整不要手法では解決困難だった8K画像生成を初めて実現し、4K動画生成でも最小限の微調整で高品質な結果を得ています。
UNetとDiTの両アーキテクチャに対応する統一的なフレームワークの提供により、将来の拡散モデル開発の方向性を示した点も重要な貢献です。

### 5.2 今後の展望
技術面では、さらなる高解像度化（16K以上）や、より低コストなアーキテクチャの探索が期待されます。
リアルタイム生成への最適化や、モバイルデバイスでの動作に向けた軽量化も重要な発展方向です。
応用面では、インタラクティブな動画編集システムや、VR/AR環境での高解像度コンテンツ生成への拡張が有望です。
商用化においては、コンテンツ制作パイプラインへの統合や、クリエイター向けツールとしての実用化が期待されます。
