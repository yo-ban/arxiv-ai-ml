# Language Models that Think, Chat Better

## 基本情報
- arXiv ID: 2509.20357v1 (https://arxiv.org/abs/2509.20357)
- 著者: Adithya Bhaskar*, Xi Ye*, Danqi Chen (*Equal contribution)
- 所属: Princeton Language and Intelligence, Princeton University
- 投稿日: 2024年9月26日
- カテゴリ: cs.CL, cs.AI

## 簡単に説明すると

この論文は、大規模言語モデル（LLM）の会話性能を向上させる新しい訓練手法を提案しています。
提案手法は「RLMT（Reinforcement Learning with Model-rewarded Thinking）」と呼ばれ、モデルに「考える」能力を与えることを目的としています。
従来のRLVR（強化学習と検証可能な報酬）は数学やコードなどの領域では成功していました。
しかし日常的な会話や創作など開放的なタスクでは性能が限定的でした。
RLMTは長い思考チェーンと人間の嗜好に基づく報酬モデルを組み合わせることで問題を解決しています。
実験では会話ベンチマークで3-7ポイント、その他のタスクで1-3ポイントの性能向上を達成しました。
コードとモデルはhttps://github.com/princeton-pli/RLMTで公開されています。

## 1. 研究概要

### 1.1 背景と動機

近年の言語モデル開発において、「システム2思考」と呼ばれる思考能力の実現は重要な目標となっています。
この能力は人間の知性の特徴的な側面であり、行動の結果を考え抜いて修正する能力を指します。
現在までに、RLVRと呼ばれるパラダイムが検証可能な領域で大きな成功を収めてきました。
RLVRでは自動的に検証可能な報酬を用いてモデルを最適化します。
長い思考チェーン（Chain-of-Thought）による推論を促進しています。

しかしながら、研究者らは重要な限界を発見しました。
RLVRによって訓練されたモデルは開放的なタスクで能力が自然に転移しないことが判明しました。
具体的にはメール作成、エッセイ概要の作成、To-doリスト作成などのタスクです。
数学中心のRLVR訓練を受けたモデルは広く使用される会話ベンチマークで劣る性能を示しました。
この現象は形式的領域での推論スキルが一般的なタスクで汎化しないという根本的な課題を示しています。

### 1.2 主要な貢献

この研究は、RLVRパラダイムを汎用的な会話領域まで拡張する画期的な手法を提示しています。
論文の主要な貢献は以下の通りです。

新しい訓練手法の提案として、RLMTという新しいフレームワークを導入しました。
長い思考チェーンとオンライン強化学習をRLHFの嗜好ベース報酬モデルと組み合わせています。

包括的な実験検証として、2つのモデルファミリーで検証しました。
Llama-3.1-8BとQwen-2.5-7Bを対象とし、複数の最適化アルゴリズムで40回の訓練実行を行いました。
一貫した性能向上を実証しています。

顕著な性能改善として、3つの主要な会話ベンチマークで3-7ポイントの向上を達成しました。
対象はAlpacaEval2、WildBench、ArenaHardV2です。
創作や一般知識タスクでも1-3ポイントの改善を実現しました。

ベースモデルへの直接適用が可能であることを示しました。
SFT段階を経ずに適用でき、R1-Zero訓練に類似したアプローチの有効性を実証しました。

効率性の実証として、わずか7,000のプロンプトでLlama-3.1-8Bベースモデルを訓練しました。
2,500万以上の例を用いた複雑なパイプラインで訓練されたモデルを上回る性能を達成しました。

## 2. 提案手法

### 2.1 手法の概要

RLMTは革新的なアプローチです。
RLVRの2つの核心要素である長いChain-of-Thought推論とオンライン強化学習を組み合わせます。
それらをRLHFで使用される報酬モデルと統合しています。
この手法の基本的な考え方は言語モデルに明示的な推論プロセスを実行させることです。
最終回答を生成する前に推論し、プロセス全体を人間の嗜好データに基づく報酬モデルで評価します。

RLMTの目的関数は以下のように定式化されます。

```
max_θ E_{x~X}[E_{(y,z)~π_θ(·|x)} r(y,x)]
```

ここで各変数はプロンプト、推論トレース、最終回答、報酬モデルを表します。
この定式化により、モデルは多様な実世界のプロンプトに対して長いCoT推論を生成することが要求されます。
嗜好ベースの報酬モデルによってオンライン強化学習で最適化されます。

手法の特徴的な側面は、グラウンドトゥルースの回答に依存しないことです。
プロンプトのみを要求し、多様なプロンプトに対する人間嗜好データで訓練された報酬モデルを使用します。
これにより幅広い監督信号の活用が可能となっています。

### 2.2 技術的詳細

RLMTの実装における重要な設計選択として、複数の強化学習アルゴリズムが検討されています。
研究ではオンポリシーDPO、PPO、GRPOの3つのアルゴリズムを比較検証しています。
これらのアルゴリズムの選択は最終的な性能結果に重要な影響を与えることが実験で示されました。
特にGRPOが最も良い性能を達成しています。

訓練設定において、RLMTは2つの主要なモードで適用可能です。
1つはSFTによる事前学習を経たモデルに適用するモードです。
もう1つはSFT段階をスキップして直接ベースモデルに適用するモードです。
後者のアプローチは特に興味深く、従来の複雑な多段階訓練パイプラインの必要性を疑問視しています。

実際の実装では、Gemini 2.5 Flashによって生成された思考トレースと応答のペアでSFTを行います。
その後、特にSkywork-v2という報酬モデルに対して強化学習でモデルを最適化しています。
この2段階のプロセスにより、モデルは最初に思考パターンを学習します。
続いて人間の嗜好に合わせて微調整されます。

### 2.3 新規性

RLMTの新規性は従来のRLVRとRLHFの境界を打ち破る点にあります。
RLVRが規則ベースの報酬と検証可能な領域に限定されていたのに対し、RLMTは異なるアプローチを採用します。
多様な領域にわたるプロンプトのみを要求し、人間嗜好データで訓練された報酬モデルを活用します。
従来のRLHFは明示的な推論を奨励しない単一的な出力を扱います。
RLMTは内部推論を最初に生成し、その後に最終回答を生成するという構造化されたアプローチを採用しています。

さらにRLMTはベースモデルに直接適用できる柔軟性を持っています。
これは従来の複雑な後訓練パイプラインに対する根本的な挑戦を表しています。
実験結果では、単純なRLMTレシピがSFT段階なしでベースモデルに適用された場合でも優秀な性能を示しました。
数百万の例と拒否サンプリングや繰り返し最適化を含む複雑な後訓練パイプラインを上回る性能を示しています。

## 3. 実験結果

### 3.1 実験設定

実験評価は2つの主要な設定で行われています。
設定1ではSFTウォームスタート後のモデルにRLMTを適用しています。
設定2では直接ベースモデルに適用しています（「ゼロ」設定）。
研究者らはLlama-3.1-8BとQwen2.5-7Bの両方のベース版とInstruct版に設定1を適用しました。
ベースモデルのみに設定2を適用しています。

評価は7つのベンチマークからなる包括的なスイートで行われています。
汎用的な会話、創作、指示従い、一般知識という幅広いタスクをカバーしています。
具体的には会話評価としてAlpacaEval2、WildBench、ArenaHardV2を使用しています。
創作評価としてCreativeWritingV3、指示従い評価としてIFBench、一般知識評価としてMMLU-ReduxとPopQAを採用しています。

ベースラインとの比較においては、RLMTの効果を厳密に分離するための工夫が施されています。
思考なしの対応するRLHFベースラインが各設定で構築されています。
これらのベースラインは同じ訓練設定に従いますが、思考トレースを除去した点のみが異なります。
この設計により、長いCoTを後訓練に統合することの効果を正確に測定できます。

### 3.2 主要な結果

実験結果は、RLMTが一貫して標準的なRLHFパイプラインを上回ることを示しています。
最も印象的な成果の1つは、ベストモデルが高いスコアを達成したことです。
Llama-3.1-8B-InstructをRLMT（GRPO）で訓練したモデルがAlpacaEval2で58.7、WildBenchで50.4のスコアを達成しました。
このモデルは10倍大きなモデルを上回っています。
WildBenchではGPT-4oやClaude-3.7-Sonnetさえも打ち負かしています。

ベースモデルへの直接適用においても、RLMTは実質的な向上をもたらしています。
この設定ではRLMTはLlama-3.1-8Bで平均会話スコア15.6、Qwen-2.5-7Bで29.0を達成しています。
これらの数値はそれぞれの標準Instructモデルよりも5ポイント以上高くなっています。
後者が数百万の例、拒否サンプリング、複雑な後訓練パイプラインに依存していることを考えると驚くべき結果です。

各訓練アルゴリズム間の比較では、GRPOが最も優れた全体的性能を示しています。
DPOとPPOがそれに続いています。
特に会話ベンチマークにおけるRLMTの優位性は顕著です。
思考なしベースラインと比較して改善が見られます。

### 3.3 既存手法との比較

既存の手法との比較において、RLMTは複数の重要な優位性を示しています。
まず数学中心のRLVR訓練を受けた思考モデルと比較すると、RLMTは一般目的の会話において優れた性能を発揮します。
WildBenchでの比較では従来の思考モデルが劣る性能を示すのに対し、RLMT訓練されたモデルは最大48ポイントの向上を達成しています。

標準的なRLHFアプローチとの比較では、RLMTは一貫した向上を示しています。
会話ベンチマークで3-7ポイント、その他のタスクで1-3ポイントの向上です。
この改善は思考能力の統合による直接的な効果と考えられます。

さらに興味深いのは、モデルファミリー間でのRLの前後での性能差です。
Llama-3.1はRL前にはQwen-2.5を下回っていましたが、RL後にはその傾向が逆転しています。
これはRLMTが事前訓練やSFT中に十分に最適化されていない場合でも効果があることを示しています。
モデルの特定の能力を強化するのに役立つという仮説を支持しています。

## 4. 実用性評価

### 4.1 実装の容易性

RLMTの実装は既存のRLHFフレームワークの自然な拡張として設計されています。
比較的実装しやすい手法です。
主要な変更点はモデルに思考トレースの生成を要求することです。
報酬モデルの選択も重要な変更点です。
研究者らはDPO、PPO、GRPOという複数の既存アルゴリズムでの実装を実証しています。
これは手法の汎用性を示しています。

特に注目すべきは、SFTステージをスキップして直接ベースモデルに適用できる柔軟性です。
これにより複雑な多段階訓練パイプラインの必要性が削減されます。
実装コストと複雑さが軽減されます。
ただし最適な性能を得るためにはプロンプトミックスと報酬モデルの慎重な選択が重要です。
プロンプトミックスと報酬モデルの選択がアブレーション研究で重要であることが示されています。

### 4.2 計算効率

計算効率の観点から、RLMTは思考トレースの生成により追加の計算オーバーヘッドを伴います。
しかしこの追加コストは得られる性能向上と比較して十分に正当化されます。
特に印象的なのは、わずか7,000のプロンプトという比較的少ないデータセットで性能向上を達成していることです。

従来の手法が数百万の例を必要とするのに対し、RLMTのデータ効率性は実用的な展開において大きな利点となります。
また異なるアルゴリズム間でのトレードオフも考慮します。
GRPOが最良の性能を示すものの、実装の複雑さとのバランスを考慮した選択が重要です。

### 4.3 応用可能性

RLMTの応用可能性は極めて広範囲にわたります。
会話AI、創作支援、複雑な問題解決、意思決定支援など様々な領域での活用が期待されます。
明示的な推論が有益な領域です。
特に従来のRLVRが限定的だった開放的なタスクにおいて、RLMTは大きな可能性を示しています。

研究結果はRLMT訓練後のモデルが推論行動の変化を示すことを明らかにしています。
線形的なチェックリスト形式のアウトラインから、より豊かな推論行動への移行が見られます。
制約の列挙、テーマのグループ化、改良の繰り返しなどです。
これはモデルが単に表面的な改善ではなく、根本的な推論スタイルの変化を学習していることを示唆しています。

将来的には、この手法は教育支援システム、技術コンサルティング、創作支援ツールなどへの展開が期待されます。
複雑な思考プロセスが要求される様々な実世界アプリケーションでの活用が可能です。

## 5. まとめと所感

### 5.1 論文の意義

この研究は言語モデルの後訓練パラダイムに対する根本的な再考を促す重要な貢献を提供しています。
RLMTの提案により従来のRLVRとRLHFの境界を超えた新しいアプローチが示されました。
思考能力の統合が汎用的なタスクにおいて性能向上をもたらすことが実証されました。
特に複雑な多段階後訓練パイプラインなしに単純なレシピでベースモデルから高性能モデルを作成できることが示されました。
業界の標準的な慣行に挑戦する画期的な発見です。

実験の包括性も特筆すべき点です。
40回の訓練実行、複数のモデルファミリー、3つの異なるアルゴリズムにわたる検証が行われました。
結果の信頼性と一般化可能性を強く支持しています。
また定量的結果に加えてモデルの推論スタイルの質的変化を分析していることも重要です。
理解を深める貢献となっています。

### 5.2 今後の展望

この研究は将来の研究に向けて複数の重要な方向性を示しています。
まず思考能力の改善が既存モデルの特性の増幅によるものかという根本的な問題の解明が必要です。
SFTウォームスタートやRL訓練中の新しい特性の学習によるものかを明らかにします。
この理解はより効果的な訓練パイプラインの設計において重要な意味を持ちます。

また研究者らが認めているように最適化の余地は大きいです。
内部CoTのフォーマット、ハイパーパラメータ、プロンプトミックスの構築における改善が可能です。
これらの要素のさらなる最適化により結果をさらに押し上げることが可能と考えられます。

長期的にはこの研究は思考能力をより広範囲に理解し活用するという大きなビジョンを提示しています。
現在の7つのベンチマークを超えたより包括的な評価が求められます。
異なる領域における思考パターンの分析や人間の認知プロセスとの比較など多くの興味深い研究課題が残されています。
RLMTは言語モデルが真に「考える」能力を獲得するための重要な1歩を踏み出したと言えるでしょう。