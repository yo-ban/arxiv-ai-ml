# SSRL: Self-Search Reinforcement Learning

## 基本情報
arXiv ID: 2508.10874v1 (https://arxiv.org/abs/2508.10874)

著者: Yuchen Fan、Kaiyan Zhang、Heng Zhou、Yuxin Zuo、Yanxu Chen、Yu Fu。その他Xinwei Long、Xuekai Zhu、Che Jiang、Yuchen Zhang、Li Kang、Gang Chen。また Cheng Huang、Zhizhou He、Bingning Wang、Lei Bai、Ning Ding、Bowen Zhou。

所属: Tsinghua University、Shanghai Jiao Tong University、Shanghai AI Laboratory。その他University College London、CSCEC Third Bureau、WeChat AI。

投稿日: 2025年08月16日。

カテゴリ: cs.LG, cs.AI。

## 簡単に説明すると

この論文は、大規模言語モデル（LLM）を外部検索エンジンへの依存なしにWebシミュレータとして活用する新しいアプローチを提案しています。

従来の強化学習ベースの検索エージェント（Search-R1、ZeroSearchなど）は外部検索エンジンやAPIへの高コストなアクセスを必要としていました。本研究では「Self-Search」という概念を導入し、LLMが内部の知識パラメータから自動的に検索クエリと情報を生成する手法を開発しました。

Self-Search Reinforcement Learning（SSRL）は、フォーマットベースとルールベースの報酬を通じてこの自己検索能力を強化します。この手法は外部ツールへの依存なしに内部知識の活用を自律的に改善します。実験結果では、SSRLで訓練されたモデルがSearch-R1やZeroSearchなどの外部検索APIベースのベースラインを上回る性能を示しています。さらに実際のWebアクセスを使用するオンライン設定への堅牢なsim-to-real転移を実現しています。

GitHub Repository: https://github.com/TsinghuaC3I/SSRL

## 1. 研究概要
### 1.1 背景と動機

現代の強化学習ベースの検索エージェントは、数学やコード生成ではLarge Reasoning Models（LRM）として成功を収めています。しかし、エージェントのコンテキストでのツール学習では、外部Web検索エンジンとのインタラクションに高いコストがかかります。

特にRL訓練中は大量のロールアウトとマルチターンのツール呼び出しが必要であり、Search-R1やKimi V2のようなモデルでさえコストが問題となっています。一方で、LLMは大規模Webコーパスでの事前訓練により世界知識を含んでおり、しばしば外部検索なしで質問に答えることができます。

ZeroSearchのような研究では、ファインチューンされたLLMがWeb検索を効果的に置き換え、安定で信頼性のある知識を提供できることが示されています。これは、セミリアル設定を採用することで検索RLのコストを約50%削減できることを示しています。

### 1.2 主要な貢献
[詳細な説明]。

主要な貢献は以下の通りです。

1. LLMの内在的な検索能力の定量化を行いました。
2. 自己検索を用いた強化学習（SSRL）手法の開発を実現しました。
3. 外部検索エンジンへの依存を削減する完全シミュレーション環境を構築しました。
4. sim-to-real転移の有効性を実証しました。

## 2. 提案手法
### 2.1 手法の概要

本研究では、完全シミュレーション（full-sim）設定での自己検索RLを探索し、訓練中は実際の検索を一切使用しません。主な研究質問は以下の2点です。

第一に、内部知識のみを使用した検索ベースQAタスクでのLLMの性能限界はどこにあるか。第二に、完全シミュレーション検索RLは推論時の実際Web検索での効果的なsim-to-real転移を可能にするか。

手法の中核は「Self-Search」であり、LLMがパラメータ内の埋め込み知識を基盤としてクエリと情報の両方を生成し、外部検索エンジンへのクエリをシミュレートします。これにより、単一の生成軌跡内でマルチターンのツールフォーマット出力を使用して検索プロセスをシミュレートします。

### 2.2 技術的詳細

**pass@kの定式化**

問題iに対してKサンプルを生成し、正確な応答数をC_iとします。pass@kは以下の式で計算されます。

pass@k = (1/問題数) ∑_{i=1}^{問題数} (1 - C(K-C_i, k)/C(K, k))

**スケーリング則の定式化**

カバレッジとサンプル数の相関をモデル化します。log(カバレッジ) ≈ aサンプル^b (パラメータa, bはフィットされる)、つまり カバレッジ ≈ exp(aサンプル^b)。

**SSRLのRL目的関数**

従来の外部検索ベースの目的関数: max_{π_θ} E[r_φ(x,y)] - β D_{KL}[π_θ(y|x;R) || π_{ref}(y|x;R)]。

SSRLでは取得情報Rがπ_θと同じ分布に従うため、目的関数は以下に簡略化されます。

max_{π_θ} E[r_φ(x,y)] - β D_{KL}[π_θ(y|x) || π_{ref}(y|x)]

**報酬モデリング**

結果報酬: R(ŷ, y) = 1 (is_equivalent(ŷ, y)の場合), -1 (それ以外)。

結合報酬は4つのケースで定義されます。正解でありフォーマット正しい場合はr_φ(y_i, y) = 1です。正解でフォーマット間違いの場合はr_φ(y_i, y) = 1-λ_fです。不正解でフォーマット正しい場合はr_φ(y_i, y) = λ_fです。不正解かつフォーマットも間違いの場合はr_φ(y_i, y) = 0です。

ここでλ_f = 0.1です。

### 2.3 新規性

本研究の主要な新規性は、完全シミュレーション環境での自己検索アプローチにあります。従来のSearch-R1やZeroSearchは部分的または完全な外部検索に依存していましたが、SSRLは主に内部知識に依存します。

特に重要なのは、モデルが推論者と知識取得器の両方の役割を同時に果たすことです。これにより、外部依存性をなくしてパラメトリック知識から関連情報を効果的に抽出できます。

さらに、Information Token Maskという技術を導入し、自己生成された情報であってもマスキングが性能向上に有効であることを示しました。これは従来の外部検索ベースの研究での知見を自己検索設定にも適用できることを示しています。

## 3. 実験結果
### 3.1 実験設定

**ベンチマーク**

評価は7つの主要ベンチマークで実施されました。以下の3カテゴリに分類されます。一般的な質問応答（Natural Questions、TriviaQA）。マルチホップ質問応答（HotpotQA、Musique、Bamboogle、2WikiMultiHopQA）。あいまいな質問応答（BrowseComp）です。

**モデル**

モデルは3つのファミリーにわたって評価されました。Qwen2.5、Llama3（Llama-3.1とLlama-3.2を含む）、Qwen3です。パラメータスケールは0.6Bから72Bまでの幅広い範囲でテストしました。サンプリングパラメータはtemperature 0.7、top-k -1、top-p 0.95、最大トークン数8192で統一されました。

**訓練設定**

RL実験は主にLlamaモデルファミリーで実施され、Llama-3.2-3B（Base/Instruct）とLlama-3.1-8B（Base/Instruct）を使用しました。訓練データセットはNQとHotpotQAの組み合わせで、一般的QAとマルチホップQAのミックスを確保しました。GRPOをデフォルトアルゴリズムとし、PPO、REINFORCE++などの代替手法でも検証しました。

### 3.2 主要な結果

**Test-Time Scalingでの性能向上**

サンプル数の増加に伴って一貫した性能向上が観察されました。特にBamboogleでは、Llama-3.1-8B-Instructがpass@1024で87.2%を達成し、pass@1に比べて150%の大幅な改善を示しました。この結果は3つのモデルファミリーすべてで確認され、特にLlamaシリーズで顕著でした。

BrowseCompでの結果は特に注目すべきです。GPT-4oによる検索では1.9%、o1で10%の性能でした。一方、Self-SearchではQwen2.5-14B-InstructとLlama-3.1-8B-Instructが十分なサンプル数でo1の性能を上回りました。

**モデルサイズ間の性能ギャップの縮小**

繰り返しサンプリングにより、小さなモデルが約10倍のパラメータを持つモデルに匹敵する性能を達成できました。例えばTriviaQAで1024サンプルでは、Llama-3.1-8B-Instructが81.2%、Llama-3.1-70B-Instructが81.4%でした。巨大なモデルサイズの違いにもかかわらず無視できる程度の差しかありませんでした。

**SSRLとTTRLの性能比較**

TTRL（Test-Time RL）を適用した結果、GRPOと比較して大幅な性能向上が観察されました。Llama-3.2-3B-Instructでは平均性能が59%改善し、特にBrowseCompではSSRLモデルが外部検索エンジンなしでも大幅な改善を示しました。

### 3.3 既存手法との比較

**SSRLの優位性**

実験結果では、自己回帰型の内部検索で訓練されたモデルが優れた性能を示しました。これらのモデルは、他のLLMやGoogle検索などの外部検索エンジンに依存する手法を一貫して上回りました。特にLlama-3.1-8B-Instructでは平均性能43.1%を達成し、Search-R1やZeroSearchなどの外部検索APIベースのベースラインを上回りました。

**Instructionモデルの内部知識活用**

同一データで同期間訓練した場合、InstructionモデルがBaseモデルよりも明確に優れた性能を示しました。これは督学ファインチューニング中に追加の知識操作が組み込まれたことを示唆しています。ただし、この優位性はコンテキスト依存であり、Baseモデルは外部情報源が利用可能な場合に優れた性能を示しました。

**訓練効率と安定性**

ZeroSearchと比較して、SSRLは訓練効率を5.53倍改善し、訓練全体を通じて安定した報酬成長を示しました。SSRLは初期訓練段階では外部知識の限界により相対的に低い訓練報酬を示しましたが、優れた効率と堅牢性がこの初期の不利を補いました。

## 4. 実用性評価
### 4.1 実装の容易性
[評価]。

### 4.2 計算効率
[評価]。

### 4.3 応用可能性
[評価]。

## 5. まとめと所感
### 5.1 論文の意義
[考察・総合評価]。

### 5.2 今後の展望
[将来性や改善点]。
