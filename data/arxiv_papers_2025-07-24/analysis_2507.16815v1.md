# ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning

## 基本情報
- **arXiv ID**: 2507.16815v1 (https://arxiv.org/abs/2507.16815)
- **著者**: Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang
- **所属**: NVIDIA, National Taiwan University
- **投稿日**: 2025年07月24日
- **カテゴリ**: cs.CV, cs.AI, cs.LG

## 簡単に説明すると
この論文は、視覚・言語・行動（VLA）タスクにおいて、高レベルの推論と低レベルの行動実行を橋渡しする新しいフレームワーク「ThinkAct」を提案しています。
従来のVLAモデルは入力から行動への直接的なマッピングを学習し、明示的な推論なしに動作するため、複数ステップの計画や複雑なタスクへの適応が困難でした。
ThinkActは、強化学習を用いてマルチモーダルLLMを訓練し、目標達成と軌道一貫性に基づく視覚的報酬によって導かれる具現化推論計画を生成します。
これらの推論計画は視覚的計画潜在表現に圧縮され、下流の行動モデルの条件付けに使用されます。
実験結果は、ThinkActが複雑な具現化AIタスクにおいて、少数ショット適応、長期計画、自己修正行動を可能にすることを示しています。

関連リンクは以下の通りです。
- プロジェクトページ: https://jasper0314-huang.github.io/thinkact-vla/

## 1. 研究概要
### 1.1 背景と動機
マルチモーダル大規模言語モデル（MLLM）は、視覚的質問応答や画像・動画キャプション生成などのタスクで素晴らしい進歩を遂げています。
しかし、これらのモデルは、長期目標のための多段階計画や動的環境との相互作用に苦戦しています。
現在のVLAモデルは、視覚・言語入力から低レベル行動への直接的な学習により、多様な視覚シーンでの推論や長期計画の能力が制限されています。

既存のアプローチには以下の課題があります。
- エンドツーエンドVLAモデル（OpenVLA、TraceVLAなど）：構造化された計画なしに視覚言語入力から直接行動を予測し、複雑な指示や長期目標への対応が困難。
- 教師ありCoTアプローチ（ECoT、RADなど）：高品質な推論トレースアノテーションに依存し、特定の視覚シーンや推論パターンへの過学習が起きやすい。
- RLベースの推論アプローチ（Video-R1など）：QAスタイルの報酬を使用し、長期計画や実世界での行動実行との接続を完全にサポートできない。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです。
- 抽象的推論と低レベル制御をコンパクトな潜在表現で橋渡しする視覚潜在計画フレームワーク
- 目標達成と軌道分布マッチングを組み合わせた新規な行動整合視覚フィードバック報酬
- 非同期動作をサポートし、高速制御を維持しながら低速思考を可能にする二重システムアーキテクチャ
- 自己修正、少数ショット適応、長期計画などの創発的能力の実証

## 2. 提案手法
### 2.1 手法の概要
ThinkActは、構造化推論と実行可能な行動を接続する二重システムアーキテクチャです。
主要コンポーネントは以下の通りです。
- MLLM推論モジュール（Fθ）：観察と指示に基づいて視覚計画潜在表現を生成
- 行動モジュール（πφ）：推論生成された計画に条件付けられた低レベル行動を実行

### 2.2 技術的詳細
**視覚潜在計画**：
MLLMは推論ステップ（vt）と視覚計画埋め込み（ct）を自己回帰的に生成します。
視覚計画はグリッパーの動きを表す2D軌道ポイントにデコードされます。
非同期動作により、各潜在計画ctはN個の環境相互作用に対応します。

**行動整合強化学習**：
報酬設計は以下の要素から構成されます。
- 目標報酬（rgoal）：予測された開始・終了位置と真の軌道端点を比較
- 軌道報酬（rtraj）：動的時間伸縮（DTW）距離を使用して物理的に妥当なグリッパー動作への適合を正則化
- 結合報酬：r = 0.9 rvisual + 0.1 rformat（rvisual = 0.5 rgoal + 0.5 rtraj）

**強化微調整**：
グループ相対政策最適化（GRPO）を使用して具現化推論を促進します。
複数の応答をサンプリングし、相対的な報酬品質に基づいて最適化します。
過度の逸脱を防ぐためKL正則化を組み込みます。

**推論強化行動適応**：
潜在プロジェクタ（32クエリのQ-Former）が視覚計画ctを行動モデル入力空間に接続します。
行動モデルは推論生成された計画に条件付けられた模倣学習により訓練されます。

### 2.3 新規性
本研究の新規性は以下の点にあります。

第一に、タスク固有の監督なしに具現化シーンに基づく長期推論を可能にする行動整合視覚フィードバック報酬の導入です。
これは目標達成と軌道分布マッチングを組み合わせた初めての試みです。

第二に、抽象的推論と低レベル制御を橋渡しする視覚潜在計画フレームワークです。
コンパクトな潜在表現により、計算効率を保ちながら複雑な推論を可能にします。

第三に、行動実行と視覚基盤具現化推論を相互に強化する二重システムアーキテクチャです。
これにより、推論ガイドされた計画を通じて新しい環境への柔軟な適応が可能になります。

## 3. 実験結果
### 3.1 実験設定
**評価ベンチマーク**：
- ロボット操作：SimplerEnv（Google-VM、Google-VA、Bridge-VM）、LIBERO
- 具現化推論：EgoPlan-Bench2、RoboVQA、OpenEQA

**実装詳細**：
- ベースモデル：推論モジュールにQwen2.5-VL 7B
- 行動モデル：OXEデータセットで事前訓練されたDiTベースポリシー（432Mパラメータ）
- 訓練：SFTコールドスタート後のGRPO強化学習による多段階アプローチ
- 計算リソース：80GBメモリのNVIDIA A100 GPU 16台

### 3.2 主要な結果
**ロボット操作性能**：

SimplerEnvベンチマークでは以下の成功率を達成しました。
- Google-VM：71.5%（ベースライン比+15.5%）
- Google-VA：65.1%（ベースライン比+16.9%）
- Bridge-VM：43.8%（ベースライン比+11.4%）

LIBEROベンチマークでは84.4%の全体成功率を達成し、全手法中最高の性能を示しました。
最新のCoT-VLAを上回る結果となりました。

**具現化推論性能**：
- EgoPlan-Bench2：2位の手法より+2.5%改善
- RoboVQA：+4.1 BLEUスコア改善
- OpenEQA：強化されたシーン理解を示す強力な性能

**少数ショット適応**：
タスクあたり10デモンストレーションのみで以下の改善を達成しました。
- LIBERO-Goal（新しいスキル）：Magma比+7.3%
- LIBERO-Spatial（未見環境）：Magma比+9.5%

### 3.3 既存手法との比較
アブレーション研究により、各コンポーネントの重要性が明らかになりました。

**報酬コンポーネントの影響**：
- 軌道報酬の除去：顕著な性能低下
- 目標報酬の除去：長期推論の性能低下
- 両視覚報酬なし：SFTベースラインからの改善はわずか

**創発的能力**：
ThinkActは以下の能力を示しました。
- 自己修正：時間的文脈が与えられた場合、失敗を検出して再計画が可能
- 少数ショット適応：推論により新しいスキルと環境への汎化が向上
- 長期計画：複雑なタスクを意味のあるサブゴールに分解することに成功

## 4. 実用性評価
### 4.1 実装の容易性
ThinkActの実装は、既存のMLLMと行動モデルの組み合わせにより比較的容易です。
主要コンポーネントは以下の通りです。
- 標準的なMLLMアーキテクチャ（Qwen2.5-VL）の使用
- Q-Formerベースの潜在プロジェクタ
- 事前訓練済み行動モデルへの条件付け

コードの実装は明確に構造化されており、各モジュールが独立して開発・テスト可能です。

### 4.2 計算効率
ThinkActは効率的な計算を実現しています。
- 非同期動作により、推論と行動実行を並列化
- 視覚計画潜在表現により、長い推論連鎖を圧縮
- 7Bパラメータモデルで高性能を達成し、大規模モデルよりも効率的

推論時間は標準的なVLAモデルと同等で、追加の計算オーバーヘッドは最小限です。

### 4.3 応用可能性
ThinkActは以下のような幅広い応用が期待されます。
- 家庭用ロボット：複雑な日常タスクの実行
- 産業自動化：新しいタスクへの迅速な適応
- 支援技術：高齢者や障害者の支援
- 研究プラットフォーム：具現化AI研究の基盤

特に、少数ショット適応能力により、新しい環境やタスクへの迅速な展開が可能です。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、具現化AIにおける推論と行動の統合という重要な課題に取り組んでいます。
特に注目すべき点は以下の通りです。

**理論と実践の架橋**：抽象的な推論能力と具体的な物理的行動を効果的に接続する手法を提案し、実世界のロボットタスクで有効性を実証しています。

**汎用性の実現**：タスク固有の監督なしに多様なロボット操作タスクで高性能を達成し、少数ショット学習により新しい環境への適応も可能にしています。

**創発的能力の発現**：自己修正や長期計画など、明示的に訓練されていない能力が創発的に現れたことは、より知的なロボットシステムへの道を示しています。

### 5.2 今後の展望
著者らが示唆する今後の研究方向は以下の通りです。
- より複雑な環境での評価と応用
- マルチモーダル入力（触覚、音声など）の統合
- 人間との協調作業への拡張
- より大規模なモデルでの性能向上の検証

さらに、以下の発展も期待されます。
- リアルタイム性の向上による高速タスクへの対応
- 説明可能性の向上による信頼性の確保
- 転移学習による新規ドメインへの効率的な適応
- 安全性とロバスト性の強化による実世界展開の促進