# Beyond Binary Rewards: Training LMs to Reason about Their Uncertainty

## 基本情報
- arXiv ID: 2507.16806v1 (https://arxiv.org/abs/2507.16806)
- 著者: Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas
- 所属: Massachusetts Institute of Technology
- 投稿日: 2025年07月24日
- カテゴリ: cs.LG, cs.AI

## 簡単に説明すると
この論文は、言語モデルが正確な回答を生成するだけでなく、その回答に対する信頼度も正しく推定できるようにする新しい学習手法「RLCR（Reinforcement Learning with Calibration Rewards）」を提案しています。
従来の強化学習では回答の正誤のみを評価していましたが、RLCRでは回答の正確性と信頼度の正しさを同時に最適化します。
モデルが回答と数値的な信頼度を出力し、正解時の高信頼度や不正解時の低信頼度に報酬を与え、過信や過小評価にペナルティを課します。
この手法により、医療や法律など高リスクな分野でより信頼できるAIシステムの構築が可能になります。

## 1. 研究概要
### 1.1 背景と動機
近年、推論モデル（reasoning models）と呼ばれる、自然言語で「声に出して考える」ように訓練された言語モデルが、数学やプログラミングなどの困難なタスクで優れた性能を達成しています。
これらのモデルは通常、単純な二値正誤報酬（RLVR: Reinforcement Learning with Verifiable Rewards）を用いて強化学習で訓練されます。
しかし、この報酬関数には重要な限界があります。正解を確信して出力した場合と、単に推測で正解した場合を同等に扱い、誤答と回答を控えることを同じペナルティとして扱うため、過信的な推測を促してしまいます。

実際、LLMは最初は良好なキャリブレーションを持っていても、強化学習で訓練した後は過信的になる傾向があります。
特に推論モデルは、ベースモデルと比較してキャリブレーションが悪化し、幻覚（hallucination）率が増加します。
これは医療や法律などの高リスク領域では致命的な限界となります。これらの領域では、モデルが正確であるだけでなく、必要に応じて不確実性を伝える必要があります。

### 1.2 主要な貢献
本研究では、推論モデルが正確性とキャリブレーションの両方を最適化できることを示し、以下の貢献をしています。

- 適切スコアリングルール（proper scoring rules）の理論を活用し、正確性とキャリブレーションを同時に最適化するRLCR手法を提案しました。
- RLCR報酬関数が理論的に正確性とキャリブレーションの両方を促進することを証明しました。
- 事実質問応答と数学的推論タスクで、RLCRがタスク精度を維持しながらキャリブレーションを15%から85%改善することを実証しました。
- 分布外タスクでも、RLCRがベースモデル、RLVR訓練モデル、事後的な信頼度推定器を上回ることを示しました。
- 言語化された信頼度がテスト時スケーリング手法に組み込めることを実証しました。

## 2. 提案手法
### 2.1 手法の概要
RLCRの核心的なアイデアは、言語モデルを強化学習で訓練する際に、正確性とキャリブレーションの両方を促進する報酬を使用することです。
モデルは以下の構造化された形式で出力を生成します。

1. 思考過程（<think>タグ内）で問題を解くための推論過程を記述します。
2. 回答（<answer>タグ内）で最終的な答えを提示します。
3. 不確実性分析（<analysis>タグ内）で回答に対する不確実性の推論を行います。
4. 信頼度スコア（<confidence>タグ内）で0から100の数値を出力します。

この構造により、モデルは回答を生成するだけでなく、その回答に対する信頼度も明示的に推論し、数値化します。

### 2.2 技術的詳細
RLCRの報酬関数は以下のように定義されます。

R_RLCR(回答, 信頼度, 正解) = 正誤指示関数 - (信頼度 - 正誤指示関数)²

ここで、回答は予測された答え、信頼度は0から1にスケールされた値、正解は真の答え、正誤指示関数は正解なら1、不正解なら0を返す関数です。

この報酬関数は2つの項から構成されています。
- 第1項（正確性報酬）では、正解時に1、不正解時に0を与えます。
- 第2項（キャリブレーション報酬）では、Brierスコアに基づいて信頼度と実際の正誤の差を罰します。

理論的に重要な性質として、この報酬関数は以下を満たします。
1. キャリブレーション促進では、任意の回答に対して、期待報酬は信頼度が真の成功確率と一致するときに最大化されます。
2. 正確性促進では、キャリブレートされた予測の中で、成功確率が最も高い予測が最大の期待報酬を得ます。

この性質は、有界な適切スコアリングルールを使用する限り成立します。対数損失のような非有界なスコアリングルールでは、この性質は成立しません。

### 2.3 新規性
既存手法との主な違いは以下の通りです。

1. 正確性とキャリブレーションの同時最適化において、従来の手法は正確性のみ（RLVR）またはキャリブレーションのみを最適化していましたが、RLCRは両方を同時に最適化します。

2. 推論過程での不確実性推論により、モデルが回答生成後に不確実性について明示的に推論することで、より良いキャリブレーションを実現します。

3. 理論的保証として、報酬関数が正確性を犠牲にすることなくキャリブレーションを改善することを数学的に証明しています。

4. 単一モデルでの実現により、別途信頼度を推定する器械を訓練する必要がなく、単一のモデルで回答生成と信頼度推定を実施します。

5. 分布外汎化において、訓練タスクとは異なるタスクでもキャリブレーションが改善されます。

## 3. 実験結果
### 3.1 実験設定
ベースモデルとして、Qwen2.5-7Bベースモデルを使用しました。Qwenファミリーは強化学習タスクで優れた性能を示すことで知られています。

訓練データセットは以下の2つを使用しました。
- HotpotQA-Modifiedは、マルチホップ推論を要する事実質問応答データセットです。情報の完全性を変化させるため、必要な段落を0個、1個、2個含む3つの条件を均等に混合した20,000例を使用しました。
- Big-Mathは、数学問題250,000個以上を含む大規模データセットです。難易度を適切に保つため、LLaMA-8B解答率0-70%の問題を選択し、数値回答のみに限定した15,000例を使用しました。

評価指標として以下を使用しました。
- 精度（Accuracy）はタスク性能の指標です。
- AUROCは陽性と陰性クラスを区別する能力を測定します。
- Brierスコアは信頼度と正解の二乗差を計算します。
- 期待キャリブレーション誤差（ECE）は信頼度をビンに分け、平均正解率と平均信頼度の差を計算します。

評価データセットとして、HotpotQA、SimpleQA、TriviaQA、GPQA、Math500、GSM8K、Big-Math、CommonsenseQAの8つのベンチマークを使用しました。

### 3.2 主要な結果
**分布内性能（HotpotQA）**：
- RLCRは精度62.1%でRLVR（63.0%）とほぼ同等の性能を達成
- キャリブレーション指標では大幅な改善：ECE 0.37→0.03、Brierスコア 0.37→0.21
- ベースモデルとRLVRは過信的で不良なキャリブレーションを示したが、RLCRは良好なキャリブレーションを実現

**分布外汎化**：
- 6つの分布外データセットの平均で、RLCRは精度56.2%を達成し、RLVR（53.9%）を上回る
- キャリブレーション指標でも一貫して最良の性能：Brierスコア0.21、ECE 0.21
- RLVRは分布外タスクでベースモデルよりもキャリブレーションが悪化したが、RLCRは改善を示した

**数学タスク**：
- 3つの数学データセット（Math500、GSM8K、Big-Math）の平均で、RLCRは72.7%の精度を達成
- SFT+RLCR変種（事前に教師あり学習を行う）は最良のキャリブレーション（ECE 0.08）を達成したが、分布外精度は低下（43.8%）
- 通常のRLCRは分布外設定でより良いトレードオフを提供（精度50.9%、ECE 0.25）

### 3.3 既存手法との比較
以下の手法と比較評価を実施：

1. **RLVR**：従来の二値正誤報酬による強化学習。高精度だが過信的
2. **RLVR + 分類器**：RLVRの出力に対して別途信頼度推定器を訓練。高コストだが良好なキャリブレーション
3. **RLVR + プローブ**：最終層埋め込みから線形プローブで信頼度を推定。分類器より安価だが性能は劣る
4. **回答確率**：生成された回答トークンの平均確率を信頼度として使用。過信的で不良なキャリブレーション

**主要な比較結果**：
- RLCRは単一モデルで回答生成と信頼度推定を行いながら、2つのモデルを使用する分類器アプローチと同等以上の性能を達成
- 特に分布外タスクで、RLCRは全ベースラインを上回る性能を示した
- 計算効率の面でも、単一モデルであるRLCRは分類器アプローチより優れている

## 4. 実用性評価
### 4.1 実装の容易性
RLCRの実装は比較的シンプルで、既存の強化学習フレームワークに容易に統合できる。主な実装要件は以下の通り：

1. **報酬関数の変更**：標準的な二値報酬にBrierスコア項を追加するだけで実装可能
2. **出力フォーマット**：構造化タグ（<think>、<answer>、<analysis>、<confidence>）を使用することで、出力の解析が容易
3. **既存インフラの活用**：GRPOなどの標準的な強化学習アルゴリズムをベースに使用可能
4. **フォーマット報酬**：タグの正しい使用を促すための追加報酬により、高い形式遵守率を実現

実装の課題としては、数学タスクでより高品質な不確実性分析を得るために、軽量なSFTウォームアップフェーズが有効であることが判明した。これは500例程度の小規模データで実施可能である。

### 4.2 計算効率
RLCRは計算効率の面で複数の利点を持つ：

1. **単一モデルアーキテクチャ**：回答生成と信頼度推定を同じモデルで行うため、別途信頼度推定器を訓練・推論する必要がない

2. **訓練時の効率性**：標準的な強化学習と同等の計算コストで、追加の計算は報酬計算のみ

3. **推論時の柔軟性**：
   - 通常の推論：1回の前向き計算で回答と信頼度を同時に生成
   - 信頼度改善：固定回答に対して複数の不確実性分析をサンプリングし、アンサンブルすることで信頼度を改善可能

4. **メモリ効率**：別途の分類器モデルを保持する必要がないため、メモリ使用量を削減

### 4.3 応用可能性
RLCRは幅広い実用的応用が可能である：

**1. 高リスク領域での応用**
- 医療診断：不確実な診断に対して適切に低い信頼度を示すことで、人間の専門家への相談を促進
- 法律相談：複雑な法的問題で確実性が低い場合に、適切に不確実性を伝達
- 金融アドバイス：投資判断などで、リスクを適切に伝える

**2. テスト時スケーリング**
- 信頼度重み付き多数決：複数の回答をサンプリングし、信頼度で重み付けすることで精度向上
- 最大信頼度選択：複数の候補から最も信頼度の高い回答を選択
- 信頼度アンサンブル：固定回答に対して複数の不確実性分析を生成し、より良いキャリブレーションを実現

**3. 対話システムへの統合**
- ユーザーへの透明性：AIアシスタントが自身の確信度を明示的に伝えることで、より信頼できるインタラクションを実現
- 適応的な振る舞い：低信頼度の場合に追加の確認や代替案の提示を行う

**4. 研究・開発への応用**
- モデル評価：キャリブレーションを考慮したより包括的なモデル評価
- アクティブラーニング：不確実性の高いサンプルを優先的に学習データに追加
- エラー検出：過信的な誤答を自動的に検出し、修正を促す

## 5. まとめと所感
### 5.1 論文の意義
本研究は、言語モデルの実用性と信頼性を大幅に向上させる重要な貢献をしている。主な意義は以下の通りである：

1. **理論と実践の橋渡し**：統計的決定理論の適切スコアリングルールを言語モデルの強化学習に適用し、理論的保証を持つ実用的な手法を開発した。これにより、正確性を犠牲にすることなくキャリブレーションを改善できることを証明した。

2. **信頼できるAIへの貢献**：過信的な推測を抑制し、適切な不確実性推定を可能にすることで、高リスク領域でのAI活用への道を開いた。医療、法律、金融などの分野では、この特性が不可欠である。

3. **分布外汎化の改善**：訓練タスクとは異なるタスクでもキャリブレーションが改善されるという発見は、実世界での適用可能性を大きく高める。

4. **単純さと効果性の両立**：複雑な追加モジュールを必要とせず、報酬関数の変更だけで実装できる簡潔さは、広範な採用を促進する可能性がある。

5. **推論の透明性向上**：モデルが不確実性について明示的に推論することで、なぜその信頼度を持つのかを人間が理解しやすくなる。

### 5.2 今後の展望
本研究は有望な成果を示したが、以下の改善点と研究方向が考えられる：

**改善が必要な点**：
1. **絶対的なキャリブレーション性能**：分布外タスクでのキャリブレーション誤差は相対的に改善されたが、絶対値としてはまだ高い場合がある
2. **矛盾する回答への高信頼度**：複数の相反する回答に高い信頼度を割り当てる可能性がまだ残っている
3. **SFTウォームアップの副作用**：数学タスクでSFTウォームアップが分布外精度を低下させる問題への対処

**将来の研究方向**：
1. **他の適切スコアリングルールの探索**：Brierスコア以外の有界な適切スコアリングルールの効果を検証
2. **マルチモーダルへの拡張**：視覚-言語モデルなど、他のモダリティへの適用
3. **より複雑な不確実性モデリング**：単一の信頼度スコアではなく、不確実性の種類（認識的/偶然的）を区別するモデル
4. **人間との協調**：適切にキャリブレートされたモデルを使った人間-AI協調システムの開発
5. **大規模モデルへの適用**：より大きなモデルでの効果検証と、スケーリング則の理解

この研究は、単に正確なだけでなく、自身の限界を理解し適切に伝えることができる、真に信頼できるAIシステムへの重要な一歩となっている。
