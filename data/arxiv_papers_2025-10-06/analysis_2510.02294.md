# F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data

## 基本情報
- **arXiv ID**: 2510.02294v1 (https://arxiv.org/abs/2510.02294)
- **著者**: Ziyin Zhang, Zihan Liao, Hang Yu, Peng Di, Rui Wang
- **所属**: Ant Group, Shanghai Jiao Tong University
- **投稿日**: 2025年10月03日
- **カテゴリ**: cs.LG

## 簡単に説明すると
この論文は、Ant GroupのCodeFuseチームが開発したF2LLM（Foundation to Feature Large Language Models）という最先端のテキスト埋め込みモデルファミリーを紹介している。
従来のSOTA埋め込みモデルが数億件の大規模データや高コストな合成データを必要とするのに対し、F2LLMはわずか600万件のオープンソース非合成データのみで同等の性能を達成している点が革新的である。
0.6B、1.7B、4Bの3つのサイズが提供されており、MTEBリーダーボードでF2LLM-4Bが4Bクラスで2位、全体で7位、F2LLM-1.7Bが1B-2Bクラスで1位を獲得している。
モデル、訓練データ、コードがすべて公開されており、GitHub（https://github.com/codefuse-ai/CodeFuse-Embeddings）とHugging Face（https://huggingface.co/collections/codefuse-ai/codefuse-embeddings）で利用可能。

## 1. 研究概要
### 1.1 背景と動機
近年、LLMベースの埋め込みモデルは情報検索、クラスタリング、分類などの埋め込みベースアプリケーションにおいて急速な進歩を遂げており、MTEBリーダーボードでの性能向上がそれを物語っている。
これらのモデルは、MistralやQwen3などの基盤LLMをクエリ-文書ペアでの対照学習により適応させ、大規模事前学習で獲得した言語理解能力を効率的に活用して高品質なテキスト埋め込みを生成する。

しかし、現在のSOTA埋め込みモデルの多くは複雑で再現困難な問題を抱えている。
第一に、大規模弱教師付き事前学習を含む洗練された多段階訓練パイプラインを採用しているものが多く、第二に、LLMによって生成された高コストな合成データに依存している場合が多い。
さらに深刻なことに、これらのモデルの大部分はモデルチェックポイントのみを公開し、訓練スクリプトやデータは非公開としているため、文献における一貫性の欠如と再現性の問題を引き起こしている。

この状況は、埋め込みモデル研究の進歩を阻害する重大な障壁となっている。
研究者が新しい手法を開発しようとしても、既存手法の正確な再現ができないため、公正な比較や改善効果の検証が困難になっている。
また、訓練データの詳細が不明であることから、どのようなデータが効果的なのかという根本的な理解も深まらない状況にある。

### 1.2 主要な貢献
本研究は、埋め込みモデル研究における再現性と効率性の課題を解決する包括的なソリューションを提供している。
F2LLMは、CodeFuseのD2LLM以降の新たな最先端埋め込みモデルファミリーとして、従来の複雑なアプローチとは根本的に異なる簡潔で効果的な手法を採用している。

最も重要な技術的貢献は、基盤モデルから直接ファインチューニングするシンプルなアプローチにより、わずか600万件の高品質クエリ-文書-困難な負例の三重組でSOTA性能を達成したことである。
これらの訓練データは、すべてオープンソースの非合成データセットから収集されており、多様なタスクタイプをカバーしている。

- **データ効率性の実証**: 他のSOTAモデルが数億件のデータを必要とする中、600万件という大幅に少ないデータで同等性能を実現
- **完全オープンソース化**: モデルチェックポイント、訓練データ、訓練コードをすべて公開し、再現可能な研究基盤を提供
- **統一データフォーマット**: 検索、分類、クラスタリングの異なるタスクを(クエリ、正例文書、困難な負例×n)の統一形式で処理
- **段階的モデルサイズ展開**: 0.6B、1.7B、4Bの3サイズで、計算リソースに応じた選択肢を提供
- **新しいベンチマーク記録**: クラスタリングタスクで68.54という全モデル中最高スコアを達成

## 2. 提案手法
### 2.1 手法の概要
F2LLMの訓練手法は、複雑さを極力排除した直接的なアプローチを特徴とする。
従来のSOTAモデルが多段階の複雑な訓練パイプラインを採用するのに対し、F2LLMは基盤モデルから対照学習による単一段階のファインチューニングのみで最終モデルを構築する。

データ収集では、490万件の検索サンプル、20万件の分類サンプル、80万件のクラスタリングサンプルを含む大規模複合データセットを構築している。
すべてのデータはMTEBで提供される訓練セットを使用し、該当しない場合はMTEBテストセットとの重複排除を実行している。

統一データフォーマットの採用により、異なるタスクタイプを一貫して処理できる。
各データサンプルは(クエリ、正例パッセージ、困難な負例×n)の三重組で構成され、nは検索・クラスタリングタスクで24、分類タスクで1に設定されている。
すべてのクエリには、タスク固有の指示がプリフィックスとして付加される。

### 2.2 技術的詳細
困難な負例マイニングには、マージンベース適応手法を採用している。
Qwen3-Embedding-0.6Bを用いて各クエリに対して上位100件の関連パッセージを検索し、上位5件を偽負例回避のため除外、スコア0.8未満かつ正例スコアの95%未満のパッセージからトップ24件を困難な負例として選択する。

訓練では、困難な負例損失と batch内損失の2つの対照学習損失を組み合わせている。
困難な負例損失は以下の式で計算される：

$$\ell_{\text{hard}} = -\log\frac{e^{s(q_i,d_i^+)/\tau}}{e^{s(q_i,d_i^+)/\tau}+\sum_{j=1}^{n}e^{s(q_i,d_{i,j}^-)/\tau}}$$

ここで、τ=0.05は温度パラメータ、s()はクエリと文書の埋め込み間のコサイン類似度である。

さらに、検索タスクのみに対してbatch内の全文書を負例として使用するbatch内損失も計算し、より豊富な負例からの学習を促進している。
多タスクデータローダーにより、各GPUが独立してタスクプールからバッチをサンプリングし、効率的な並列訓練を実現している。

### 2.3 新規性
F2LLMの最大の新規性は、「少ないデータで最大の効果」を実現するシンプルで効率的なアプローチにある。
従来のSOTAモデルが採用する複雑な多段階訓練パイプライン、大規模弱教師付き事前学習、高コストな合成データ生成といった重厚なアプローチを完全に排除し、基盤モデルからの直接ファインチューニングという軽量手法で同等以上の性能を達成している。

データ効率性の観点では、他のSOTAモデルが数億件のデータを必要とする中、F2LLMは600万件という10分の1以下のデータで競合性能を実現している。
これは、データの量よりも質と多様性を重視し、困難な負例マイニングによる効果的な学習信号の構築に成功したことを示している。

完全オープンソース化という点でも、従来のモデルがチェックポイントのみの公開に留まる中、訓練データ、コード、モデルのすべてを公開することで、研究コミュニティに真の再現可能性を提供している。
これにより、研究者は正確な比較実験と公正な改善評価が可能になり、埋め込みモデル研究の加速が期待される。

統一データフォーマットの採用により、検索、分類、クラスタリングという異なるタスクを一つの訓練パイプラインで効率的に処理できる点も革新的である。
従来は各タスクごとに異なる処理が必要だったが、F2LLMの統一アプローチにより実装の簡素化と保守性の向上を実現している。

## 3. 実験結果
### 3.1 実験設定
評価は、MTEBの41個の英語タスクで実施されている。
MTEBは、検索、分類、クラスタリング、ペア分類、再ランキング、STS（意味的テキスト類似度）、要約の7つのタスクカテゴリを含む包括的なベンチマークである。

比較対象として、現在のMTEBリーダーボード上位のSOTAモデルが選択されており、これには数億件のデータで訓練されたモデルや、高コストな合成データを使用したモデルが含まれている。
評価指標は各タスクの標準的な指標を使用し、全体的な性能は各カテゴリの平均スコアで評価されている。

特に重要なのは、モデルサイズごとの比較が行われていることである。
4B前後のモデル、1B-2Bモデル、1B未満のモデルという3つのカテゴリで分類し、各カテゴリ内での性能ランキングと全体での位置づけが明確に示されている。

### 3.2 主要な結果
F2LLMは、はるかに少ない訓練データにもかかわらず、SOTAレベルの性能を達成している。
F2LLM-4Bは、約4Bパラメータのモデル中で2位、全体で7位という優秀な成績を収めており、上位モデルはすべてクローズドソースまたは数億件のデータで訓練されたものである。

特に印象的なのは、F2LLM-1.7Bが1B-2Bサイズ範囲で1位を獲得していることである。
これは、計算リソースが限られたアプリケーションにとって理想的な選択肢となることを示している。
また、F2LLM-0.6Bも1B未満のモデル中で2位という強い性能を示している。

タスク別の分析では、クラスタリングタスクでの圧倒的な優位性が目立っている。
F2LLM-4Bはクラスタリングで68.54というスコアを達成し、これは全モデル中の最高記録となっている。
この結果は、F2LLMの統一データフォーマットと効果的な負例マイニング戦略がクラスタリングタスクに特に適していることを示唆している。

検索タスクでも安定した性能を示しており、多様な検索データセットでの訓練効果が確認されている。
分類タスクでは中程度の性能となっているが、これは訓練データの大部分が検索データで構成されていることを考慮すると妥当な結果である。

### 3.3 既存手法との比較
データ効率性の観点から、F2LLMの優位性は明確である。
例えば、数億件のデータで訓練されたモデルと比較して、10分の1以下のデータで同等の性能を達成している。
これは、単純なデータ量の増加よりも、質の高いデータと効果的な学習戦略の重要性を示している。

訓練の複雑さの比較では、多段階訓練パイプラインを採用する他のモデルに対し、F2LLMは単一段階の直接ファインチューニングで同様の結果を得ている。
これにより、訓練時間の短縮、実装の簡素化、デバッグの容易性といった実用的な利点が得られている。

再現性の観点では、F2LLMが完全オープンソースであることの価値は計り知れない。
従来のSOTAモデルがチェックポイントのみを公開し、訓練の詳細を秘匿している中、F2LLMは研究コミュニティに対して透明性と再現可能性を提供している。

コスト効率性では、合成データ生成に大きなコストをかける他のモデルと対照的に、F2LLMはオープンソースの既存データのみを使用することで、大幅なコスト削減を実現している。
これにより、リソースの限られた研究機関や企業でも高品質な埋め込みモデルの開発が可能になっている。

## 4. 実用性評価
### 4.1 実装の容易性
F2LLMの実装は、極めて簡潔で理解しやすい設計となっている。
統一データフォーマットの採用により、異なるタスクタイプを一つのパイプラインで処理でき、実装の複雑さが大幅に軽減されている。
また、完全なコードとデータが公開されているため、研究者や開発者は容易に実験を再現し、自身のデータセットに適用することができる。

多段階訓練パイプラインを排除した単一段階アプローチにより、訓練プロセスの理解と管理が容易になっている。
複雑な調整が必要なハイパーパラメータも少なく、新しいデータセットへの適用やドメイン固有の調整が比較的簡単に行える。

Hugging Faceプラットフォームでの公開により、モデルの利用開始までの時間も大幅に短縮されている。
標準的なTransformersライブラリを通じて、数行のコードでモデルをロードし、実際のアプリケーションで使用することができる。

### 4.2 計算効率
F2LLMは、データ効率性だけでなく計算効率性においても優れた特性を示している。
600万件という比較的少ないデータセットでの訓練により、必要な計算時間と電力消費を大幅に削減している。
これは、特に環境への配慮や計算資源の制約がある状況において重要な利点となる。

3つの異なるモデルサイズ（0.6B、1.7B、4B）の提供により、アプリケーションの要求と利用可能な計算リソースに応じた最適な選択が可能である。
小規模なモデルでも競合性能を維持しているため、リアルタイム処理や大規模デプロイメントにおいても実用的である。

推論時の計算効率も良好で、標準的なLLMベースの埋め込みモデルと同様の処理速度を維持しながら、高品質な埋め込みを生成できる。
バッチ処理や並列化にも対応しており、大量のテキストを効率的に処理することができる。

### 4.3 応用可能性
F2LLMの応用範囲は非常に広く、情報検索システム、推薦エンジン、文書分類、コンテンツクラスタリングなど、多様な自然言語処理アプリケーションに適用可能である。
特にクラスタリングタスクでの優秀な性能は、大規模文書の自動分類や知識ベースの構築において高い価値を持つ。

完全オープンソースという特性により、商用利用においても法的制約が少なく、企業のプロダクト開発に組み込みやすい。
また、訓練データとコードが公開されているため、特定のドメインやタスクに特化したカスタマイズも容易である。

研究分野では、他の研究者の手法と公正に比較できる強いベースラインとして機能することが期待される。
再現可能な実験環境の提供により、埋め込みモデル研究の加速と品質向上に寄与する可能性が高い。

多言語展開の基盤としても有望で、同様のアプローチを他の言語のデータに適用することで、多言語埋め込みモデルの開発も可能である。
Ant Groupの実績を考慮すると、中国語をはじめとするアジア言語への展開も期待される。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、埋め込みモデル分野において「効率性」と「再現性」という2つの重要な課題を同時に解決した画期的な貢献である。
従来のSOTAモデルが「より多くのデータ、より複雑な手法」という方向性を追求する中、F2LLMは「シンプルで効果的」なアプローチでSOTA性能を達成し、研究コミュニティに新たなパラダイムを提示している。

特に重要なのは、完全オープンソース化による透明性の確保である。
モデル、データ、コードのすべてを公開することで、埋め込みモデル研究における再現性の危機に対する解決策を提供している。
これにより、研究者は公正な比較実験が可能になり、真の技術進歩を評価できるようになる。

技術的な観点では、データ効率性の実証が特に価値が高い。
600万件という比較的少ないデータで数億件のデータと競合する性能を達成したことは、データの質と処理手法の重要性を示している。
これは、データ収集コストや環境負荷の観点からも重要な示唆を与える。

産業的インパクトも大きく、Ant Groupという大手IT企業による実用レベルのモデル公開は、商用アプリケーションでの採用を加速させる可能性がある。
特に、計算リソースが限られた中小企業や研究機関にとって、高品質な埋め込みモデルへのアクセスが容易になることの意義は大きい。

### 5.2 今後の展望
F2LLMが示したシンプルで効果的なアプローチは、今後の埋め込みモデル研究に大きな影響を与えると予想される。
特に、「データ効率性」への注目が高まり、質の高いデータセットの構築と効果的な学習戦略の研究が活発化するだろう。

多言語展開は最も期待される発展方向の一つである。
F2LLMのアプローチを中国語、日本語、その他のアジア言語に適用することで、多言語埋め込みモデルの新たなベンチマークが確立される可能性がある。
Ant Groupの多言語データアクセスを考慮すると、この方向での展開は十分に現実的である。

ドメイン特化型モデルの開発も有望な方向性である。
金融、医療、法律など特定分野に特化したデータセットでのファインチューニングにより、より高精度なドメイン固有埋め込みモデルの構築が期待される。
オープンソースという特性により、各分野の専門家が独自の拡張を加えることも容易である。

長期的には、F2LLMのアプローチを基盤とした自動化されたデータキュレーションシステムや、動的な負例マイニング手法の発展が期待される。
また、他のモダリティ（画像、音声）への拡張や、マルチモーダル埋め込みモデルへの発展も興味深い研究方向となるだろう。

ただし、現在の限界として、訓練データの大部分が検索タスクに偏っていることによる分類性能の相対的な低さがある。
今後は、タスク間のバランスを改善したデータ構成や、タスク適応的な学習手法の開発が重要な課題となる。

総合的に見て、F2LLMは埋め込みモデル研究における重要なマイルストーンであり、実用性と研究価値を兼ね備えた優れた貢献として、今後の発展が大いに期待される。