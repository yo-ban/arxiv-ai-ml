# Kolmogorov Arnold Networks (KANs) for Imbalanced Data - An Empirical Perspective

## 基本情報
- arXiv ID: 2507.14121v1 (https://arxiv.org/abs/2507.14121)
- 著者: Pankaj Yadav・Vivek Vijay
- 所属: 記載なし
- 投稿日: 2025年07月19日
- カテゴリ: cs.LG

## 簡単に説明すると
Kolmogorov Arnold Networks（KANs）は、多層パーセプトロン（MLP）とは異なるアプローチで神経ネットワークを構築する新しい技術です。通常のニューラルネットワークが線形の重みと固定の活性化関数を使うのに対し、KANは、コルモゴロフ-アーノルド表現定理に基づいて、学習可能な一変数関数（スプライン関数）を使用します。この研究では、クラス不均衡データ（一部のクラスのデータが極端に少ない問題）に対してKANがどのように機能するかを徹底的に検証しました。結果、KANは生データのままでMLPより優れた性能を示すものの、リサンプリングなどの一般的な不均衡対策を適用すると性能が30％から39％劣化しました。計算コストはMLPの1000倍以上になることが判明しました。

## 1. 研究概要
### 1.1 背景と動機
機械学習において、クラス不均衡データは教師あり学習の重要な課題であり、単純なMLPから高度なTransformerアーキテクチャまで、モデルの複雑さに関わらず常に問題となっています。多数クラスが少数クラスを圧倒することで、モデルが多数クラスに偏った予測をし、少数クラスの正確な識別が困難になります。この問題は二値分類を超えた多クラスシナリオで特に深刻になります。

最近登場したKANは、解釈可能なニューラルネットワークとして注目されています。コルモゴロフ-アーノルド表現定理に基づき、任意の多変数の連続関数が一変数の連続関数の有限合成として表現できることを示しています。KANは従来の線形重みをスプラインとしてパラメータ化された学習可能な一変数関数に置き換えることで、本質的に解釈可能性を優先しています。

しかし、KANの解釈可能性とパラメータ数が少ない関数近似の利点にもかかわらず、実世界のテーブルデータでの動作や有効性はほとんど未探索のままです。金融、医療、サイバーセキュリティなどの重要な領域のテーブルデータは独特の構造的課題を提示します。重要なことに、KANの解釈可能なアーキテクチャと不均衡なテーブル分類における堅牢なモデルの必要性の交差点は、重要な研究の空白を構成しています。

### 1.2 主要な貢献
本研究では、クラス不均衡なテーブルデータ分類におけるKANの能力を徹底的に調査し、以下の貢献をしました。
- 10個のベンチマークデータセットを使用して、少数クラスに敏感なメトリクス（G-mean、F1スコア、バランス精度、AUC）でKANとMLPを比較
- KANアーキテクチャが不均衡効果を説明し、性能の相乗効果または劣化を明らかに
- 異なるリサンプリング技術を実装する際にKANがMLPに対して被る計算ペナルティの定量化

包括的なベンチマークの結果、KANは生の不均衡データをMLPよりも本質的によく扱うことを実証しました。具体的には、ベースライン設定でG-meanは54%向上し、F1スコアは55%向上しました。しかし、従来のリサンプリング技術を適用すると、性能が30％から39％劣化しました。重要なことに、KANは法外な計算コストを被り、中央値でMLPの1000倍の訓練時間と11倍のメモリを必要としました。

## 2. 提案手法
### 2.1 手法の概要
本研究では、KANのクラス不均衡データに対する性能を徹底的に評価するための包括的な実験フレームワークを設計しました。バイナリおよび多クラスの不均衡データセットを使用しました。多クラス問題ではone-vs-all（OvA）アプローチを採用して、最小クラスを少数クラスとして使用し、他のすべてのクラスを複合多数クラスにマージしました。

クラス不均衡に対処するため、以下の2つの異なる戦略を実装・比較しました。
1. データレベル：SMOTE-Tomekハイブリッドリサンプリング技術
2. アルゴリズムレベル：Focal Loss法

これらのバランシング戦略の有効性を、生の不均衡データで訓練されたベースラインモデルに対して厳密に評価しました。この比較分析はKANとMLPの両方のアーキテクチャで実施され、MLPをベースラインとしてアーキテクチャ間の直接比較を可能にしました。

### 2.2 技術的詳細
コルモゴロフ-アーノルド表現定理は、任意の連続な多変数関数$f(\mathbf{x}): [0,1]^n \rightarrow \mathbb{R}$が以下のように表現できることを示しています。

$$f(\mathbf{x}) = \sum_{i=1}^{2n+1} \psi_i \left(\sum_{j=1}^{n} \phi_{ij}(x_j) \right)$$

ここで$\phi_{ij}: [0,1] \rightarrow \mathbb{R}$と$\psi_i : \mathbb{R} \rightarrow \mathbb{R}$は連続な一変数関数。この分解は基本的な2層構造を示します。内部変換層が$n(2n+1)$個の一変数マッピングを計算します。外部変換層は各集約値に$\psi_i(\cdot)$を適用します。

KANアーキテクチャでは、これらの一変数関数をBスプラインで近似します。Bスプラインは、多項式近似の限界（ルンゲ現象）を克服し、局所的なサポートと連続性を提供します。具体的には、各一変数関数は次のようにパラメータ化されます。

$$\phi(x) = w_b \cdot b(x) + w_s \cdot \text{spline}(x)$$

ここで、$b(x) = \text{silu}(x) = x/(1+e^{-x})$は残差接続に対応する微分可能な基底関数です。$\text{spline}(x)$はBスプライン近似、$w_b, w_s \in \mathbb{R}$は相対的貢献を制御する訓練可能なスケーリング係数です。

Focal Lossの実装では、KANは確率分布ではなく生のロジット$\mathbf{z}$を出力するため、明示的な確率正規化を通じて適応させました。

$$p_t = \sigma(\mathbf{z})_t = \frac{e^{z_t}}{\sum_{j=0}^{1}e^{z_j}}$$

$$\mathcal{L}_{\text{focal}} = -\alpha_t(1-p_t)^\gamma \log(p_t)$$

### 2.3 新規性
本研究の新規性は、KANをクラス不均衡データ分類に適用した初めての包括的な実証研究である点にあります。既存のKANの文献は主に合成データセット、標準ベンチマーク、または物理情報タスクに焦点を当てており、実用的な偏ったテーブル分類におけるバイアスを緩和する可能性は未検証のままでした。

さらに、KANと従来の不均衡・処理技術（リサンプリング、Focal Loss）との相互作用を分析し、KANの数学的構造が標準的な不均衡対策と根本的に衝突することを明らかにした点も新しい発見です。この発見は、リサンプリングがより単純なアーキテクチャにMLPのように有益であることを示します。一方で、KANの一変数関数の学習を積極的に妨げることも示しています。

## 3. 実験結果
### 3.1 実験設定
本研究では、KEELリポジトリから10個のクラス不均衡データセットを選択しました。データセットは、不均衡比が1:5（軽度）から1:50（重度）の範囲に意図的に選択され、KANのロバスト性を不均衡の重篤度が増加するにつれて体系的に評価することを可能にしました。

評価に使用したデータセットは以下の通りです。
- yeast4 (8特徴、1484インスタンス、不均衡比 28.1)
- yeast5 (8特徴、1484インスタンス、不均衡比 32.73)
- yeast6 (8特徴、1484インスタンス、不均衡比 41.4)
- glass2 (9特徴、214インスタンス、不均衡比 11.59)
- ecoli3 (7特徴、336インスタンス、不均衡比 8.6)
- winequality-red-8_vs_6-7 (11特徴、855インスタンス、不均衡比 46.5)
- new-thyroid1 (5特徴、215インスタンス、不均衡比 5.14)
- glass4 (9特徴、214インスタンス、不均衡比 15.47)
- glass6 (9特徴、214インスタンス、不均衡比 6.02)
- winequality-red-8_vs_6 (11特徴、656インスタンス、不均衡比 35.44)

実験はCPUベースの環境で実施され、すべてのモデルは同一条件下で訓練・評価されました。

### 3.2 主要な結果
KANとMLPの包括的な評価は、ベースライン、リサンプル、Focal Lossの方法論にわたって興味深い性能パターンを明らかにしました。

ベースライン設定において、KANはすべての評価メトリクスで一貫して性能優位性を維持しました。
- バランス精度：KAN 0.6335 vs MLP 0.5800
- G-mean：KAN 0.4393 vs MLP 0.2848（+54%の改善）
- F1スコア：KANが少数クラス認識で優位性を示す

リサンプルアプローチを調査した際、アーキテクチャ間の性能差が著しく縮まりました。
- バランス精度：KAN 0.5904 vs MLP 0.5943
- G-mean値：両モデルが0.30近くに収束

このパターンは、Focal Lossの実装でも継続し、両アーキテクチャが約0.580のバランス精度と0.191の近いF1スコアを達成しました。

重要なことに、この性能同等性は計算プロファイルの決定的な違いにもかかわらず発生しました。KANはすべての構成でMLPよりも数桁多い訓練時間とメモリリソースを一貫して必要としました。

### 3.3 既存手法との比較
リサンプリング技術がKANの性能を根本的に変えることが明らかになりました。リサンプリングとFocal Lossの実装はどちらもクリティカルなメトリクスでKANの性能を劣化させました。
- G-meanは32%（リサンプル）から39%（Focal）低下
- F1スコアは22-26%低下

レーダーチャートの分析では、ベースラインKANが最大の性能領域を占めることを示しました。この発見は、KANの本質的なアーキテクチャがすでに、従来の不均衡技術がMLPに人工的に誘導しようとする能力を組み込んでいることを示しています。

計算効率の分析では、KANの精度優位性に対して急峻な計算ペナルティが明らかになりました。
- ベースラインKAN：訓練時間505秒（MLPの1000倍）、メモリ1.48MB（MLPの11倍）
- リサンプルKAN：訓練時間885秒、メモリ6.51MB（MLPの900倍遅く、1.4倍多いメモリ）

## 4. 実用性評価
### 4.1 実装の容易性
KANの実装は、コルモゴロフ-アーノルド表現定理に基づく数学的基盤は明確であるものの、Bスプラインパラメータ化と残差を含む活性化関数の実装が複雑です。

ハイパーパラメータの最適化がデータセット特性に応じたクリティカルなアーキテクチャパターンを明らかにしました。
- 単層アーキテクチャが主流で、幅は4（yeast6）から8（glass6）まで変動
- 2層設計（yeast5、ecoli3、new_thyroid1）は漸進的な幅削減を特徴とする
- グリッドサイズは中程度の基底関数粒度を反映して5にクラスター化（80%のケース）

パラメータの選択は計算負荷に直接影響しました。
- 2層は単層アーキテクチャよりも2.1倍長い中央値の訓練時間を必要とする（589秒 vs 281秒）
- 幅の増加4→ 8はメモリ使用量を3.2倍増幅させる

### 4.2 計算効率
計算コストの分析は、KANの最も深刻な制約を明らかにしました。対数スケールの視覚化は、アーキテクチャの効率フロンティアを鮮明に対比します。

MLPはすべての戦略にわたって原点近くのタイトなクラスターを維持し、最小のリソース分散を反映しました（0.06MBメモリ、0.5-0.97秒時間）。一方、KANは極端な分散を示し、ランタイムでほぼ3桁、メモリで2桁にまたがりました。

重要なことに、このインフレーションは減少する結果をもたらします。Focal Loss KANは535秒を消費し（MLPの0.89秒に対して）、ほぼ同一の精度でした。KANの計算と精度向上の逆関係は、一変数関数を直接学習するにはリソースが必要であることを意味しますが、高次元データセットでの根本的なスケーラビリティの課題を強調しています。

### 4.3 応用可能性
統計的検証により、研究の中核的な発見が確認されました。
- KANはベースライン不均衡設定で非有意だが一貫した優位性を示した（G-mean: d=0.73, p=0.06、F1スコア: d=0.61, p=0.10）
- Focal Loss実装はKANのG-meanを有意に劣化させた（p=0.042、大効果サイズd=0.79）
- リサンプリングアプローチも有意水準に近づいた（p=0.057、d=0.73）

Wilcoxon検定はKANの訓練時間の不利（p=0.002、大量効果サイズd=2.94）とメモリ比較で同様の極端を示しました（p=0.002、d=2.17）。

重要なことに、リサンプリング戦略はリソースペナルティを削減することなくKANの性能優位性を消去しました。リサンプルKAN対MLPは、すべてのメトリクスで無視できる効果サイズを示しました（|d|<0.08）。この統計的収束は、MLPと不均衡技術が最小の計算コストでKANと同等を達成することを確認します。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、クラス不均衡分類に対するKANの包括的な実証分析を提示し、理論的理解と実用的展開の両方に重要な含意を持つ基本的な洞察を明らかにしました。

KANは、生の不均衡データに対して独特のアーキテクチャ上の利点を示しました。ベースライン構成において、重要な少数クラスメトリクスにMLPを一貫して上回りました。この能力は、複雑な決定境界を本質的に捕捉する適応関数と一変数の応答特性を持つ数学的定式化に由来します。

しかし、より重要な発見は、従来の不均衡戦略がKANのアーキテクチャ原理と根本的に衝突することです。リサンプリングとFocal Lossの両方が、MLPにはわずかに有益である一方で、KANの性能を少数クラスメトリクスで30％から39％劣化させました。この逆説的な発見は、KANが本質的に、MLPが制供する表現上の利益を組み込んでいることを示唆しています。

計算効率の観点から、KANの実用的有用性を制約する深刻な性能リソーストレードオフを定量化しました。ベースラインの利点にもかかわらず、KANはMLPよりも中央値で桁違いに長い訓練時間とメモリを要求しました。この計算ペナルティは、比例的な性能向上なしにリサンプリング技術の利用下でエスカレートしました。

### 5.2 今後の展望
本研究の発見は、以下の明確な研究優先順位を示しています。

1. KAN固有の不均衡技術の開発：計算インフレーションを避けながらKANの本質的な不均衡処理を保持するアーキテクチャ修正を開発する。有望な方向には、スパーシティ制約学習や、データ拡張なしで少数クラスの特徴を増幅するアテンションメカニズムが含まれる。

2. リソース最適化：極端な計算オーバーヘッドは、基底関数の量子化やハードウェア対応実装など、専用のKAN圧縮研究を必要とする。単層設計が2層よりも2.1倍訓練時間を削減したことは初期的な経路を示唆しているが、アルゴリズム的なブレークスルーが不可欠である。

3. 理論的協調：リサンプリング技術がKANを劣化させるがMLPに有益である理由を解明する。データ拡張がコルモゴロフ-アーノルド表現定理の仮定を破壊すると仮説を立てる。この相互作用の正式な分析が優先されるべきである。

MLPとリサンプリング戦略により、現在ほとんどのアプリケーションに対して優れた効率性能均衡を提供しています。一方、KANは生データの不均衡が極端で、前処理が禁止され、計算リソースが制約されない重要なシステムに対して独特の価値を保持しています。リサンプリングなしのベースライン性能により、展開パイプラインが簡素化されます。この利点を見過ごすべきではありません。しかし、リソースギャップを埋めることは、より幅広い採用の前に重要です。

この研究は、KANがMLPの総括的な置換ではなく、実用的な不均衡学習シナリオでその理論的可能性を実現するために継続的なアーキテクチャ革新を必要とする専門ツールであることを確立しています。