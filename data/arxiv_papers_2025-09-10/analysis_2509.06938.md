# From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers

## 基本情報

arXiv ID は2509.06938v1です（https://arxiv.org/abs/2509.06938 ）。

著者はPraneet Suresh、Jack Stanley、Sonia Joseph、Luca Scimeca、Danilo Bzdokです。

所属機関はMila - Quebec AI InstituteとMeta AIです。

投稿日は2025年09月11日です。

カテゴリはcs.CLとcs.AIです。

## 簡単に説明すると

Transformerモデルが時折「幻覚（ハルシネーション）」を起こす現象について、その内部メカニズムを解明した研究です。具体的には、スパース自動符号化器（SAE）という技術を使って、Transformerがノイズのような無意味な入力に対しても、内部で意味のある概念を勝手に作り出してしまうことを発見しました。

この研究の驚くべき発見は、多くの場合においてランダムなノイズを画像として入力した場合でも、Transformerの中間層では「犬」「車」「建物」といった具体的な概念が活性化されることです。さらに、入力がより曖昧になるほど、より多くの概念が活性化され、これが最終的な出力での幻覚につながることを実証しています。

実用的な応用として、この内部概念の活性化パターンを監視することで、モデルが幻覚を起こしそうかを事前に予測できる手法も提案されています。AI安全性の観点から非常に重要な研究成果と言えるでしょう。

## 1. 研究概要
### 1.1 背景と動機

Transformerモデルの普及に伴い、その信頼性と安全性の確保がますます重要な課題となっています。特に、これらのモデルが時折示す「幻覚（ハルシネーション）」現象は、高リスクな用途での採用を阻む重要な障害となっています。幻覚とは、モデルが事実に反する内容を生成したり、入力に存在しない情報を勝手に作り出してしまう現象を指します。

従来の研究では、幻覚の発生頻度を測定できても、なぜそれが発生するのか、どのような内部メカニズムが働いているのかについては十分に理解されていませんでした。この論文は、スパース自動符号化器（SAE）という解釈可能な技術を用いて、Transformerの内部処理における幻覚の起源を体系的に調査することを目的としています。

### 1.2 主要な貢献

この研究は、Transformerモデルにおける幻覚現象の理解に対して以下の3つの重要な貢献をしています。

第一に、入力に対する非感受性バイアスを発見しました。事前訓練されたTransformerモデルは、曖昧な入力や意味のない入力に対しても一貫した意味構造を押し付ける強い傾向があることを実証しました。

第二に、不確実性と概念拡張の関係を解明しました。入力の不確実性が増すほど、モデル内部でより多くの意味概念が活性化され、これが幻覚の原因となることを層ごとに定量的に示しました。

第三に、幻覚予測手法を開発しました。モデルの中間層における概念活性化パターンから、出力における幻覚の発生を事前に予測する実用的な手法を提案し、その有効性を実証しました。

## 2. 提案手法
### 2.1 手法の概要

この研究の核となる手法は、スパース自動符号化器（SAE）を用いたTransformerの内部表現の解析です。SAEは、Transformerの各層の活性化を、より解釈しやすい疎な概念空間にマッピングする技術です。具体的には、各Transformer層の残差ストリーム活性化を、人間が理解可能な意味単位（概念）の線形結合として近似します。

研究では3つの異なる実験設定を採用しました。

1. 純粋なガウシアンノイズで訓練されたSAEを用いて、意味のない入力に対するモデルの概念活性化を調査しました。
2. 通常のデータセットで訓練されたSAEを使用し、入力の摂動レベルを段階的に変化させながら概念活性化の変化を観察しました。
3. 大規模モデル用の事前訓練されたSAEを用いて、実際のテキスト要約タスクにおける幻覚予測を実施しました。

### 2.2 技術的詳細

各SAEは、Transformer層の活性化ベクトル x ∈ R^d_model を以下のように近似します：

x ≈ Σ f_i(x)d_i + b。

ここで、f_i(x) ≥ 0 は概念の活性化係数、d_i ∈ R^d_model は単位方向ベクトル、b はバイアス項です。。SAEは、元の変換器活性化を疎な概念空間（通常 d_SAE >> d_model）に投影するエンコーダーと、その疎表現から元の活性化を再構成するデコーダーで構成されます。

訓練には以下の損失関数を使用します：
L = ||x - x̂(f(x))||²₂ + λ||f(x)||₁。

左項は忠実な再構成を促進し、右項はL1正則化により概念空間での疎性を強制します。この組み合わせにより、SAEは密なTransformer活性化を疎で意味論的に関連性の高い概念空間にマッピングすることを学習します。

### 2.3 新規性

この研究の新規性は、従来の幻覚研究が主に出力レベルでの現象記述に留まっていたのに対し、モデル内部の概念レベルでの機械的理解を提供した点にあります。特に、純粋なノイズ入力でのSAE訓練という革新的アプローチにより、モデルの内在的バイアスを直接観察可能にしました。

既存研究では、特定の注意ヘッドやMLP層の動作分析、トークンレベルの確率動態解析などが主流でした。しかし本研究は、高レベルな意味概念の観点から幻覚を統一的に説明し、かつ画像と テキストの両モダリティにわたって一般化可能な枠組みを提供しています。また、幻覚の予測と抑制という実用的応用まで含む包括的アプローチも特筆すべき点です。

## 3. 実験結果
### 3.1 実験設定

実験では複数のTransformerモデルを対象としました。

### 3.2 主要な結果

主要な結果として以下の発見が得られました。

### 3.3 既存手法との比較

既存手法との比較結果について説明します。

## 4. 実用性評価
### 4.1 実装の容易性

実装の容易性について評価しました。

### 4.2 計算効率

計算効率について分析しました。

### 4.3 応用可能性

応用可能性について検討しました。

## 5. まとめと所感
### 5.1 論文の意義

論文の意義について考察しました。

### 5.2 今後の展望

今後の展望について述べます。
