# Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation

## 基本情報
- arXiv ID: 2509.15224v1 (https://arxiv.org/abs/2509.15224)
- 著者: Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia
- 所属: Advanced Research Center on Electronic System (ARCES), Department of Computer Science and Engineering (DISI), University of Bologna, Italy
- 投稿日: 2025年09月23日
- カテゴリ: cs.CV

## 簡単に説明すると
この論文は、イベントカメラによる単眼深度推定において、Vision Foundation Model（VFM）を活用したクロスモーダル蒸留手法を提案しています。
イベントカメラは高速な動きや激しい照明変化に対して従来のカメラよりも堅牢です。
一方で、大規模なデータセットの不足により深度推定が困難でした。
本研究では、RGBフレームで事前訓練されたDepth Anything v2などのVFMから知識を蒸留し、イベントデータ用の深度推定モデルを効率的に訓練する手法を開発しました。
プロジェクトページは https://bartn8.github.io/depthanyevent で公開されています。

## 1. 研究概要
### 1.1 背景と動機
深度知覚は自律ナビゲーションやロボティクスにおいて極めて重要な技術です。
従来カメラを用いた学習ベースの手法は過去10年間で目覚ましい発展を遂げてきました。
しかし、従来のカメラシステムは高速に動く物体や急激な照明変化を伴う環境において信頼性のある知覚を提供することが困難です。
これらの制限は、離散的な周期間隔での撮影と限られたダイナミックレンジという従来カメラの技術的特性に起因します。
その結果、モーションブラーや露出不足・過度な露出、フレーム間での重要な情報の欠失を引き起こします。

これに対して、イベントカメラは輝度変化を検出した瞬間にマイクロ秒の時間分解能で応答します。
非常に高いダイナミックレンジを持つため、これらの課題に対処するのに理想的です。
イベントカメラは各ピクセルで独立して輝度変化のみを記録し、優れた時間分解能と照明変動に対する堅牢性を提供します。
しかし、これらの特徴は従来のカメラと比較して情報量が乏しいという代償を伴います。
イベントカメラは十分なテクスチャがありイベントを引き起こす画像の小さな部分についてのみ意味のある手がかりを提供します。
そのため、これらのデバイスからの深度知覚は極めて困難です。

### 1.2 主要な貢献
本研究では、イベントベースの単眼深度推定における以下の4つの主要な貢献をしています。

第一に、画像ベースのVFMから得られる堅牢な擬似ラベルを活用した新しいクロスモーダル蒸留パラダイムを提案しました。
この手法により、密な教師信号を用いてイベントベースの深度推定ネットワークを効率的に訓練できます。

第二に、既存の画像ベースVFMをイベントドメインに容易に適応させる戦略を開発しました。
この戦略により、大規模な事前訓練で得られた知識をイベントカメラデータに転移できます。

第三に、適応された画像ベースVFMに基づく新しいリカレントアーキテクチャ「DepthAnyEvent-R」を提案しました。
このアーキテクチャは時系列情報を効果的に活用し、イベントストリームの特性により適合しています。

第四に、VFMをイベントドメインに適応させることで従来手法を上回る性能を達成し、提案する蒸留パラダイムが深度センサーによる監視と競合する性能を示すことを実証しました。

## 2. 提案手法
### 2.1 手法の概要
本研究の核心となるのは、フレームベースの単眼深度モデル（Depth Anything v2など）の知識を活用して擬似ラベルを抽出し、位置合わせされた強度フレームとイベントスタックを用いて任意のイベントベース学生深度モデルを訓練するクロスモーダル蒸留戦略である。
この手法は、データが豊富な画像ドメインからデータが稀少なイベントドメインへの効果的な知識転移を実現する。

具体的には、DAVISカメラのような市販のデバイスを使用して、同一ピクセルアレイに従来のグローバルシャッターカメラとイベントベースセンサーを組み込み、空間的に位置合わせされたイベントストリームとRGBフレームを収集する。
教師VFMはRGB入力フレームを処理して擬似深度ラベルを生成し、学生モデルは位置合わせされたイベントスタックを入力として最終的な深度マップを予測する。

さらに、本研究では既存の画像ベースVFMをイベントベース単眼深度推定に適応させる2つのアプローチを提案している：vanilla Depth Anything v2モデルの直接適応と、時間的手がかりを活用する新しいリカレントアーキテクチャの構築である。

### 2.2 技術的詳細
**クロスモーダル蒸留の損失関数**

学生モデルの訓練には、スケール不変損失とグラディエント正則化項を組み合わせた損失関数を使用する：
L = L_si + λL_reg

スケール不変損失L_siは以下のように定義される：
L_si(D̂,D̂*) = 1/(2|M|) Σ_(x,y)∈M (D̂ - D̂*)²

ここで、Mは有効ピクセルの集合、D̂=sD+tとD̂*=D*はそれぞれ学生予測DとプロキシラベルD*のスケーリングおよびシフト版である。
スケーリング因子(s,t)は最小二乗法により求められる。

**リカレントアーキテクチャ（DepthAnyEvent-R）**

DepthAnyEvent-Rモデルは、位置エンコーディングを持つ画像パッチを複数のTransformerステージで処理し、マルチスケール特徴マップを生成する。
これらの特徴はConvLSTMモジュールにおいて隠れ状態と結合され、前のイベントスタックからの時間情報を組み込む。
階層的融合プロセスが異なるスケールからの特徴を統合し、最終的な深度予測を生成する。

**イベント表現**

本研究では主にTencodeイベント表現を採用している。
これはRチャンネルとBチャンネルで正負の極性をエンコードし、Gチャンネルで全体の時間経過に対する相対的なタイムスタンプをエンコードするRGB画像表現である。

### 2.3 新規性
本手法の主要な新規性は、大規模な事前訓練を受けたVFMの知識をイベントドメインに転移する初のクロスモーダル蒸留フレームワークの提案にある。
従来のイベントベース深度推定手法は限られたイベントデータセットでの直接的な教師あり学習に依存していたが、本手法はデータが豊富な画像ドメインの知識を活用することで、高価な深度アノテーションを必要とせずに競合する性能を達成している。

さらに、既存の画像ベースVFMをイベントドメインに適応させる戦略と、時間的手がかりを効果的に活用する新しいリカレントアーキテクチャの提案により、イベントストリームの特性により適合したモデル設計を実現している。

## 3. 実験結果
### 3.1 実験設定
実験では合成データセットEventScapeと実世界データセットMVSEC、DSECを使用した。
評価指標として、Abs Rel、Sq Rel、RMSE、RMSE log、SI log（値が小さいほど良い）およびδ < 1.25、δ < 1.25²、δ < 1.25³（値が大きいほど良い）を採用した。

ハイパーパラメータとして、スライシングウィンドウΔT=50ms、Voxel Gridビン数B=5、損失因子λ=0.25を設定した。
学習率はE2Depthで10⁻⁴、EReFormerで3.2×10⁻⁵、DepthAnyEvent系列で5×10⁻⁶とし、訓練ステップ数を75k、バッチサイズを10（EReFormerは2）として設定した。

プロキシラベル生成には、EventScapeで10kステップファインチューニングを行ったDAv2 ViT-Largeを使用した。

### 3.2 主要な結果
**ゼロショット汎化性能**

EventScapeでのみ訓練したモデルのMVSECとDSECでのゼロショット評価において、DepthAnyEventとDepthAnyEvent-Rは既存手法を大幅に上回る性能を示した。
MVSECでは、DepthAnyEventがAbs Rel 0.466、DepthAnyEvent-RがAbs Rel 0.469を達成し、従来の最良手法E2Depth（0.527）を大きく改善した。
DSECでは更に顕著な改善を示し、DepthAnyEvent-RがAbs Rel 0.276を達成した。

**ドメイン内評価**

EventScapeで事前訓練後、各ターゲットデータセットでファインチューニングしたin-domain評価では、DepthAnyEvent-RがMVSECでAbs Rel 0.365、DSECでAbs Rel 0.191という最先端の性能を達成した。

**蒸留手法の有効性**

クロスモーダル蒸留を用いた訓練と従来の教師あり学習の比較実験において、蒸留手法は多くの場合で教師あり学習と競合する性能を示した。
特にE2DepthとEReFormerでは、蒸留手法が合成データのみでの訓練を大幅に上回る性能を示し、高価な深度アノテーションなしでも効果的な学習が可能であることを実証した。

### 3.3 既存手法との比較
提案手法は既存のイベントベース深度推定手法であるE2DepthとEReFormerと比較して、一貫して優れた性能を示した。
特に、DepthAnyEvent-Rは時間的情報を効果的に活用することで、静的なDepthAnyEventよりも更なる性能向上を実現している。

定性的評価では、提案手法がより詳細で一貫性のある深度マップを生成し、特に動的シーンや挑戦的な照明条件下での堅牢性を示している。

## 4. 実用性評価
### 4.1 実装の容易性
提案手法は既存のDepth Anything v2の実装をベースとしており、比較的容易に実装できる。
クロスモーダル蒸留フレームワークは汎用的であり、任意のイベントベース学生モデルに適用可能である。
DAVISカメラのような市販のハードウェアを使用することで、データ収集も比較的簡単に行える。

### 4.2 計算効率
本研究では具体的な計算時間の詳細は報告されていないが、既存のTransformerベースのVFMを活用しているため、推論時の計算コストは比較的高いと予想される。
ただし、訓練時には高価な深度アノテーションが不要であるため、データ収集コストの大幅な削減が期待できる。

### 4.3 応用可能性
イベントカメラの特性を活かした応用分野は多岐にわたる。
自律走行車やドローンの高速ナビゲーション、ロボティクスにおける動的環境での作業、極端な照明条件下での監視システムなど、従来のカメラが困難な環境での深度推定に大きな可能性を秘めている。
また、提案するクロスモーダル蒸留の概念は他のモダリティ間の知識転移にも応用可能である。

## 5. まとめと所感
### 5.1 論文の意義
本論文は、イベントベース深度推定における重要な課題である大規模データセットの不足を、VFMを活用したクロスモーダル蒸留という革新的なアプローチで解決した点で極めて意義深い。
従来のイベントカメラ研究が限られたデータセットでの直接的な学習に制約されていた中で、データが豊富な画像ドメインの知識を効果的に転移する手法を確立したことは、この分野の発展に大きく貢献している。

また、既存のVFMをイベントドメインに適応させる戦略と、時間的手がかりを活用するリカレントアーキテクチャの提案により、イベントカメラの特性により適合したモデル設計の新しい方向性を示している。

### 5.2 今後の展望
今後の発展として、より効率的なリカレントアーキテクチャの設計や、他のVFMの活用可能性の探索が期待される。
また、マルチモーダル融合（イベント+RGB+LiDARなど）への拡張や、リアルタイム処理に向けた計算効率の改善も重要な研究方向である。

さらに、提案するクロスモーダル蒸留の概念を他のコンピュータビジョンタスク（セマンティックセグメンテーション、物体検出など）に適用することで、イベントカメラの応用分野を更に拡大することができると考えられる。
特に、極端な環境条件での堅牢な視覚システムの構築において、本手法は重要な基盤技術となる可能性が高い。
