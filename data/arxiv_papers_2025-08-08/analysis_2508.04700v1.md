# SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience

## 基本情報
- **arXiv ID**: 2508.04700v1 ([https://arxiv.org/abs/2508.04700](https://arxiv.org/abs/2508.04700))
- **著者**: Zeyi Sun (Shanghai Jiao Tong University, Shanghai AI Lab), Ziyu Liu (Shanghai Jiao Tong University, Shanghai AI Lab), Yuhang Zang (Shanghai AI Lab), Yuhang Cao (Shanghai AI Lab), Xiaoyi Dong (Shanghai AI Lab, CUHK), Tong Wu (CUHK), Dahua Lin (Shanghai AI Lab, CUHK), Jiaqi Wang (Shanghai AI Lab)
- **所属**: Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, The Chinese University of Hong Kong
- **投稿日**: 2025年08月09日
- **カテゴリ**: cs.AI, cs.LG

## 簡単に説明すると

本論文は、人間の監督なしに未知のソフトウェア環境で自律的に学習・進化できるコンピュータ使用エージェント「SEAgent」を提案しています。
SEAgentは、新しいソフトウェアを探索し、試行錯誤を通じて学習し、段階的に複雑なタスクをこなせるようになる自己進化型フレームワークです。
プロジェクトのGitHubリポジトリは[https://github.com/SunzeY/SEAgent](https://github.com/SunzeY/SEAgent)で公開されています。

このシステムは、ステップレベルの軌跡を評価するWorld State Modelと、ますます多様で挑戦的なタスクを生成するCurriculum Generatorを組み合わせることで、
エージェントが自律的に学習できる環境を実現しています。失敗行動の敵対的な模倣学習と成功行動のGroup Relative Policy Optimization (GRPO)を
組み合わせた経験学習により、エージェントのポリシーを更新します。

## 1. 研究概要
### 1.1 背景と動機

大規模視覚言語モデル（LVLM）の発展により、コンピュータ使用エージェント（CUA）は実用的な段階に到達しつつあります。
これらのエージェントは、スクリーンショットを視覚入力として受け取り、キーボードとマウス操作でコンピュータを制御できます。

しかし、現在のCUAには重要な課題があります。主に人間がラベル付けしたデータセットに依存しており、新しいソフトウェアや
定期的に更新される既存ソフトウェアへの対応が困難です。人間のアノテーションなしに、エージェントが自律的に
未知のソフトウェア環境を探索し、専門家レベルまで進化できる能力が求められています。

本研究は、David SilverとRichard S. Suttonの「新世代のエージェントは主に経験から学習することで超人的な能力を獲得する」
という予言を実現することを目指しています。

### 1.2 主要な貢献

本研究の主要な貢献は以下の通りです：

- コンピュータ使用エージェントが自身の経験から学習し、以前に見たことのないソフトウェア環境で専門化できる新しいフレームワークの提案
- エージェントの相互作用からのスクリーンショットの完全なシーケンスを処理し、強化学習を導くための細粒度のステップレベルの報酬信号を提供する報酬モデル「World State Model」の開発
- OSWorldの5つの専門的ソフトウェア環境での実験により、既存の強化学習ベースのアプローチを大幅に上回る新しい最先端性能の達成
- 個々のソフトウェアで訓練されたスペシャリストを統合し、スペシャリストのアンサンブルを超える性能を持つジェネラリストモデルを開発する「スペシャリストからジェネラリストへ」戦略の導入

## 2. 提案手法
### 2.1 手法の概要

SEAgentは、コンピュータ使用エージェント（CUA）が自律的に環境を探索し、強化学習を通じて段階的に自己進化することを可能にする
トレーニングパイプラインを確立しています。このパイプラインは3つの主要コンポーネントから構成されます：

1. **Actor Model π**: タスクを実行するために探索的なアクションを実行するポリシーモデル
2. **World State Model M_state**: 現在の環境状態を記述し、実行されたアクションの成功または失敗を評価する微調整されたLVLM
3. **Curriculum Generator M_task**: より多様で挑戦的な探索タスクを継続的に提案する強力なLLM

これらのコンポーネントが協調して動作することで、エージェントは未知のソフトウェア環境で段階的に学習し、
簡単なタスクから複雑なタスクへと進化していきます。

### 2.2 技術的詳細

**自己進化カリキュラムパラダイム**：
システムはP個の連続したフェーズで構成されています。各フェーズでは、Curriculum Generatorが生成したタスクをActor Modelが実行し、
World State Modelがその軌跡を評価します。評価結果と状態変化の記述は、ソフトウェアガイドブックの更新と、
より挑戦的な次フェーズのタスク生成に使用されます。

**強化学習による経験学習**：
World State Modelは、軌跡全体を入力として受け取り、各アクションを正しいアクション（a_T）または失敗アクション（a_F）に分類します。
正しいアクションには検証可能な報酬関数を使用してGroup Relative Policy Optimization (GRPO)を適用し、
失敗アクションには敵対的模倣学習を使用してペナルティを与えます。

報酬設計について、アクションタイプごとに異なる距離ベースの報酬を定義しています。具体的には以下の通りです。
- clickアクション：クリック座標間の正規化L1距離
- dragとselectアクション：予測と正解のバウンディングボックス間のIoU
- typeアクション：予測と正解テキスト間の文字レベルBLEUスコア

### 2.3 新規性

本手法の新規性は以下の点にあります：

1. **完全自律的な学習**: 人間のアノテーションに依存せず、エージェントが自身の経験から学習
2. **ステップレベルの報酬信号**: 軌跡全体を考慮した高精度な評価により、従来の二値報酬よりも効果的な学習を実現
3. **カリキュラム学習**: エージェントの能力に応じて段階的に難易度を上げるタスク生成
4. **スペシャリストからジェネラリストへの戦略**: 個別ソフトウェアで訓練したスペシャリストの知識を統合し、より強力なジェネラリストを構築

## 3. 実験結果
### 3.1 実験設定

実験はOSWorldベンチマークの5つの専門的ソフトウェアアプリケーション（VSCode、GIMP、LibreOffice Impress、VLC、LibreOffice Writer）で実施されました。
初期のアクターモデルとしてUI-TARS-7B-DPOを使用し、World State ModelとしてQwen2.5-VL-7Bベースの微調整モデル、
Curriculum GeneratorとしてQwen2.5-72Bを使用しています。

訓練は8台のNVIDIA A100 80GB GPUで実施され、各フェーズで1,000イテレーション、バッチサイズ16、
学習率2×10^-5（コサイン減衰）で行われました。

### 3.2 主要な結果

SEAgentは、OSWorldベンチマークにおいて顕著な性能向上を達成しました：

- 初期のUI-TARS-7B-DPOの成功率11.3%から、SEAgent（スペシャリストからジェネラリスト）は34.5%まで向上（+23.2%）
- 既存の強化学習手法（DigiRL: 21.8%、WebRL: 21.8%）を大幅に上回る性能
- 個々のソフトウェアでの成功率：VSCode 40.5%、GIMP 42.3%、Impress 22.7%、VLC 35.3%、Writer 31.8%

報酬モデルの評価では、提案したWorld State ModelがAgentRewardBenchで71.6%の精度を達成し、
オープンソースモデルの中で最高性能を記録しました。

### 3.3 既存手法との比較

商用モデルとの比較：
- GPT-4o: 7.08%、Claude3.7 Sonnet: 19.7%、Gemini-Pro-2.5: 21.7%を上回る性能

強化学習手法との比較：
- DigiRL（General RL）: 21.0%、WebRL（General RL）: 19.6%に対し、SEAgent（General RL）は30.6%を達成
- スペシャリストからジェネラリスト戦略により、さらに34.5%まで向上

## 4. 実用性評価
### 4.1 実装の容易性

SEAgentの実装は、既存のオープンソースモデル（UI-TARS、Qwen2.5シリーズ）をベースにしており、
商用APIに依存しない完全オープンソースのシステムとして構築されています。GitHubでコードが公開されており、
研究者や開発者が容易に再現・拡張できる設計となっています。

### 4.2 計算効率

訓練には8台のA100 GPUを使用し、各ソフトウェアについて3フェーズの訓練を実施します。
推論時は単一のGPUで動作可能で、実用的な計算コストで運用できます。

### 4.3 応用可能性

SEAgentのフレームワークは、以下の分野への応用が期待されます：

- 新しいソフトウェアへの自動適応：人間のアノテーションなしに新しいアプリケーションを学習
- ワークフロー自動化：複雑な業務プロセスの自動化
- ゲームプレイエージェント：正規化された仮想世界での学習
- 実世界の身体性を持つエージェント：ロボット制御などへの応用

## 5. まとめと所感
### 5.1 論文の意義

本論文は、コンピュータ使用エージェントの分野に重要な貢献をしています。人間の監督なしに自律的に学習できる
エージェントの実現は、AIの実用化において大きな前進です。特に、経験から学習するという概念を
具体的なシステムとして実装し、実験的に有効性を示した点は高く評価できます。

スペシャリストからジェネラリストへの学習戦略は、効率的な汎用エージェントの構築方法として興味深く、
他のドメインでも応用可能な手法と考えられます。

### 5.2 今後の展望

著者らも言及していますが、いくつかの制限事項があります：

1. 報酬信号は実環境からの真の信号ではなく、World State Modelに依存している
2. テストされたタスクは比較的単純で、人間の専門家なら20ステップ未満で完了できるもの
3. より長時間のワークフローや、より複雑なソフトウェアへの対応が今後の課題

将来的には、より複雑で長時間のタスクへの対応、実環境からの報酬信号の直接利用、
より多様なソフトウェアやドメインへの拡張が期待されます。また、このアプローチは
ゲームプレイや実世界のロボット制御など、他の分野のエージェントシステムにも
インスピレーションを与える可能性があります。