# VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing

## 基本情報
- arXiv ID: 2509.22651v1 (https://arxiv.org/abs/2509.22651)
- 著者: Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li
- 所属: CUHK MMLab, SenseTime Research, CPII under InnoHK
- 投稿日: 2025年09月30日
- カテゴリ: cs.AI, cs.LG

## 簡単に説明すると
この論文では、AI音声アシスタントの能力を「聞く」「話す」「見る」の3つの観点から総合的に評価するための新しいベンチマーク「VoiceAssistant-Eval」を提案しています。
従来のベンチマークでは音声理解や会話能力の一部しか評価されていませんでしたが、本研究では10,497件の厳選されたサンプルを使って、
自然音や音楽の理解、音声模倣、マルチモーダル（音声+画像）理解など、実際の音声アシスタントに求められる幅広い能力を評価できます。
評価の結果、現在のモデルは話すタスクは得意だが聞くタスクは苦手で、小さな専門モデルが大きなモデルを上回る場合があることが判明しました。
コードとデータは https://mathllm.github.io/VoiceAssistantEval/ で公開予定です。

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデル（LLM）と大規模マルチモーダルモデル（LMM）の進歩により、音声ファーストなAIアシスタントの実現可能性が高まっています。
しかし、既存の音声ベンチマークは範囲が限定的で、実世界の音声アシスタントに求められる能力を包括的に評価できていません。
現在のベンチマークには4つの主要な弱点があります。

第一に、音声のパーソナライゼーション評価の欠如です。特定の音声を模倣する能力は、パーソナライズされた魅力的なAIアシスタントの構築において重要ですが、
既存のベンチマークは明瞭性や自然さを重視するものの、特定の音声を模倣する能力を体系的に評価することはほとんどありません。

第二に、ハンズフリー相互作用への限定的な焦点です。現在の音声理解ベンチマークは、しばしばテキストベースの指示に依存しており、
実際の音声ファーストの使用との間にモダリティのミスマッチを生じています。

第三に、日常生活における様々な音声コンテキストの軽視です。実際のアシスタントは、人間の音声を超えて、
自然音、音楽、その他の複雑なコンテキストに関連する会話にも対応することが期待されています。

第四に、マルチモーダル（視覚+音声）統合評価の不十分さです。多くのアプリケーションでは、
アシスタントが言語と視覚的コンテキストを共同で処理する必要がありますが、現在のベンチマークではこれらの評価が不足しています。

### 1.2 主要な貢献
これらの課題を解決するため、本研究では包括的なベンチマーク「VoiceAssistant-Eval」を提案し、以下の重要な貢献を行いました。

- 聞く、話す、見るの3つの能力を統合的に評価する初の大規模ベンチマークの開発
- 13のサブセットにわたる10,497件の厳選されたインスタンスからなる包括的なデータセット構築
- 音声のパーソナライゼーション、ハンズフリー音声相互作用、マルチモーダル理解、音声コンテキストでの音声QAを含む4つの代表的なタスクの設計
- 内容品質、音声品質、一貫性を評価する三元評価システムの確立
- 22のモデル（21のオープンソースモデルとGPT-4o-Audio）に対する包括的な評価と分析

## 2. 提案手法
### 2.1 手法の概要
VoiceAssistant-Evalは、前述の4つの弱点に対処するために設計された包括的なベンチマークです。
ベンチマークは3つの主要なサブセットから構成されています。

「Listening」サブセットは、様々な音、音楽、人間の音声、およびそれらの組み合わせを含む音声クリップで構成され、
しばしば現実的な背景ノイズが伴います。これは、多様なコンテキストで音を認識し解釈するアシスタントの能力を評価するために設計されています。

「Speaking」サブセットは、複数のトピックと実世界のシナリオにわたる音声相互作用タスクから構成されます。
すべての相互作用は音声駆動であり、モデルがターゲット話者の音声を模倣することを要求するタスクに特に重点を置いています。

「Viewing」サブセットは、音声タスクと組み合わせて使用される様々なドメインからの幅広い画像を含んでいます。
これは、視覚的コンテキストと聴覚情報を統合するアシスタントの能力を評価します。

### 2.2 技術的詳細
データ収集プロセスでは、37のデータセットから多様な視覚的および聴覚的入力を抽出しました。
音声合成には、F5TTS、ChatTTS、Dia-1.6Bの3つの先進的なテキスト音声合成（TTS）モデルを使用し、
UTMOSを用いて生成された音声を評価し、3.8未満のスコアを受けたクリップは再生成しました。

データの品質と信頼性を確保するため、4段階のキュレーションプロセスを実施しました。
第一段階では、語彙的重複と編集距離マッチングの組み合わせを使用してコンテンツの重複除去を行いました。
第二段階では、破損した画像、無音の音声、不適切な質問や間違った答えを含む不完全または不適切なアイテムをフィルタリングしました。
第三段階では、残りの問題を13の事前定義されたタスクカテゴリに割り当て、
第四段階では、異なるタスクの比率をバランスさせるためにカテゴリ化された問題からデータをサンプリングしました。

評価には三元評価システムを採用し、内容品質、音声品質、および両者間の一貫性の3つの主要な次元でモデル応答を評価します。
内容品質の評価にはgpt-oss-20bを使用し、音声品質の測定にはUTMOSを使用します。
一貫性の評価では、Whisper-Large-v3を使用して音声出力を転写し、転写とモデルのテキスト応答間の修正されたWord Error Rate（WER）を計算します。

### 2.3 新規性
本研究の新規性は、既存のベンチマークが個別に扱っていた複数の評価次元を統合的に評価する点にあります。
従来のベンチマークは音声理解、パラ言語学、または視覚（もしくは音声）理解の部分的な側面のみをカバーしていましたが、
VoiceAssistant-Evalは話者の音色、マルチラウンド対話、複雑なコンテキスト、パラ言語的生成を同時に評価します。

特に革新的なのは、ロールプレイタスクにおける音声模倣能力の評価です。
RoleBenchから100の異なる役割を選択し、各役割につき3つの代表的な音声クリップを収集することで、
モデルの役割固有の話し方と音声の音色の両方を分析し、パーソナライズされた相互作用の可能性を実証しています。

また、真の音声ベースのマルチターン会話の評価も新しい試みです。
テキストでマルチラウンドのコンテキストを提供する代わりに、完全に音声ベースの対話を実現することで、
実際のハンズフリー使用シナリオをより正確に反映しています。

## 3. 実験結果
### 3.1 実験設定
22のモデル（21のオープンソースモデルとプロプライエタリなGPT-4o-Audio）を評価しました。
オープンソースモデルは、パラメータサイズによって以下のように分類されます。

小規模モデル（4B未満）には、mini-omni、mini-omni2、LLaMA-Omni2シリーズ、Baichuan-Omni-1d5の7つのコンパクトモデルが含まれます。
中規模モデル（約7B）には、Moshiバリアント、Llama-3.1-8B-Omni、Freeze-Omni、LLaMA-Omni2-7B-Bilingual、
glm-4-voice-9b、Kimi-Audio-7B-Instruct、MiniCPM-o-2_6、Step-Audio-2-mini、Qwen2.5-Omniの11のモデルが含まれます。
大規模モデル（10B以上）には、Step-AudioとLLaMA-Omni2シリーズが含まれます。

評価は、内容品質、音声品質、一貫性の3つの次元で実施され、最終スコアはこれら3つの個別スコアを乗算してパーセンテージに変換して算出されます。

### 3.2 主要な結果
実験結果から、いくつかの重要な発見が得られました。

第一に、プロプライエタリモデルがオープンソースモデルを一律に上回るわけではないということです。
GPT-4o-Audioは13タスク中4タスクでオープンソースモデルを上回ることができませんでした。
具体的には、Listening SoundとListening Speechタスクにおいて、Step-Audio-2-miniと比較してそれぞれ4.3点（47.7対52.0）、
9.1点（37.4対46.5）の低下が見られました。

第二に、現在のモデルは聞くタスクよりも話すタスクの方が得意であることが判明しました。
22モデル中20モデルがListeningよりもSpeakingで高いスコアを記録しました。
これは、音声アシスタントが音声コマンドや対話は上手に処理できるが、
音楽や環境音などの非音声音声の解釈にはまだ苦労していることを示唆しています。

第三に、小規模で良く設計されたモデルが大規模モデルに匹敵または上回る性能を示すことが明らかになりました。
特筆すべきは、Step-Audio-2-miniが32BのLLaMA-Omni2モデルの2倍以上のリスニング精度（40.06対16.00）を達成し、
最高の総合スコアを獲得したことです。

第四に、ロールプレイタスクとマルチモーダル（視覚+音声）統合は現在のモデルにとって困難な課題であることが示されました。
例えば、Qwen2.5-Omni-7Bは画像+テキストクエリで59.2%の精度を達成しましたが、
画像+音声クエリでは42.9%に留まり、16.3ポイントの低下を反映しています。

### 3.3 既存手法との比較
既存のベンチマークとの系統的な比較により、VoiceAssistant-Evalの包括性が明確になりました。
既存のベンチマークは部分的な側面のみをカバーしており、例えば音声リスニング、パラ言語学、
または視覚（もしくは音声）理解に焦点を当てているものの、話者の音色、マルチラウンド対話、
複雑なコンテキスト、パラ言語的生成を同時に扱うものはありませんでした。

VoiceBench、VocalBench、SOVA-Bench、SD-Evalなどの既存ベンチマークと比較して、
VoiceAssistant-Evalは聞く、話す、見るの能力を共同でテストし、
パーソナライズされた音声クローニング、ハンズフリー音声相互作用、マルチモーダル理解をカバーしています。

また、22のモデルに対する詳細な分析により、安全性アライメントと堅牢性にはさらなる改善が必要であることも明らかになりました。
Moshikaファミリーなどの一部の小規模または非整合モデルは、両方の面で貧弱な性能を示しました（堅牢性1未満、安全性28未満）。

## 4. 実用性評価
### 4.1 実装の容易性
VoiceAssistant-Evalの実装容易性は非常に高く評価できます。
研究チームは再現性を確保するため、データセットと評価コードの両方を公開する予定であり、
評価されたモデルの詳細情報、生成設定、すべての評価プロンプトが提供されます。

三元評価システムは既存のツール（gpt-oss-20b、UTMOS、Whisper-Large-v3）を活用しており、
新しい評価インフラストラクチャを一から構築する必要がありません。
データキュレーションプロセスも明確に文書化されており、他の研究者が同様のベンチマークを構築する際の参考になります。

ただし、37のデータセットからの大規模なデータ収集と4段階のキュレーションプロセスは、
初期構築時には相当な労力と時間を要することが予想されます。

### 4.2 計算効率
計算効率の観点では、ベンチマーク自体は効率的に設計されています。
10,497のサンプルというサイズは、包括的な評価を可能にしながらも、
過度に大規模すぎて実用的でないということはありません。

音声合成において複数のTTSモデルを使用し、UTMOSによる品質チェックと再生成プロセスを含むことは、
高品質なデータを保証する一方で、計算コストの増加を伴います。
しかし、これは一度限りの前処理コストであり、継続的な評価段階では問題になりません。

評価プロセスでは、内容評価にLLMを使用することで高精度な判定が可能ですが、
大規模なモデルの評価には相応の計算リソースが必要です。
しかし、これは現代のAI研究における標準的な要件の範囲内です。

### 4.3 応用可能性
VoiceAssistant-Evalの応用可能性は極めて高く、複数の重要な分野での活用が期待されます。

学術研究においては、音声アシスタント技術の進歩を定量的に追跡し、
異なる手法間の公平な比較を可能にする標準的なベンチマークとして機能します。
特に、聞く、話す、見るの統合評価により、真のマルチモーダルAIの発展を促進できます。

産業応用では、実際の音声アシスタント製品の開発において、
製品がリリース前に達成すべき性能基準の設定に活用できます。
ヘルスケア、教育、スマートホーム、顧客サービス、自動車産業など、
様々な分野での音声アシスタントの評価に直接適用可能です。

また、本ベンチマークで特定された課題（マルチモーダル統合の困難さ、音声理解の限界、
安全性とロバストネスの問題）は、今後の研究開発の方向性を明確に示しており、
技術的ロードマップの策定にも貢献します。

さらに、ハンズフリー操作が重要な運転、機械操作、視覚障害者支援などの
安全性が重要なアプリケーションでの評価基準としても活用できます。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、AI音声アシスタント分野における評価の標準化と包括化において重要な貢献を行っています。
最も注目すべき点は、従来のベンチマークが個別に扱っていた「聞く」「話す」「見る」の能力を統合的に評価する
初の大規模ベンチマークを提案したことです。

実用的な観点から見ると、本研究は現実世界での音声アシスタント使用シナリオを正確に反映した評価設計を行っています。
ハンズフリー操作、音声のパーソナライゼーション、マルチモーダル統合といった実際のアプリケーションで重要な要素を
系統的に評価できる点は、学術研究と産業応用の両方において極めて価値があります。

22のモデルに対する包括的な評価から得られた知見も重要です。
特に、プロプライエタリモデルがオープンソースモデルを一律に上回るわけではないという発見や、
小規模な専門モデルが大規模モデルを上回る場合があるという結果は、
今後のモデル開発戦略に重要な示唆を与えています。

三元評価システム（内容品質、音声品質、一貫性）の導入も革新的です。
従来の研究では音声の自然さや内容の正確性を個別に評価することが多かったのに対し、
これらを統合的に評価することで、より実用的な性能指標を提供しています。

### 5.2 今後の展望
本研究により明らかになった課題は、今後の音声アシスタント技術発展の明確な方向性を示しています。

最も重要な課題は、音声理解能力の向上です。ほとんどのモデルが話すタスクよりも聞くタスクで低い性能を示したことから、
音声エンコーダーの改善や、より効果的な音声表現学習手法の開発が急務です。
特に、自然音、音楽、環境音など、人間の音声以外の音響信号の理解能力向上が重要な研究課題となります。

マルチモーダル統合においても大きな改善の余地があります。
画像+音声クエリの性能が画像+テキストクエリより大幅に低いという結果は、
視覚情報と音声情報の効果的な融合手法の開発が必要であることを示しています。

安全性とロバストネスの向上も重要な課題です。一部のモデルで安全性スコアが低いことが示されており、
実際のアプリケーションでの使用を考慮すると、これらの側面での改善は不可欠です。

技術的な改善点としては、音声メモリの強化、応答生成の改善、視覚理解の向上が挙げられます。
また、ロールプレイタスクにおける応答の忠実性と音声の自然さのバランス調整も重要な研究課題です。

長期的には、このベンチマークが音声アシスタント技術の進歩を継続的に追跡し、
真に専門的で実用的なAIアシスタントの実現に向けた技術開発を促進することが期待されます。
特に、ヘルスケア、教育、アクセシビリティ支援などの社会的に重要な分野での応用拡大が見込まれます。
