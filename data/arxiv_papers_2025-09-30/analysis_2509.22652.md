# Pixel Motion Diffusion is What We Need for Robot Control

## 基本情報
- arXiv ID: 2509.22652v1 (https://arxiv.org/abs/2509.22652)
- 著者: E-Ro Nguyen, Yichi Zhang, Kanchana Ranasinghe ら
- 所属: Stony Brook University
- 投稿日: 2025年09月30日
- カテゴリ: cs.CV, cs.LG

## 簡単に説明すると
この論文では、ロボット制御のための新しい2段階拡散モデルフレームワーク「DAWN」を提案しています。
DAWNは「Diffusion is All We Need for robot control」の略称です。
従来の手法では高レベルの動作計画と低レベルの動作制御が十分に統合されていませんでした。
DAWNでは画素レベルの動き表現を中間表現として使用します。
言語指示から画素動作を生成する「Motion Director」と画素動作からロボット行動を生成する「Action Expert」の2つの拡散モデルを組み合わせています。
CALVINベンチマークで高い性能を達成し、MetaWorldや実世界環境でも優秀な結果を示しました。
プロジェクトページ: https://nero1342.github.io/DAWN/

## 1. 研究概要
### 1.1 背景と動機
ロボット操作における多段階の画素追跡手法は、解釈可能な中間画素動作とモジュラー制御を提供する有望な方向性として注目されています。
しかし、Im2Flow2Act、ATM、LangToMoなどの既存手法は性能面で課題がありました。
これらの手法は高性能なビジョン言語アクション（VLA）モデルや潜在特徴ベースの階層手法に比べて性能が劣っているという問題があります。

この性能差の主な原因は、2段階の中間画素動作ベースフレームワーク自体の限界ではありません。
各段階のコンポーネントが最新の技術進歩を十分に活用していないことが問題です。
具体的には、高レベル動作生成器が視覚生成モデリングの最新進歩を反映していません。
低レベルコントローラーも拡散ベース行動ポリシーの最新進歩を最適に活用していませんでした。

従来手法の問題点として、LangToMoは画素レベル拡散を使用しているため生成される動作表現の解像度が制限されます。
また、訓練のスケーラビリティに課題があります。
低レベルコントローラーは弱いViTアーキテクチャや手作りのヒューリスティックに基づいており、十分な性能を発揮できていませんでした。

### 1.2 主要な貢献
本研究では、これらの限界を解決するために構造化された中間画素動作と多様な事前訓練モデルの強みを活用しました。
以下の重要な貢献を達成しています。

- 構造化された中間画素動作を生成する言語条件付きビジュモータ―ポリシーとして、2段階拡散ベースフレームワークDAWNの提案
- 限定的なデータと小さなモデル容量にもかかわらず、CALVIN、MetaWorld、実世界ベンチマークで競合性能を達成
- 事前訓練された視覚・言語モデルを明示的に活用する設計により、ドメイン間でのデータ効率的転移を実現し、解釈可能性とモジュラー性を提供

## 2. 提案手法
### 2.1 手法の概要
DAWNフレームワークは2つの補完的な拡散モデルを組み合わせています。
高レベルの「Motion Director」は潜在画像拡散モデルで画素レベル動作生成を行い、低レベルの「Action Expert」は拡散トランスフォーマーで細かい動作シーケンス生成を担当します。
これら2つのモデルは明示的な画素動作表現を通じて相互作用します。

Motion Directorは、マルチビュー画像と言語指示を条件として、タスクに適合した画素動作を反復的に生成します。
画素動作は入力ビューの1つにグラウンディングされており、中間表現に解釈可能性を与えています。
Action Expertは生成された画素動作と追加の入力を受け取り、最終的なロボット行動シーケンスを生成します。

### 2.2 技術的詳細
Motion Directorは事前訓練された潜在拡散モデル上に構築されています。
U-Netデノイザー、テキストエンコーダー、事前訓練されたVAEエンコーダー・デコーダーペアで構成されます。
また、代替カメラビューからの埋め込みを抽出するためのビジョンエンコーダーも組み込まれています。

推論時には、ガウシアンノイズテンソルを現在フレームの潜在符号化と連結し、ノイズのある潜在表現を形成します。
U-Netは言語埋め込み、視覚埋め込み、時間オフセットの条件付けの下でこれをデノイズします。
全ての条件付けトークンが連結され、各デノイズ段階でU-Netのクロスアテンション層に注入されます。

Action Expertは4つの主要コンポーネントで構成されます。
まず、Motion Directorからの画素動作出力と現在の視覚観測の両方をエンコードする共有ビジュアルエンコーダーがあります。
次に、言語指示を埋め込むテキストエンコーダー、低次元ロボット状態を2層MLPで処理する状態エンコーダー、行動シーケンスを生成するデノイジングトランスフォーマーがあります。

### 2.3 新規性
本研究の主要な新規性は、事前訓練された潜在拡散モデルを密な画素動作生成に適応し、画素動作を使って完全学習可能な設定下でビジュモータ制御のための拡散ポリシーを導くことです。
これは著者らの知る限り初めての試みです。

従来手法と比較して、DAWNは階層的な動作分解とエンドツーエンドビジュモータエージェントの強みを橋渡しします。
同時に画素動作という中間表現を通じて解釈可能性とモジュラー性を維持しています。
2つの拡散モデルは光学フローを真の画素動作として使用して並列に訓練でき、独立してアップグレード可能です。

## 3. 実験結果
### 3.1 実験設定
CALVINベンチマークでは、4つの異なる分割（A、B、C、D）を含む34タスクの24k実演データを使用しました。
最も困難なABC→D設定で評価し、A、B、C環境で訓練してD環境でテストしました。
MetaWorldでは7自由度Sawyerロボットアームを使用した11の困難なタスクで評価しました。

実世界実験では7自由度xArm7ロボットアームと2つのRGBカメラを使用しました。
横からの第三者視点を提供するIntel RealSense D435と、グリッパー上部に取り付けられたIntel RealSense D405です。
6種類のおもちゃとコンテナを使用した持ち上げ配置操作の1000エピソードデータセットを構築しました。

評価指標として、CALVINでは連続する5つのタスクのシーケンス成功率と平均完了タスク数を使用しました。
MetaWorldでは各タスクの成功率を、実世界では正しい対象の持ち上げと配置の成功率を測定しました。

### 3.2 主要な結果
CALVINベンチマークにおいて、外部データを使用しない設定でDAWNは平均4.00タスクを完了し、従来手法を上回る性能を達成しました。
5番目のタスクで60.6%の成功率を記録し、VPP（60.4%）と同等の結果を示しました。

外部ロボットデータ（DROID）を使用した設定では、平均4.10タスクを完了しました。
これはVPP（4.33）やDreamVLA（4.44）には及びませんが、競争力のある結果を示しています。
重要なのは、DAWNがこれらの手法よりも少ないデータとモデル容量で、これらの結果を達成したことです。

MetaWorldでは全体で65.4%の成功率を達成し、LTM（57.7%）を7.7ポイント上回る性能を示しました。
特に意味的に類似だが機能的に異なるタスク（ドア開け94.7%対ドア閉め97.3%）で顕著な改善を達成しました。

実世界実験では、Enhanced Diffusion Policyやπ0と比較して、正しい対象の持ち上げと配置において一貫して優れた性能を示しました。
特に間違った対象を持ち上げるエラーが顕著に減少し、意味的な理解能力の向上を示しています。

### 3.3 既存手法との比較
従来の2段階の画素追跡手法（Im2Flow2Act、ATM、LangToMo）と比較して、DAWNは顕著な性能改善を達成しました。
これは両段階において強力な拡散モデルを使用し、事前訓練されたビジョン・言語バックボーンを活用したことによります。

VLA手法との比較では、はるかに少ないパラメータ数でありながら競争力のある性能を示しました。
これは構造化された画素動作表現により計算効率を向上させ、事前訓練モデルにより少ないデータで学習できることによるものです。

階層的手法（VPP）との比較では、同等または若干劣る性能でしたが、約半分の事前訓練データで実現しています。
重要な発見として、DAWNの変種（DAWN*）がVPPと同等の設定下で一貫してVPPを上回る性能を示したことがあります。

## 4. 実用性評価
### 4.1 実装の容易性
DAWNの実装容易性は非常に高いと評価できます。
既存の事前訓練モデル（Stable Diffusion、CLIP、DINOv3、T5）を活用するため、新しいアーキテクチャを一から構築する必要がありません。

2つのモジュールは独立して訓練可能で、光学フローを真の画素動作として使用できるため、データ収集も比較的簡単です。
PyTorch、HuggingFace Diffusers、Transformersライブラリを使用した実装で、標準的な深層学習環境で動作します。

ただし、2つの拡散モデルを順次実行するため、推論時間は単一モデルより長くなります。
また、両モデルの最適な統合には一定の調整が必要です。

### 4.2 計算効率
計算効率の観点では、事前訓練モデルの活用により訓練時間とメモリ使用量を削減できます。
Motion DirectorではU-Netデノイザーのみを更新し、その他のモジュールは凍結するため、訓練コストが削減されます。

推論時はMotion Directorで25ステップ、Action Expertで複数ステップの拡散プロセスを実行するため、計算コストは高めです。
しかし、階層的な設計により、高レベル計画は10ステップごとに更新すれば十分で、計算オーバーヘッドを軽減できます。

4台のNVIDIA A6000 GPUでの訓練が可能で、現代のロボット研究環境において実現可能な計算要求です。
Mixed precisionトレーニングによりメモリ使用量とスループットが改善されています。

### 4.3 応用可能性
DAWNの応用可能性は極めて高く、複数の分野での活用が期待されます。
シミュレーション環境から実世界への転移が限定的なファインチューニングで可能なことが実証されており、実用的な価值が高いです。

モジュラー設計により、各コンポーネントを独立してアップグレードできるため、技術進歩に応じた改良が容易です。
画素動作という解釈可能な中間表現により、失敗分析とデバッグが可能で、実用システムでの重要な要件を満たしています。

単腕操作から双腕操作まで対応可能で、様々なロボットプラットフォームへの適用が期待されます。
製造業、サービスロボット、家庭用ロボット、研究用プラットフォームなど、幅広い分野での活用が可能です。

特に、少量のデータで高い性能を達成できる特性は、新しいタスクやドメインへの迅速な適応において大きな利点となります。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、ロボット制御における中間表現の重要性を再認識させる重要な貢献をしています。
特に、構造化された画素動作表現が効果的な中間表現として機能し、解釈可能性とモジュラー性を提供しながら高い性能を実現できることを示しました。

2つの強力な拡散モデルを組み合わせるアプローチは、階層的制御とエンドツーエンド学習の利点を効果的に統合しています。
事前訓練モデルにより訓練データを1/10に削減しつつ競争力のある性能を達成した点も評価できます。

実世界実験での成功は、シミュレーションから現実への転移可能性を実証しており、実用的な価値を示しています。
特に、意味的な理解能力の改善（正しい対象の識別）は、実用システムにおいて極めて重要な能力です。

画素動作という中間表現の可視化可能性は、ロボット行動の理解とデバッグにおいて大きな利点となり、
実用システムでの信頼性向上に貢献します。

### 5.2 今後の展望
この研究により明らかになった今後の発展方向性は多岐にわたります。
まず、推論速度の改善が重要な課題です。現在の25ステップの拡散プロセスをより少ないステップで実現する手法の開発が期待されます。

より複雑なタスクへの拡張も興味深い方向性です。
現在の持ち上げ配置タスクから、組み立て、操作、協調作業などのより高次なタスクへの適用が期待されます。
双腕操作での実証はその第一歩といえます。

画素動作表現の改良も重要な研究課題です。
現在の2D画素動作から3D空間動作への拡張、時系列の画素動作の考慮、より豊富な動作表現の導入などが考えられます。

他のロボットプラットフォーム（移動ロボット、ヒューマノイドロボット）への適用も興味深い展開です。
画素動作という汎用的な中間表現の特性を活かし、様々な身体性を持つロボットでの活用が期待されます。

また、大規模データセットでの事前訓練による性能向上の余地も大きいです。
より多様で大規模なロボット実演データでの事前訓練により、さらなる性能向上が期待されます。

最終的に、この研究は構造化された中間表現を用いた解釈可能で計算負荷を削減するロボット制御の可能性を示し、
実用的なロボットシステムの実現に向けた重要な一歩を踏み出したといえます。
