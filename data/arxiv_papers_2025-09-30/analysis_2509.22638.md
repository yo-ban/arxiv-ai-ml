# Language Models Can Learn from Verbal Feedback Without Scalar Rewards

## 基本情報
- arXiv ID: 2509.22638v1 (https://arxiv.org/abs/2509.22638)
- 著者: Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du ら
- 所属: Sea AI Lab ら
- 投稿日: 2025年09月30日
- カテゴリ: cs.CL, cs.LG

## 簡単に説明すると
この論文では、従来の人間フィードバックからの強化学習（RLHF）の問題点を解決する新しいアプローチを提案しています。
従来手法では「この回答はとても良いが効率性に問題がある」といった言語的フィードバックを数値報酬に変換していました。
しかし、この変換過程で重要な情報の消失や曖昧性、報酬スケールの不均衡といった問題がありました。
本研究では「フィードバック条件付きポリシー（FCP）」を導入し、言語的フィードバックを条件信号として直接学習します。
テキスト画像生成における言語事前分布からヒントを得て、応答フィードバックペアから最尤推定で学習し、
さらにオンラインブートストラッピングでポリシーを改良します。
この手法により、数値報酬への変換なしに言語モデルが言語的フィードバックから直接学習できることを実証しました。
コードは https://github.com/sail-sg/feedback-conditional-policy で公開されています。

## 1. 研究概要
### 1.1 背景と動機
強化学習における報酬仮説は20年以上前に提案され、環境からのフィードバックを数値報酬に削減することがRL アルゴリズムの動作に必要とされてきました。
この考え方は分野の進歩を形作り、大規模言語モデル（LLM）のアライメントと推論におけるRL適用の標準的手法となっています。

しかし実際には、特に検証不可能な設定でのLLMのRLフィードバックは、
「良いスタートだが、コードはもっと効率的にできる」といった言語化されたものが多くあります。
このようなフィードバックは人間のユーザー、生成報酬モデル、またはエージェントシナリオにおけるツール出力から得られます。

言語的フィードバックを数値報酬に削減することには3つの主要な限界があります。
第一に情報損失です。数値報酬は言語的フィードバックや批評よりもはるかに少ない情報しか捉えず、しばしば解釈不可能です。
「回答は冗長だが正しい」と「回答は簡潔だが多くのタイポがある」という異なる批評が、どちらも0.8の報酬に縮約される可能性があります。

第二に曖昧性です。特に人間ユーザーからの言語的フィードバックは、しばしば混合的（長所と短所の両方を含む）、
感情的、または不確実で、「とても嬉しい」や「よくわからない、もう一度試してみて？」のようなものです。
このようなフィードバックを数値にマッピングすることは不明確で恣意的になる可能性があります。

第三に複数タスク間での不均衡な報酬スケールです。マルチタスク訓練において、
一貫した報酬スケールを維持することは困難で、学習プロセスにバイアスを与える可能性があります。

### 1.2 主要な貢献
大規模言語事前訓練の台頭により、この見方は再検討されています。
LLMは強力な常識と言語的事前分布を体現しており、新しいパラダイムを示唆しています。
本研究では以下の重要な貢献を達成しています。

- 言語的フィードバックを一級訓練信号として扱うフィードバック条件付きポリシー（FCP）の提案
- 応答フィードバックペアから直接学習し、オフラインデータでの最尤訓練を通じてフィードバック条件付き事後分布を近似する手法の開発
- ポリシーが正の条件下で生成し、新鮮なフィードバックを受け取って自己改良するオンラインブートストラッピング段階の導入
- フィードバック駆動学習を報酬最適化ではなく条件生成として再構築し、LLMが言語的フィードバックから直接学習するより表現力豊かな方法の実現

## 2. 提案手法
### 2.1 手法の概要
フィードバック条件付きポリシー（FCP）は、π_θ(o|x,c) ∝ π_ref(o|x) · p_env(c|x,o)として定式化されます。
ここで、π_ref(o|x)は指示xに対して応答oを生成する参照ポリシー、
p_env(c|x,o)は環境フィードバックcの分布です。

直感的に、FCPは各応答oが観察されたフィードバックcを引き起こす可能性によって参照ポリシーを重み付けします。
正のフィードバックc⁺で条件付けすると、π_θ(o|x,c⁺) ∝ π_ref(o|x) · p_env(c⁺|x,o)となり、
好ましいフィードバックを受ける可能性が高い応答の生成確率を増加させます。

このようにして、FCPはπ_refからの事前知識と言語的フィードバックを統合する事後分布を学習し、
混合的なものを含む多様な形式のフィードバックを処理できます。

### 2.2 技術的詳細
オフラインFCPの訓練後、オンラインブートストラッピングによってさらに改良します。
具体的には、動作ポリシーπ_θ(o|x,c⁺)（正のフィードバックを目標条件とする）からのロールアウトをサンプリングし、
p_envからの新鮮なフィードバックで再注釈することで、反復的にポリシーを強化します。

テキスト画像生成における言語事前分布からインスピレーションを得ています。
言語事前分布は未見のプロンプトから新規出力を可能にし、これは混合フィードバックc₁とc₂を組み合わせて
「バナナが海でサーフィンしている」のような稀な画像（純粋に正の応答o⁺に類似）を生成することに類似しています。

FCPは最尤推定により応答フィードバックペアから学習し、任意のフィードバックcに条件付けできます。
正のフィードバックで条件付けされた場合、好ましい応答を生成する確率が向上します。

### 2.3 新規性
本研究の主要な新規性は、言語的フィードバックを数値報酬に変換せずに直接条件信号として使用することです。
従来のRLHF手法が言語的フィードバックを必然的に数値に圧縮していたのに対し、
FCPは言語的フィードバックの豊かさを保持したまま学習を可能にします。

テキスト画像生成の言語事前分布との類推も独創的です。
見たことのあるキャプション（混合フィードバックに類似）を組み合わせて稀な画像（純正の回答に類似）を生成する能力を、
言語モデルの フィードバック学習に応用したアイデアは革新的です。

オフライン学習とオンラインブートストラッピングの2段階アプローチも特徴的です。
最初にオフラインデータで基本的なフィードバック条件付きポリシーを学習し、
その後オンラインでポリシー自体が生成した データに新しいフィードバックを付与して自己改良する設計は効率的です。

## 3. 実験結果
### 3.1 実験設定
実験は数学問題解決とコード生成タスクで実施されました。
数学タスクではGSM8KとMATHデータセットを使用し、コード生成ではHumanEvalとMBPPを使用しました。

比較ベースラインとして、オフラインRFT（Rejected Fine-Tuning）とオンラインGRPO（Group Relative Policy Optimization）を使用しました。
これらは数値報酬を使用する強力な従来手法です。

評価指標として、各タスクでの正解率を使用しました。
また、生成された応答の品質を言語的フィードバックの観点から定性的にも評価しました。

フィードバック生成には、GPT-4やClaude等の強力な言語モデルを使用し、
多様で現実的なフィードバックを確保しました。

### 3.2 主要な結果
パイロット実験では、FCPが強力な数値ベースベースラインであるオフラインRFTとオンラインGRPOと同等またはそれ以上の性能を達成しました。
重要なことに、これは検証器、数値変換、またはデータフィルタリングに依存することなく実現されています。

数学問題解決では、FCPは従来手法と競合する性能を示し、特に複雑で多段階の推論が必要な問題で優れていました。
コード生成タスクでは、生成されたコードの正確性と効率性の両方で改善が見られました。

定性的評価では、FCPが混合的なフィードバック（正の側面と負の側面の両方を含む）をより効果的に活用できることが確認されました。
従来手法では このような混合フィードバックの処理が困難でしたが、FCPは自然に処理できます。

オンラインブートストラッピング段階の導入により、さらなる性能向上が観察されました。
ポリシーが自己生成した応答に対する新しいフィードバックから学習することで、継続的な改善が可能になっています。

### 3.3 既存手法との比較
従来のRLHF手法と比較して、FCPは情報損失を大幅に削減します。
数値報酬への圧縮が不要なため、「正確だが冗長」や「簡潔だが不完全」といった 複雑なフィードバックのニュアンスを保持できます。

RFTやGRPOなどの数値ベース手法では、フィードバックの複雑さを単一の数値に削減する必要があります。
これに対してFCPは、フィードバックの全ての情報を保持し、条件生成として学習します。

計算効率の観点では、FCPは追加の報酬モデルや検証器を必要としないため、
シンプルで スケーラブルなフレームワークを提供します。
これは報酬ハッキングのリスクも回避し、より安定した学習を実現します。

また、マルチタスク設定での報酬スケール不均衡問題も自然に解決されます。
各タスクのフィードバックがそのままの形で保持されるため、異なるドメイン間での人為的な正規化が不要です。

## 4. 実用性評価
### 4.1 実装の容易性
FCPの実装は比較的シンプルで、既存の言語モデル訓練パイプラインに容易に統合できます。
最尤推定による訓練は標準的な手法であり、特別なアーキテクチャ変更を必要としません。

コードがGitHubで公開されており、再現可能性が確保されています。
実装は PyTorchベースで、一般的な深層学習フレームワーク上で動作します。

ただし、高品質な言語的フィードバックの生成には強力な言語モデル（GPT-4やClaude等）が必要で、
これには相応の計算コストが伴います。
また、フィードバックの品質が最終的な性能に大きく影響するため、フィードバック生成の設計が重要です。

### 4.2 計算効率
計算効率の観点では、FCPは追加の報酬モデルや価値関数の訓練が不要なため、
従来のRLHF手法よりもシンプルです。
最尤推定による訓練は効率的で、安定した収束を示します。

オンラインブートストラッピング段階では、ポリシーからのサンプリングと新しいフィードバック生成が必要ですが、
これは従来のオンラインRL手法と同程度の計算コストです。

フィードバック生成に外部の強力な言語モデルを使用する場合、API呼び出しコストが発生します。
しかし、一度生成されたフィードバックは再利用可能で、バッチ処理により効率化できます。

メモリ使用量の観点では、応答とフィードバックのペアを保存する必要がありますが、
これは一般的な教師あり学習と同程度の要求です。

### 4.3 応用可能性
FCPの応用可能性は極めて高く、様々なドメインでの活用が期待されます。
言語的フィードバックが自然に発生するあらゆるタスクに適用可能です。

教育分野では、学習者の回答に対する教師のフィードバックから学習するシステムを構築できます。
「理解は正しいが説明が不十分」といった複雑なフィードバックを直接活用できる利点があります。

コード レビューシステムでは、人間のレビュアーからの「ロジックは正しいがパフォーマンスに問題」
といったコメントから学習できます。
従来システムではこのような フィードバックの処理が困難でしたが、FCPは自然に対応できます。

顧客サービスやチャットボットでの応用も有望です。
ユーザーの「助かったけど回答が長すぎる」といったフィードバックから直接学習し、
応答スタイルを調整できます。

創作支援システムでは、「アイデアは良いが構成に問題」といった編集者やユーザーのフィードバックを活用し、
より良い文章生成を学習できる可能性があります。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、強化学習における長年の前提である報酬仮説に挑戦する重要な研究です。
言語的フィードバックを数値報酬に変換することの限界を明確に指摘し、
代替的なアプローチを提示した意義は大きいです。

特に、大規模言語モデルの時代において、言語的情報を言語として直接処理するアプローチは自然で合理的です。
従来のRLHF手法が強制的に行っていた数値化のボトルネックを回避し、
より豊かな情報を保持できる点は実用的価値が高いです。

テキスト画像生成の言語事前分布との類推も洞察に富んでいます。
異なるドメインからのアイデアを巧妙に組み合わせ、新しい学習パラダイムを創出した点は評価できます。

実装のシンプルさも重要な利点です。複雑な報酬エンジニアリングや価値関数の訓練を避け、
最尤推定という基本的な手法で効果的な学習を実現している点は実用性を高めています。

### 5.2 今後の展望
この研究により明らかになった今後の発展方向性は多岐にわたります。

まず、フィードバック生成の自動化と改善が重要な課題です。
現在は外部の強力な言語モデルに依存していますが、より効率的で専門的なフィードバック生成システムの開発が期待されます。

複数のフィードバック源からの情報統合も興味深い方向性です。
人間、AI、ツールからの異なる観点のフィードバックを効果的に組み合わせる手法の開発により、
より包括的な学習が可能になります。

リアルタイムでのフィードバック適応も重要な発展領域です。
現在のオンラインブートストラッピングをさらに発展させ、
ユーザーとの対話中にリアルタイムでフィードバックを取り込み学習する システムの実現が期待されます。

異なる言語や文化圏でのフィードバックパターンへの適応も研究課題です。
フィードバックの表現方法や評価基準は文化により異なるため、
多様な言語・文化に対応可能な FCPの発展が求められます。

長期的には、この研究は人工知能と人間の相互作用における新しいパラダイムの基礎となる可能性があります。
数値化を介さない直接的な言語コミュニケーションによる学習は、
より自然で効果的な AI システムの実現に貢献すると期待されます。