# CharaConsist: Fine-Grained Consistent Character Generation

## 基本情報
- arXiv ID: 2507.11533v1 (https://arxiv.org/abs/2507.11533)
- 著者: Mengyu Wang、Henghui Ding、Jianing Peng、Yao Zhao、Yunpeng Chen、Yunchao Wei
- 所属: Beijing Jiaotong University、Fudan University、Alkaid Pte. Ltd.
- 投稿日: 2025年7月16日
- カテゴリ: cs.CV、cs.AI

## 簡単に説明すると
この論文は、テキストから画像を生成する際に、同じキャラクターが複数の画像に連続して登場するような一貫性のある画像生成を実現する技術「CharaConsist」を提案しています。従来の手法では、背景が変わったり、キャラクターの服装やポーズが変化したりすると一貫性が崩れてしまう問題がありました。CharaConsistはポイントトラッキングという新しい手法を使い、キャラクターと背景の両方を細かく一貫させます。これにより、ストーリーテリング、キャラクターデザイン、動画作成などのコンテンツ制作に実用的に応用できます。

## 1. 研究概要
### 1.1 背景と動機
テキストから画像を生成する技術は近年大きく進歩し、特に拡散トランスフォーマー（DiT）の登場により高品質な画像生成が可能になりました。しかし、複数の画像にわたって同じキャラクターを一貫して描写することは依然として困難な課題です。StoryDiffusionやConsiStoryなどの既存手法は、キャラクターの背景遷移には対応できますが、以下の問題があります。第一に、同じシーン内での連続ショットを生成する際、背景の詳細が一貫せず、ビデオのキーフレームとして使用するとちらつきが発生します。第二に、キャラクターのポーズやアクションが大きく変化すると、服装や髪型などの詳細が変わってしまいます。さらに、これらの手法はSDXLなど古いモデルに基づいており、最新のDiTアーキテクチャには対応していません。

### 1.2 主要な貢献
この論文の主要な貢献は3つあります。第一に、ポイントトラッキングアテンションという新しい機構を提案しました。これは画像間の位置対応を自動的に追跡し、位置エンコーディングを再設定することで、キャラクターが大きく動いても一貫性を保てます。第二に、適応的トークンマージ戦略を導入し、位置変更による局所的な幾何構造の崩れを補正します。

第三に、前景と背景を分離して制御するマスク抽出機能を実装しました。これにより、同じシーンでの連続ショットや異なるシーンへの移動に柔軟に対応できます。技術的には、これは初めてDiTモデル（FLUX.1）に対応した一貫した画像生成手法で、追加の訓練やモジュールを必要としません。実用的には、ビジュアルストーリーテリング、バーチャルキャラクターデザイン、長編動画生成などのアプリケーションで利用可能です。

## 2. 提案手法
### 2.1 手法の概要
CharaConsistは、FLUX.1という最新のDiTモデルをベースにした訓練不要の一貫した画像生成手法です。パイプラインは2つのフェーズから構成されます。まずアイデンティティ画像を通常の生成プロセスで作成し、その際の中間変数（キー、バリュー、アテンション出力）を保存します。次にフレーム画像を生成する際、これらの保存された変数を参照して一貫性を保ちます。主要コンポーネントは以下の4つです。

1. ポイントマッチング：フレーム画像の各点に対応するアイデンティティ画像の点を見つける
2. マスク抽出：テキストトークンへのアテンション重みを分析し、前景と背景を分離する
3. ポイントトラッキングアテンション：マッチング結果に基づいてアイデンティティ画像のキーを抽出し、RoPEを用いて位置エンコーディングを再計算する
4. 適応的トークンマージ：マッチング信頼度を考慮してアテンション出力を補間するこの手法により、1枚のアイデンティティ画像だけで高い一貫性を実現できる。

### 2.2 技術的詳細
技術的な詳細として、まずポイントマッチングでは、アテンション層の出力間のコサイン類似度を計算します。各層で類似度が大きく変動するため、同じタイムステップの複数層で平均化して安定したマッチングを得ます。マスク抽出では、テキストプロンプトを背景説明、前景説明の順にフォーマットし、画像トークンから各テキストトークンへのアテンション重みを比較します。ポイントトラッキングアテンションでは、アイデンティティ画像のキー・バリューを抽出し、Rotary Position Embedding（RoPE）を用いてフレーム画像の対応位置に再エンコードします。これにより、物理的な位置関係を維持しながら意味的な対応を実現します。前景と背景は別々に処理され、アテンションマスクを用いて前景は前景、背景は背景とのみアテンションを計算するよう制御します。最後に、適応的トークンマージでは、位置再配置による局所的な幾何構造の崩れを補正するため、マッチング信頼度に基づいてアテンション出力を加重平均します。信頼度が低いトークンは抑制され、悪影響を防ぎます。これらの技術により、キャラクターが大きく動いても細かい部分まで一貫性を保てます。

### 2.3 新規性
CharaConsistの新規性は主に4点あります。

第一に、従来手法が採用している画像間アテンションの問題点を明らかにしました。従来手法では、アテンションが空間的に近い領域に偏る「局所性バイアス」により、キャラクターの位置が大きく変わると一貫性が保てませんでした。これに対し、ポイントトラッキングと位置エンコーディングの再設定により、意味的に対応する点を正確に追跡します。

第二に、DiTモデルに初めて対応した点です。従来手法はUNetアーキテクチャに依存しており、最新のDiTモデルには適用できませんでした。

第三に、前景と背景の分離制御です。従来手法はキャラクターのみに焦点を当て、背景の一貫性は無視していました。

第四に、必要な画像数の削減です。従来手法は2～4枚の画像を並列生成する必要がありましたが、本手法は1枚のアイデンティティ画像だけで十分です。

## 3. 実験結果
### 3.1 実験設定
実験では、既存のベンチマークが本タスクに適していないため、GPT-4を使用して新たな評価データセットを作成しました。プロンプトは「[環境]、[キャラクター]、[アクション]」の形式で、2つの評価タスクを設定しました。

「背景維持」タスクでは環境とキャラクターを固定し、アクションのみ変化させます。「背景切り替え」タスクではキャラクターのみ固定します。各グループは5～8個のプロンプトで構成され、各タスクに200以上のプロンプトを用意しました。

比較手法は、訓練不要の一貫した画像生成手法（StoryDiffusion、ConsiStory）と訓練必要のアイデンティティ参照手法（IP-Adapter、PhotoMaker）です。

評価指標は、CLIPテキスト-画像類似度（CLIP-T）、CLIP画像間類似度（CLIP-I）を使用しました。さらに、前景と背景を分離したCLIP-I-fg/CLIP-I-bg、顔認識によるアイデンティティ類似度（ID Sim）、画像品質スコア（IQS）、画像美学スコア（IAS）も使用しました。

### 3.2 主要な結果
定量評価の結果、CharaConsistはほとんどの一貫性指標で優れた性能を示しました。背景維持タスクでは、CLIP-Iで0.910（他手法は0.891～0.903）を記録し、特にCLIP-I-bgでは0.916と他手法（0.881～0.895）を大きく上回りました。これにより、背景の一貫性で優れていることが示されました。アイデンティティ類似度も訓練不要手法の中で高い0.647を達成し（他は0.578～0.642）、訓練ベース手法のIP-Adapter（0.664）に迫る性能です。画像品質においても、IASで0.831、IQSで0.985と高い値を記録しました。重要な発見として、アブレーション研究により、FLUX.1ベースモデル自体は従来のモデルより一貫性が低いことを確認しました。一方で、CharaConsistの適用による一貫性向上幅は最大でした。

例えばID Simの向上幅は0.171で、他手法の0.096～0.130を上回りました。定性的にも、キャラクターが大きく動いても服装や髪型が保たれ、背景も高い精度で一致していることが確認されました。

### 3.3 既存手法との比較
既存手法との比較で、CharaConsistは特に以下の点で優位性を示しました。一貫した画像生成手法（StoryDiffusion、ConsiStory）と比較して、キャラクターが大きく動いても服装や髪型の詳細が保たれます。従来手法では、キャラクターがほぼコピーペースト状態になるか、アクションに変化をつけると一貫性が崩れるというジレンマがありました。これに対し、本手法は両方を解決しています。アイデンティティ参照手法（IP-Adapter、PhotoMaker）と比較して、顔の類似度では若干劣ります。一方、CLIP-I-fg（0.881 vs 0.876-0.883）で同等以上、CLIP-I-bg（0.916 vs 0.881-0.886）で大きく優れています。これは、訓練ベース手法が顔に特化しており、服装や背景の一貫性を無視しているためです。限界として、CLIP-Tスコアが他手法より低い点があります。これはFLUX.1ベースモデルのドメイン差によるものです。また、入力アイデンティティを参照する機能がないため、特定の実在人物を生成できません。

## 4. 実用性評価
### 4.1 実装の容易性
CharaConsistの実装は比較的容易です。訓練不要のため、大規模なデータセットや計算リソースは不要で、FLUX.1モデルの動作環境があれば十分です。ソースコードはGitHubで公開されており、研究者や開発者は容易に再現可能です。必要なメモリは、通常のFLUX.1の実行に加えて、アイデンティティ画像の中間変数を保存する分のみです。従来手法のように複数枚を同時生成する必要がないため、実際にはメモリ効率が向上しています。主な実装課題は、ポイントマッチングの精度とマスク抽出の安定性ですが、論文では複数層での平均化による解決法が示されています。計算時間は通常のFLUX.1生成と同程度で、特殊なハードウェアも必要ありません。

### 4.2 計算効率
計算効率の面でCharaConsistは大きなアドバンテージがあります。従来手法はStoryDiffusionで最低2枚、ConsiStoryでは4枚の画像を同時に生成する必要があり、GPUメモリ使用量が2～4倍になります。一方、本手法は1枚のアイデンティティ画像だけで、その後のフレーム画像は単独で生成できるため、追加メモリは中間変数の保存分のみです。処理速度に関しても、ポイントマッチングとマスク抽出は中間アテンション出力の再利用で計算されるため、追加コストは最小限です。RoPEの再エンコーディングも高速な演算で、全体の処理時間は通常のFLUX.1生成とほぼ同等です。大規模なコンテンツ制作において、例えば10枚の一貫した画像を生成する場合、従来手法では3～5回のバッチ処理が必要です。しかし、本手法では1回のアイデンティティ生成後に1枚ずつ順次生成でき、GPUメモリが限られた環境でも実用的です。

### 4.3 応用可能性
CharaConsistの応用分野は多岐にわたります。ビジュアルストーリーテリングでは、同じキャラクターが登場する一連の絵本やマンガを生成できます。動画制作では、キーフレーム生成やストーリーボード作成に活用でき、特に同じシーン内でのキャラクターの動きを描写する際に背景が正確に一致するため、ちらつきのない映像が作成できます。ゲーム開発では、キャラクターのコンセプトアートやスプライト作成に利用できます。バーチャルキャラクターデザインでは、一貫したアバターを様々なシチュエーションで描写できます。教育コンテンツでは、同じキャラクターが様々な状況を説明する教材作成に活用できます。広告・マーケティングでは、ブランドキャラクターを使った一貫性のあるキャンペーン素材を作成できます。また、提案されたポイントトラッキングとマスク抽出技術は、画像編集などの関連タスクにも応用可能です。

## 5. まとめと所感
### 5.1 論文の意義
この論文の学術的意義は、一貫した画像の生成における局所性バイアス問題を初めて明らかにし、ポイントトラッキングという新しいアプローチで解決した点です。従来の画像間アテンションが空間的近傍性に支配されるという発見は、アテンション機構の理解に新たな視点を提供します。また、DiTモデルに対する初の一貫した画像生成手法であり、UNetベースの従来手法が直接適用できないDiTアーキテクチャへの対応手法を示した点でも重要です。産業的意義としては、コンテンツ制作の効率化に大きく貢献します。従来、一貫性のあるキャラクター描写には熟練したアーティストの手作業が必要でしたが、本技術により自動化が可能になります。特にメモリ効率が高く、GPUリソースが限られた環境でも実用可能な点は、幅広い普及が期待できます。背景の完全な一貫性を保つことで、動画キーフレーム生成や連続ショット作成に直接応用でき、クリエイティブ産業のワークフローを変革する可能性があります。

### 5.2 今後の展望
今後の発展方向として、まずアイデンティティ参照機能の統合が期待されます。現在はテキストプロンプトのみからキャラクターを生成しますが、入力画像を参照して特定の人物を描写できるようになれば、応用範囲が大きく広がります。これは訓練ベースのアイデンティティ手法と本手法の統合で実現可能です。次に、より複雑なシーンへの対応が課題です。現在は単一キャラクターに焦点を当てていますが、複数キャラクターの同時に一貫性を維持することや、キャラクター間のインタラクションを含むシーン生成が次のステップです。また、動画生成への直接的な拡張も重要です。現在はキーフレーム生成に止まっていますが、中間フレームの補間を含む完全な動画生成が可能になれば、更なる応用が期待できます。技術的には、ポイントトラッキングの精度向上、特に大きな変形やオクルージョンへの対応が課題です。さらに、他のモダリティ（音声やテキスト）との統合によるマルチモーダル一貫性生成も将来的な研究方向です。