# Visual Autoregressive Modeling for Instruction-Guided Image Editing

## 基本情報
- arXiv ID: 2508.15772v1 (https://arxiv.org/abs/2508.15772)
- 著者: Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei
- 所属: University of Science and Technology of China, HiDream.ai Inc.
- 投稿日: 2025年8月28日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると

VAREditは、指示に基づく画像編集において自己回帰モデルを活用する革新的なフレームワークです。従来の拡散モデルとは異なり、視覚自己回帰（VAR）モデリングを採用して画像編集を「次スケール予測問題」として再定式化します。Scale-Aligned Reference（SAR）モジュールという新しい仕組みを導入し、マルチスケール画像特徴の効果的な条件付けを実現します。標準ベンチマークでGPT-Balance スコアで30%以上の向上を達成し、512×512画像の編集を1.2秒で完了するなど、編集精度と効率の両方で大幅な改善を実現しています。コードとモデルはGitHub（https://github.com/HiDream-ai/VAREdit）で公開されています。

## 1. 研究概要

### 1.1 背景と動機

指示に基づく画像編集は生成AIの重要な応用分野として急速に発展していますが、現在主流の拡散モデルには根本的な制約があります。拡散モデルの全体的なノイズ除去プロセスは、編集対象領域と画像全体のコンテキストを本質的に絡み合わせるため、意図しない偽の修正や編集指示への不適切な対応を引き起こしやすくなっています。

さらに、拡散モデルの多段階ノイズ除去プロセスは大きな計算コストを伴い、リアルタイムでの実用的な使用を制限しています。これらの問題は、編集領域と未編集領域の適切な分離、および効率的な生成プロセスの実現という、画像編集における2つの核心的な課題を浮き彫りにしています。

一方、自己回帰（AR）モデルは根本的に異なるパラダイムを提供します。全体的な反復的な改良ではなく、トークンごとの順次生成により画像を合成する因果的かつ構成的な生成プロセスは、画像編集に自然に適合します。具体的には、未変更領域の保持と編集領域の精密な修正のための柔軟なメカニズムを提供し、拡散モデルの絡み合い問題に対処できる可能性があります。

### 1.2 主要な貢献

本研究の主要な貢献は以下の通りです。

- 指示に基づく画像編集のための初の調整ベース視覚自己回帰（VAR）フレームワークVAREditの提案
- 画像編集を次スケール予測問題として再定式化し、マルチスケール対象特徴を生成することで精密な編集を実現
- スケールミスマッチ問題の発見とScale-Aligned Reference（SAR）モジュールによる効果的な解決策の提案
- 最初の自己注意層におけるスケール対応条件付けの集中という重要な洞察の発見
- 標準ベンチマークにおいて編集遵守性と生成効率の両方で新しい技術水準を確立
- GPT-Balance スコアで主要拡散モデルを30%以上上回る性能を実現
- 512×512画像編集を1.2秒で完了し、同様サイズのUltraEditより2.2倍高速化を達成

## 2. 提案手法

### 2.1 手法の概要

VAREditは視覚自己回帰（VAR）モデリングパラダイムに基づいて構築されており、画像編集を条件付きマルチスケール生成タスクとして再定式化します。従来の拡散モデルが画像全体に対して同時に操作するのに対し、VAREditはスケールごとの順次予測を通じて編集を実行します。

フレームワークの核心は、ソース画像とテキスト指示を条件として、対象残差マップを自己回帰的に生成することです。このプロセスは、マルチスケール視覚トークナイザーとTransformerベースの生成モデルで構成されています。エンコーダーが画像を連続特徴表現にマッピングし、量子化器がこれをK個の離散残差マップの階層に分解します。

重要な設計課題は、マルチスケール生成プロセスを効果的かつ効率的にガイドするためのソース画像の組み込み方法です。完全なマルチスケール特徴での条件付けは計算コストが高く、最細スケールのみでの条件付けはスケールミスマッチを引き起こします。

### 2.2 技術的詳細

#### Scale-Aligned Reference（SAR）モジュール

VAREditの最も革新的な要素は、スケールミスマッチ問題を解決するSARモジュールです。研究者らは、最細スケールソース特徴が粗い対象特徴の予測を効果的にガイドできないという問題を発見しました。この問題に対処するため、スケール間依存関係の体系的分析を実施し、重要な洞察を得ました。

モデルのスケール対応条件付けに対する感度は、最初の自己注意層に集中しているという発見です。この洞察に基づき、SARモジュールは最初の自己注意層にのみスケール対応されたソース情報を注入し、他の層では引き続き最細スケール条件を使用します。この設計により、計算効率を維持しながら精密な編集が可能になります。

具体的には、SARモジュールは以下のプロセスで動作します：

1. ソース画像の最細スケール特徴F_K^(src)を抽出
2. 各予測スケールkに対して、対応するスケール解像度にダウンサンプリング
3. スケール対応された特徴を最初の自己注意層のキーと値の計算に使用
4. 残りの層では元の最細スケール特徴を使用

#### 条件付け戦略の比較

研究では3つの条件付け戦略を比較検討しています：

**バニラ完全スケール条件**では、ソース画像の完全なマルチスケール特徴F_1:K^(src)で生成を条件付けします。これは包括的なスケールごとの参照を提供しますが、シーケンス長の倍増により自己注意の計算コストが二次的に増加し、高解像度編集には実用的ではありません。

**最細スケールのみ条件**は計算効率的ですが、高周波詳細が粗い対象特徴の予測を妨げる深刻なスケールミスマッチを引き起こします。

**SAR条件付け**は、両方のアプローチの利点を組み合わせ、最初の自己注意層でのみスケール対応情報を提供することで、効率性と精度のバランスを達成します。

### 2.3 新規性

VAREditの主要な革新点は、画像編集分野への自己回帰モデルの本格的導入と、その特有の課題への対処法にあります。

**パラダイムシフト**: 従来の拡散ベース手法から自己回帰ベースアプローチへの転換は根本的な変化です。拡散モデルの全体的同時処理に対し、VAREditは順次スケール予測を採用し、編集の精密制御を可能にします。

**スケールミスマッチ問題の発見と解決**: 最細スケール特徴が粗いスケール予測に与える悪影響を体系的に分析し、その解決策としてSARモジュールを提案したことは独創的です。

**効率性と精度の両立**: 従来手法では精度向上がしばしば計算コスト増加を伴いましたが、VAREditは両方を同時に改善することを実現しました。

**スケール対応条件付けの洞察**: 最初の自己注意層にモデルの条件付け感度が集中するという発見は、Transformerアーキテクチャの理解を深める重要な知見です。

## 3. 実験結果

### 3.1 実験設定

VAREditは392万の対例からなる大規模データセットで学習されており、SEED-Data-EditとImgEditデータセットから集約されています。評価には、8つの異なる編集タイプをカバーする3,589例のEMU-Editと、10の編集タイプにわたる700例のPIE-Benchという2つの確立されたベンチマークを使用しています。

評価指標として、従来のCLIPベースメトリクスに加えて、GPT-4oを自動判定者として使用する新しい評価プロトコルを採用しています。GPT-Success（編集指示への遵守）、GPT-Overedit（未編集領域の保持）、およびGPT-Balance（両者の調和平均）という3つの主要スコアを0-10スケールで測定します。

実装においては、事前学習済みInfinityモデルから重みを初期化し、ソース画像トークンと対象画像トークンを区別するために2D回転位置埋め込みに位置オフセットΔ=(64,64)を導入しています。2.2Bと8.4Bの2つの異なるモデルサイズを開発し、段階的学習戦略を採用しています。

### 3.2 主要な結果

VAREditは標準ベンチマークにおいて顕著な性能向上を実現しています。EMU-EditベンチマークでGPT-Balance スコア6.773を達成し、最も競合する手法（ICEditの4.785）を大幅に上回りました。PIE-Benchでも同様に優秀な結果を記録し、GPT-Balance スコア7.298で最高性能を示しています。

特に重要な成果は、編集精度と生成効率の両方での改善です。GPT-Successスコアでは、EMU-Editで7.512、PIE-Benchで8.156を達成し、指示遵守において他の手法を大きく上回っています。同時に、512×512画像の編集を1.2秒で完了し、同様サイズのUltraEdit（2.6秒）より2.2倍高速化を実現しています。

CLIPベースの従来指標においても競合する性能を示しており、CLIP-Dir.で0.115（EMU-Edit）、CLIP-Wholeで0.260、CLIP-Editで0.225（PIE-Bench）を記録しています。これらの結果は、VAREditが既存の評価基準すべてにおいて優秀な性能を発揮していることを示しています。

### 3.3 既存手法との比較

主要な拡散ベース手法との比較において、VAREditの優位性は複数の側面で確認されています。InstructPix2Pix、UltraEdit、OmniGen、AnySD、ACE++、ICEditなどの最先端手法と比較して、VAREditは一貫して高い性能を示しています。

計算効率の面では、EditAR（45.5秒）、OmniGen（16.5秒）、ICEdit（8.4秒）、ACE++（5.7秒）と比較して、VAREditの1.2秒という生成時間は圧倒的な高速化を実現しています。この効率性は実用的応用において決定的な優位性を提供します。

編集品質については、GPT-Balance スコアでの30%以上の改善が特に注目されます。これは単なる数値的改善ではなく、実際の編集品質における質的な向上を意味します。未編集領域の保持（GPT-Overedit）と編集指示への遵守（GPT-Success）の両方において、バランスの取れた高性能を実現しています。

注目すべきは、モデルサイズとの関係です。VAREdit-2.2Bは多くの大型モデル（ICEditの17.0B、ACE++の16.9B、UltraEditの7.7B）を上回る性能を示しており、アーキテクチャの効率性を証明しています。

## 4. 実用性評価

### 4.1 実装の容易性

VAREditの実装容易性は非常に高く評価できます。研究チームはGitHub（https://github.com/HiDream-ai/VAREdit）でコードとモデルを公開しており、再現性と実用性を重視した設計となっています。事前学習済みInfinityモデルからの初期化により、既存のVARインフラストラクチャとの互換性が確保されています。

フレームワークの設計は比較的シンプルで、明確なモジュール構造を持っています。SARモジュールは既存のTransformerアーキテクチャへの最小限の変更で実装でき、他のプロジェクトへの統合が容易です。2D回転位置埋め込みへの位置オフセットの追加など、実装上の工夫も理解しやすく再現可能です。

段階的学習戦略（256×256から512×512への解像度向上）は実践的で、計算資源の制約がある環境でも適用可能な手法を提供しています。モデルサイズも2.2Bと8.4Bの選択肢があり、用途に応じた柔軟な運用が可能です。

### 4.2 計算効率

VAREditの計算効率は画像編集分野において画期的な改善を実現しています。512×512画像の編集を1.2秒で完了することは、リアルタイム応用への道を開く重要な成果です。この高速化は、拡散モデルの多段階ノイズ除去プロセスに対する自己回帰アプローチの根本的優位性を示しています。

SARモジュールによる効率的な条件付け戦略も計算効率に大きく貢献しています。完全マルチスケール条件付けと比較して、最初の自己注意層のみでのスケール対応処理により、二次的な計算コスト増加を回避しながら高品質な結果を実現しています。

メモリ効率の面でも、最細スケール特徴を基本とし、必要な層でのみ追加的な特徴を使用する設計により、メモリ使用量の抑制を図っています。これは特に高解像度画像や大規模バッチ処理において重要な利点です。

推論時のclassifier-free guidance強度（η=4）やlogits温度（τ=0.5）などのハイパーパラメータも計算効率を考慮した設定となっており、品質と速度のバランスを適切に調整しています。

### 4.3 応用可能性

VAREditの応用可能性は極めて広範囲にわたっています。1.2秒という高速生成により、リアルタイム画像編集アプリケーション、インタラクティブなクリエイティブツール、ライブ配信での即座の画像加工など、従来不可能だった用途への適用が現実的になります。

編集の精度と制御性の高さは、プロフェッショナルな画像編集ワークフローでの活用を可能にします。オブジェクトレベルの修正（追加、置換、削除）、属性変更（材質、テキスト、姿勢、スタイル、色）、複雑な構成編集など、多様な編集シナリオに対応できることが実証されています。

教育分野では、画像編集の学習ツールとして活用でき、リアルタイムフィードバックによる効果的な学習体験を提供できます。研究分野では、大規模なデータセット生成やドメイン適応のためのデータ拡張において威力を発揮します。

商業的応用では、電子商取引での製品画像の自動編集、ソーシャルメディアでのコンテンツ生成、デジタルマーケティングでの広告素材作成など、幅広い用途が考えられます。モバイルデバイスでの動作可能性も、消費者向けアプリケーションでの普及を促進するでしょう。

## 5. まとめと所感

### 5.1 論文の意義

本研究は画像編集分野における重要なパラダイムシフトを示す画期的な貢献です。拡散モデルが支配的だった指示ベース画像編集分野に、自己回帰モデルという新しい有効なアプローチを提示し、その優位性を実証的に証明しました。

技術的革新の面では、スケールミスマッチ問題の発見とSARモジュールによる解決策は、マルチスケール生成における根本的な理解を深める重要な貢献です。最初の自己注意層にモデルの条件付け感度が集中するという発見は、Transformerアーキテクチャの内部動作に関する新しい洞察を提供し、今後の研究方向に影響を与える可能性があります。

実用的意義として、30%以上のGPT-Balance スコア改善と2.2倍の高速化という定量的成果は、研究から実用への橋渡しにおいて重要なマイルストーンとなります。特に、リアルタイム画像編集の実現は、この分野の応用範囲を大幅に拡張する可能性を秘めています。

### 5.2 今後の展望

VAREditの成功により、自己回帰ベース画像編集の新しい研究領域が開かれました。今後の発展方向として、より高解像度への対応、動画編集への拡張、マルチモーダル条件付けの強化などが期待されます。

技術的な改善の余地として、SARモジュールのさらなる最適化、他の自己回帰アーキテクチャへの適用、計算効率のさらなる向上などが考えられます。特に、モバイルデバイスでの実行を可能にする軽量化は重要な研究課題となるでしょう。

データセットと評価の観点では、より多様で challenging な編集タスクへの対応、ユーザー体験を考慮した新しい評価指標の開発、長期的な編集品質の維持などが重要な課題です。GPT-4oベースの評価プロトコルの導入は評価の質向上に貢献していますが、さらなる客観性と信頼性の向上が求められます。

最終的には、VAREditのような効率的で高精度な画像編集技術が、クリエイティブ産業の民主化、教育での活用、研究開発の加速など、社会的に広範な影響を与えることが期待されます。自己回帰モデルの画像編集分野への本格参入により、この分野の技術的多様性と競争環境が大幅に向上し、最終的にはより良いツールとサービスの提供につながるでしょう。