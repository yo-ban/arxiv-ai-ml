# Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching

## 基本情報
- arXiv ID: 2507.02860v1 (https://arxiv.org/abs/2507.02860)
- 著者: Xin Zhou他9名（Huazhong University of Science and Technology他）
- 所属: Huazhong University of Science and Technology, MEGVII Technology, University of Hong Kong
- 投稿日: 2025年07月03日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると
EasyCacheは、ビデオ生成AIの推論を高速化する新しい技術です。最新のビデオ生成モデル（HunyuanVideoやSoraのようなモデル）は、ノイズから高品質な動画を作るために、数10回も繰り返し計算します。EasyCacheは、この繰り返し処理の中で「変化が少ない部分」を自動的に見つけ、前の計算結果を再利用することで、品質を落とさずに2.1〜3.3倍の高速化を実現します。事前の学習や調整が不要で、既存のモデルにそのまま適用できる点が大きな特徴です。GitHub: https://github.com/H-EmbodVis/EasyCache

## 1. 研究概要
### 1.1 背景と動機
拡散モデル（Diffusion Models）によるビデオ生成技術は革新的な進歩を遂げていますが、実用化には大きな課題があります。例えば、HunyuanVideoで5秒間の720P動画を生成するには、H20 GPUで2時間もの計算が必要です。

この問題の根本原因は、拡散モデルがノイズから清浄なデータを生成するために、デノイジング（ノイズ除去）を数10回も繰り返す必要があることです。従来の高速化手法には、固定間隔でのキャッシュ（例：PAB）や、データセット事前分析に基づく動的キャッシュ（例：TeaCache）がありましたが、どちらも限界がありました。

### 1.2 主要な貢献
本研究の主要な貢献は以下の3点です。
- 拡散モデルのデノイジング過程で、変換率（入出力の変化の比率）が相対的に安定するという重要な現象を発見
- この安定性を利用した軽量なランタイム適応基準に基づく、学習不要なキャッシュ枠組みEasyCacheを提案
- 様々なビデオ生成モデルでの実験により、既存手法を上回る高速化と品質保持を実証

## 2. 提案手法
### 2.1 手法の概要
EasyCacheの核心アイデアは、拡散モデルのデノイジング過程における「変換ベクトル」（入力と出力の差分）の安定性を利用することです。

具体的には、デノイジングの各ステップで、前回の変換ベクトルを再利用できるかをランタイムに判断します。変化が小さいと判断された場合は、キャッシュされた変換ベクトルを使用して出力を近似し、変化が大きい場合はフル計算します。

### 2.2 技術的詳細
**変換率の定義：** ステップ t における相対変換率 k_t を次のように定義します。
```
k_t = ||ν_t - ν_{t-1}|| / ||x_t - x_{t-1}||
```
ここで、x_tは入力、ν_tは出力（予測ノイズ）です。

**変換ベクトルの再利用：** 変換ベクトルΔ_t = ν_t - x_tを定義し、安定期には以下の近似を使用します。
```
ν̂_t = x_t + Δ_i (前回フル計算したステップiの変換ベクトルを使用)
```

**適応的な判断基準：** 局所安定性の指標ε_tと累積偏差E_tを計算し、E_t < τ（閾値）の場合にキャッシュを使用します。

### 2.3 新規性
既存手法との主な違いは以下の点です。
- **オフライン分析不要：** TeaCacheのような事前のデータセット分析や多項式フィッティングが不要
- **ランタイム適応：** モデルの内部動態に基づいてリアルタイムでキャッシュ戦略を決定
- **軽量実装：** 1次の動態情報のみを使用し、計算オーバーヘッドが極めて少ない
- **汎用性：** モデルアーキテクチャの変更や再学習が不要で、様々なモデルに適用可能

## 3. 実験結果
### 3.1 実験設定
- **評価モデル：** Open-Sora 1.2、Wan2.1-1.3B、HunyuanVideoなどの最新ビデオ生成モデル
- **評価指標：** 推論速度（レイテンシ、高速化率）、視覚品質（PSNR、SSIM、LPIPS、VBench）
- **ベースライン：** 直接ステップ削減、固定キャッシュ、PAB、TeaCache、SVGなど
- **実験環境：** NVIDIA A800 GPU、VBenchデフォルトプロンプト使用

### 3.2 主要な結果
**HunyuanVideoでの結果は以下のとおりです。**
- 高速化率：2.21倍（TeaCacheの1.67倍より36%向上）
- PSNR：32.66（TeaCacheの23.85より37%向上）
- SSIM：0.9313（TeaCacheの0.8185より14%向上）
- VBenchスコア：82.01（オリジナル82.20とほぼ同等）

**他モデルでの結果は次のとおりです。**
- Open-Sora 1.2：2.12倍高速化、PSNR 23.95
- Wan2.1-1.3B：2.54倍高速化、PSNR 25.24
- FLUX.1-dev（画像生成）：4.64倍高速化、FID 23.2

### 3.3 既存手法との比較
- **簡易手法との比較：** 50%ステップ削減や固定キャッシュは同程度の高速化を達成しても、品質が40%以上劣化（PSNR値）
- **PABとの比較：** PABはピラミッド型の固定間隔キャッシュで、細部のボケや歪みが発生
- **TeaCacheとの比較：** データセット事前分析に依存するため汎化性能が低く、品質もEasyCacheに劣る
- **SVGとの比較：** スパース性を活用したattention実装では1.40倍高速化に留まるが、EasyCacheとの組み合わせで3.3倍まで高速化可能

## 4. 実用性評価
### 4.1 実装の容易性
EasyCacheはシンプルに実装できます。主要なハイパーパラメータは2つのみです。
- τ（許容閾値）：品質と速度のトレードオフを制御（推奨値：2.5〜10%）
- R（ウォームアップステップ数）：初期の不安定なフェーズをスキップ（推奨値：5〜10）

モデルのアーキテクチャ変更や再学習は一切不要で、推論コードに数行追加するだけで導入できます。

### 4.2 計算効率
EasyCacheのオーバーヘッドは極めて小さく、各ステップでの判断に必要な計算は、L1ノルム計算と簡単な閾値判定のみです。これはフルのDiTフォワードパスと比較して無視できるレベルです。

実際にHunyuanVideoで129フレームの動画を生成する場合の結果を以下に示します。
- オリジナル：1124.30秒
- EasyCache適用後：507.97秒（2.21倍高速）
- SVGとの組み合わせ：約340秒（3.3倍高速）

### 4.3 応用可能性
EasyCacheの応用範囲は非常に幅広いです。
- **様々なモデルへの適用：** DiTベースのビデオ生成モデルだけでなく、画像生成モデル（FLUXなど）にも適用可能
- **他の高速化手法との併用：** 効率的attention実装（SVGなど）と組み合わせてさらなる高速化が可能
- **リアルタイムアプリケーション：** インタラクティブなコンテンツ生成やストリーミングサービスへの統合

## 5. まとめと所感
### 5.1 論文の意義
EasyCacheは、ビデオ生成AIの実用化に向けた重要な技術的ブレークスルーです。特に注目すべき点は以下の3つです。

1. **シンプルさと有効性の両立：** 複雑な事前分析やモデル変更なしで、モデルの内部動態を利用して高速化を実現

2. **実用性の高さ：** 様々なモデルへ即座に適用でき、現実的な高速化（2〜3倍）と品質保持を両立

3. **理論的裏付け：** 変換率の安定性という現象の発見と、その理論的解釈が手法の信頼性を高めている

### 5.2 今後の展望
**改善の余地は以下の点が考えられます。**
- 現在は一定の閾値を使用しているが、コンテンツに応じた適応的な閾値調整が可能
- 変換ベクトルのより高度な予測手法の探索

**将来の応用として、次のような分野が期待されます。**
- 3D生成モデルやマルチモーダルモデルへの拡張
- エッジデバイスでのビデオ生成を可能にする軽量化
- ストリーミングビデオ生成への応用

EasyCacheは、ビデオ生成AIの民主化に向けた重要な一歩であり、今後の発展が大いに期待されます。