# MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real

## 基本情報
- arXiv ID: 2507.02864v1 (https://arxiv.org/abs/2507.02864)
- 著者: Renhao Wang, Haoran Geng, Tingle Li, Feishi Wang, Gopala Anumanchipalli, Philipp Wu, Trevor Darrell, Boyi Li, Pieter Abbeel, Jitendra Malik, Alexei A. Efros
- 所属: University of California, Berkeley
- 投稿日: 2025年07月03日
- カテゴリ: cs.AI, cs.LG, cs.RO

## 簡単に説明すると
MultiGenは、ロボットが実世界で複数の感覚（視覚・聴覚など）を統合して動作するための革新的な学習フレームワークです。従来のシミュレーションでは視覚情報の再現は高品質でしたが、音などの他の感覚情報の再現は困難でした。この研究では、大規模な生成モデル（特に動画から音声を生成するMMAudioモデル）を物理シミュレータと組み合わせることで、リアルな音声を含むマルチモーダルなシミュレーション環境を構築しました。

具体的な応用例として、ロボットによる液体の注ぎタスクを扱っています。このタスクは液体の遮蔽や透明性により視覚情報だけでは困難で、音響フィードバックが重要な役割を果たします。実験の結果、シミュレーションのみで学習したポリシーが、実世界の新しい容器や液体に対してゼロショットで転移可能であることが示されました。

## 1. 研究概要
### 1.1 背景と動機
人間の行動において、マルチモーダル知覚は堅牢で適応的な動作に不可欠です。例えば、物体を掴む際には視覚と触覚を、飲み物を注ぐ際には聴覚的な手がかりを活用します。ロボットが同様の汎化性と適応性を達成するには、複数の感覚入力を効果的に統合します。

しかし、ロボット学習のための大規模なマルチモーダルデータセットの取得は大きな課題です。同期された動画、音声、触覚、行動データには、精密なキャリブレーション、高価なハードウェア、労働集約的なセットアップが必要です。その結果、最近の大規模なロボットデータセット収集の取り組みでも、多様な感覚の組み合わせが欠けているのが現状です。

シミュレーションベースの学習は有望な代替手段ですが、非視覚的モダリティのシミュレーションの困難さが課題となっています。例えば、音は複雑な波動力学と材料相互作用に依存するため、高忠実度の音響シミュレーションは計算コストが高く、大規模には実用的ではありません。

### 1.2 主要な貢献
本研究では、以下のような貢献をしました。
- 従来の物理ベースシミュレータに大規模な事前学習済み生成モデルを統合する新しいフレームワーク「MultiGen」の提案
- 視覚情報から現実的な音声を生成することで、マルチモーダルなシミュレーション環境を実現
- ロボットの液体注ぎタスクにおいて、シミュレーションのみで学習したポリシーの実世界へのゼロショット転移を実証
- 生成された音声の品質と、それがポリシー学習に与える影響の定量的評価。

## 2. 提案手法
### 2.1 手法の概要
MultiGenフレームワークは2つの主要コンポーネントから構成されています。
第一に、物理ベースのシミュレーションエンジンがロボット学習のための制御可能な環境を提供します。
このシミュレータは、視覚的なシーン、剛体ダイナミクス、流体相互作用、ロボットキネマティクスなどをモデル化します。
これにより、視覚的および体性感覚入力の収集が可能になります。

第二に、生成モデルがシミュレータ情報に基づいて条件付けを行い、従来シミュレーションが困難だった音声などの補完的な感覚信号を生成します。
これら2つのコンポーネントはハイブリッドパイプラインで相互作用します。物理エンジンが構造化された入力を提供し、生成モデルがそれを使用して追加の感覚信号を合成します。

### 2.2 技術的詳細
音声生成には、動画から音声への生成モデルであるMMAudioを採用しています。MMAudioは、マルチモーダルトランスフォーマーアーキテクチャを使用し、視覚、音声、テキストの特徴をクロスアテンションメカニズムで共同処理します。

しかし、MMAudioのゼロショット性能は不十分であったため、次のような改良を実施しました。
1. EPIC-Kitchensデータセットと独自に収集したYouTube動画（合計1031本）を使用して、実世界の注ぎ音に特化したファインチューニングを実施
2. シミュレーション動画と実世界動画の視覚的ドメインギャップに対処するため、SAMv2を使用したセマンティックセグメンテーション条件付けメカニズムを導入。

シミュレーション環境には、高忠実度レンダリングと高度な並列実行をサポートするRoboVerseプラットフォームを使用しました。構造化ドメインランダム化（DR）を適用し、液体特性、カメラポーズ、照明条件、背景テクスチャなどのパラメータを変化させることで、汎化性能を向上させています。

### 2.3 新規性
既存手法との主な違いとして、以下の点が挙げられます。
- 生成モデルを使用してシミュレーションに新しい感覚モダリティを導入する初めての試み
- 高品質な物理シミュレーションと大規模生成モデルの組み合わせによる、マルチモーダルデータ生成の計算効率向上
- セマンティックセグメンテーションを活用した、シミュレーションから実世界への視覚的ドメインギャップの橋渡し手法

## 3. 実験結果
### 3.1 実験設定
実験はKinova Gen3ロボットとRobotiq 2F-85アダプティブグリッパーを使用して実施されました。視覚入力は単眼RGBカメラ、音声入力はエンドエフェクタに取り付けられた全方向性USBマイクから取得しました。

評価ベンチマークは、容器材質、液体タイプ、遮蔽レベルの多様な注ぎ条件を網羅しています。
具体的には、プラスチック、紙、金属の容器と、水、ジュース、炭酸飲料、熱い液体を使用しました。
各条件で3つのランダムな開始位置ペアを選択し、4つの言語指示それぞれで評価しました。

評価指標には、NMAE（Normalized Mean Absolute Error）を使用しました。
NMAE = |実際の注ぎ量 - 目標注ぎ量| / 目標注ぎ量。

### 3.2 主要な結果
MultiGenで学習したポリシーは、実世界の注ぎタスクにおいて高い成功率を達成しました（平均NMAE: 0.46）。特に重要な発見として、視覚+音声のマルチモーダルポリシーは、視覚のみのベースラインと比較してNMAEを平均23.3%削減しました。

不透明な容器を使用する設定では、音声の効果がより顕著で、平均29.4%のNMAE削減を達成しました（透明容器では16.2%の削減）。これは、視覚的フィードバックが制限される状況で、音声が流量と容器の充填レベルを推定する上で重要な役割を果たすことを示しています。

### 3.3 既存手法との比較
従来のデータ拡張手法（ManiWAVプロトコル）と比較して、MultiGenは次のような優位性を示しました。
- より多様な音声サンプルの生成（より高いLog-Spectral Distance）
- より正確な音声の生成（より高いSignal-to-Distortion Ratio）
- データセットサイズの増加に対するより良いスケーリング特性。

これらの結果は、MultiGenがより現実的でタスクに関連した音声を生成し、それが直接的にマルチモーダルポリシー学習の改善につながることを確認しています。

## 4. 実用性評価
### 4.1 実装の容易性
MultiGenフレームワークは、既存の物理シミュレータと事前学習済み生成モデルを組み合わせることで実装できます。
主な実装要件は以下の通りです。
- 高品質な物理シミュレーション環境（本研究ではRoboVerse）
- 動画から音声への生成モデル（MMAudio）とそのファインチューニング用データ
- セマンティックセグメンテーションモデル（SAMv2）。

実装の課題としては、実世界条件をシミュレーションに投影する際の初期セットアップがあります。
ただし、厳密なキャリブレーションは不要です。

### 4.2 計算効率
MultiGenは、計算コストの高い物理ベースの音響シミュレーションを回避します。
代わりに生成モデルを使用することで、従来手法と比較して約3倍の計算速度を実現しています。
並列化されたシミュレーションパイプラインにより、大規模なマルチモーダルデータセットを作成できます。

具体的には、2つのスレッドで並列処理を行います。
第1スレッドが動作計画された注ぎ軌道を実行してRGBフレームを記録します。
第2スレッドが完了した軌道を処理して音声を生成します。

### 4.3 応用可能性
MultiGenのアプローチは、注ぎタスク以外にも応用可能です。
具体的な応用例を以下に示します。
- 接触の多いタスクで重い遮蔽と明確な音響信号がある場合（物体の探索・取り出しなど）
- 触覚フィードバックが重要な精密操作タスク
- 環境音が重要な屋外ロボティクスタスク。

また、このフレームワークは音声以外のモダリティ（触覚、温度など）にも拡張可能です。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、生成モデルをロボット学習のシミュレーションに統合する新しいパラダイムを提示しています。従来のシミュレーションでは困難だった感覚モダリティを、大規模生成モデルを活用して現実的に再現することで、マルチモーダルなsim-to-real転移を実現しました。

特に重要な貢献は、実世界のロボットデータを一切使用せずに、シミュレーションのみで学習したポリシーがゼロショットで実世界に転移できることを実証した点です。これにより、実世界データ収集のコストを約90%削減できる可能性を示しています。

視覚情報が制限される状況では、音声フィードバックが特に有効であることを定量的に示しました。
この知見は、マルチモーダル学習の重要性を裏付けています。

### 5.2 今後の展望
論文で指摘されている制限事項と今後の改善点は以下の通りです。
- 実世界条件をシミュレーションに投影する際の手動セットアップの自動化
- 極端な容器形状への汎化性能の向上
- 雑然とした環境での衝突回避機能の統合。

将来的な研究の方向性として、他の感覚モダリティ（触覚、温度、匂いなど）への拡張、より複雑なマルチモーダルタスクへの適用、生成モデルのさらなる改良による忠実度の向上などが考えられます。また、このアプローチは、ロボティクス以外の分野（VR/AR、ゲーム開発など）にも応用可能な技術として期待されます。