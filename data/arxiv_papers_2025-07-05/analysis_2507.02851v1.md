# MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs

## 基本情報
- arXiv ID: 2507.02851v1 (https://arxiv.org/abs/2507.02851)
- 著者: Purbesh Mitra, Sennur Ulukus (University of Maryland) 
- 所属: University of Maryland
- 投稿日: 2025年07月03日
- カテゴリ: cs.LG, cs.AI, cs.CL

## 簡単に説明すると
MOTIFは、大規模言語モデル（LLM）が長い推論をする際の新しい学習方法です。従来のLLMは一度に長い思考をしますが、コンテキストサイズの制限により限界があります。MOTIFは、推論を複数のラウンドに分けて「モジュール式に考える」ことを可能にします。各ラウンドで部分的な進捗を生成し、次のラウンドではその進捗を基に更なる推論をします。Qwen2.5-3Bモデルで実験した結果、数学問題の解答精度がMATH500で3.8%、AIME2024で3.3%向上しました。コードとモデルは以下で公開されています。
- GitHub: https://github.com/purbeshmitra/MOTIF
- Hugging Face: https://huggingface.co/purbeshmitra/MOTIF

## 1. 研究概要
### 1.1 背景と動機
最近のLLM研究において、推論時により多くのトークンを使用することで、より良い回答を得られることが示されています。DeepSeek-R1やOpenAI o1などのモデルは、長い中間的な思考過程を生成することで高い推論精度を達成しています。

しかし、LLMのコンテキストサイズには限界があります。例えば、注意機構は有限数のトークンに対してのみ計算可能であり、トークン数が増えるにつれて情報伝播の質が低下します。この制限を克服するため、複数ラウンドでの推論が必要となります。

既存の多ラウンド推論手法の多くは、プロセス監督や蒸留データが必要でした。本研究は、純粋に結果ベースの報酬関数を用いた強化学習による多ラウンド推論の実現を目指しています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の4点です。
- 強化学習による多ラウンド推論学習手法MOTIFの提案
- 結果ベースの報酬関数のみを使用し、プロセス監督が不要
- MATH500とAIME2024ベンチマークで、それぞれ3.8%と3.3%の精度向上を実証
- 従来手法の15%のサンプル数で性能向上を達成し、高いサンプル効率を実現

## 2. 提案手法
### 2.1 手法の概要
MOTIFは、LLMが複数のラウンドにわたって推論するための強化学習フレームワークです。基本的なアイデアは、長い推論を3つのラウンドに分割し、各ラウンドで思考プロセスと進捗を生成することです。

具体的な流れは以下のとおりです。
1. 第1ラウンド：問題に対する初期的な思考と部分的な進捗を生成
2. 第2ラウンド：前回の進捗を基に、さらなる思考と進捗を生成
3. 第3ラウンド：これまでの進捗を統合し、最終的な答えを生成

各ラウンドでは、思考プロセスを`<reasoning>`タグで、進捗・答えを`<answer>`タグで囲みます。これにより、モデルは段階的に問題を解決していきます。

### 2.2 技術的詳細
**強化学習の定式化**
MOTIFはGRPO（Group Relative Policy Optimization）アルゴリズムを使用します。報酬関数は以下の2つの要素から構成されます。

1. 精度報酬（r^a_i）：第1ラウンドの応答から最終ラウンドまで推論を進めた場合の正答確率の推定値
   - 各第1ラウンド応答に対して k 個の軌跡を生成
   - 正答率を計算：r^a_i = (1/k)Σ(最終答えが正解と一致)。

2. フォーマット報酬（r^f_i）：応答が正しいタグ形式に従っているかを評価
   - 正しい形式の場合は1、そうでない場合は0

総報酬：r_i = r^a_i + r^f_i。

**アルゴリズムの流れ**
1. 温度0.8で多様な第1ラウンド応答を生成（m 個）
2. 各応答に対して k 個の完全な推論軌跡を生成
3. 報酬を計算し、GRPOステップで方策を更新
4. プロセスレベルの監督なしに、結果ベースの学習を実現

### 2.3 新規性
MOTIFの新規性は以下の点にあります。

1. **純粋な結果ベース報酬**：従来の多ラウンド推論手法が必要とするプロセス監督や蒸留データを使用せず、最終的な正答率のみで学習

2. **シンプルなルールベース報酬**：複雑な段階的な報酬設計ではなく、第1ラウンドの出力に対する将来の精度推定という直感的な報酬設計

3. **サンプル効率性**：従来のGRPO学習に比べて15%のサンプル数で同等以上の性能を達成

4. **汎用性**：特別なデータセット準備や複雑な前処理なしに、既存のGRPOパイプラインに組み込み可能

## 3. 実験結果
### 3.1 実験設定
**データセット**
- 学習データ：GSM8K（小学校レベルの数学問題8,000問）
- 評価データ：MATH500（高校レベルの数学問題500問）、AIME2024（米国数学オリンピック予選問題30問）

**モデルと学習設定**
- ベースモデル：Qwen2.5-3B-Instruct
- パラメータ効率的学習：LoRA（rank=64、全パラメータの約4%を更新）
- 学習サンプル数：MOTIFは300サンプル（GRPOの2000サンプルの15%）
- GPU：NVIDIA A100 40GB（1枚）

**評価指標**
- Pass@1精度：最初の試行で正解を得る確率
- 学習時間：同じwall-clock時間での比較。

### 3.2 主要な結果
**MATH500ベンチマークでの結果**
- ベースモデル（Qwen2.5-3B）：37.6%
- GRPO学習後：44.8%
- MOTIF学習後：48.6%（3.8%改善）

**AIME2024ベンチマークでの結果**
- ベースモデル：0.0%
- GRPO学習後：3.33%
- MOTIF学習後：6.67%（3.3%改善）

**学習効率に関する観察**
- 期待報酬は学習初期から滑らかに上昇
- 平均応答長は学習を通じて若干減少（各ラウンドが効率的にモジュール化されている証拠）
- 従来のGRPOと比較して、より少ないステップで報酬が85%以上向上。

### 3.3 既存手法との比較
**サンプル効率性**
MOTIFは300サンプル（GSM8Kの約4%）で学習したにも関わらず、2000サンプルで学習したGRPOを上回る性能を達成しました。これは、MOTIFの報酬設計が効率的に良い推論戦略を学習できることを示しています。

**他の多ラウンド推論手法との違い**
- INFTYTHINK：蒸留データセットが必要、MOTIFは不要
- SWIRL：プロセスレベルの報酬モデルが必要、MOTIFは結果ベースのみ
- 既存手法の多くは特殊なデータ準備や複雑な報酬設計を必要とするが、MOTIFはシンプルで汎用的。

## 4. 実用性評価
### 4.1 実装の容易性
MOTIFの実装は非常にシンプルです。
- 既存のGRPOパイプラインへの変更は最小限
- 特別なデータセット準備は不要
- システムプロンプトを変更するだけで多ラウンド推論が可能
- コードはGitHubで公開されており、再現が容易。

### 4.2 計算効率
推論時の計算コストは増加しますが、学習時の効率性が6倍以上向上しています。
- 学習サンプル数：従来の15%で同等以上の性能
- 推論時：3ラウンドの推論により、単一ラウンドの約3倍の計算量
- トレードオフ：推論コストの増加を学習効率の大幅な改善が相殺。

### 4.3 応用可能性
MOTIFは様々な複雑な推論タスクに適用可能です。
- 数学問題解決（本研究で実証）
- プログラミング課題
- 科学的推論
- 複雑な計画立案タスク
- 任意のコンテキストサイズ制限を超える推論が必要なタスク。

## 5. まとめと所感
### 5.1 論文の意義
MOTIFは、LLMの推論能力向上における重要な技術的進歩を示しています。特に注目すべき点は以下の3つです。

1. **実用性の高さ**：複雑な前処理やデータ準備なしに、既存モデルの推論能力を向上できる

2. **効率性**：わずか15%のサンプル数で従来手法を上回る性能を実現し、実用的なコスト削減を達成

3. **理論的シンプルさ**：結果ベースの報酬のみで多ラウンド推論を学習できることを実証

この研究は、コンテキストサイズの制限という根本的な問題に対する実用的な解決策を提供しており、今後のLLM研究に大きな影響を与える可能性があります。

### 5.2 今後の展望
**改善の余地がある点**
- 3ラウンド固定ではなく、問題の複雑さに応じた適応的なラウンド数の決定
- より大規模なモデルでの検証（現在は3Bモデルのみ）
- 他のドメイン（コーディング、科学推論など）での性能評価

**将来の発展可能性**
- より長いコンテキストが必要なタスク（論文執筆、大規模コード生成など）への応用
- 人間のフィードバックを組み込んだ対話的な多ラウンド推論
- 複数のエージェント間での協調的な多ラウンド推論への拡張

MOTIFは、LLMの推論能力を実用的に向上させる重要な一歩であり、今後のさらなる発展が期待されます。