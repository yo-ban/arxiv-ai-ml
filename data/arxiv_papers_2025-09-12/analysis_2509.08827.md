# A Survey of Reinforcement Learning for Large Reasoning Models

## 基本情報
- arXiv ID: 2509.08827 (https://arxiv.org/abs/2509.08827)
- 著者名: Kaiyan Zhang, Yuxin Zuo, Bingxiang Heらを筆頭とし、
  Tsinghua University、Shanghai AI Laboratory、
  Shanghai Jiao Tong Universityなど複数機関から40名を超える研究者が参画
- 所属機関: Tsinghua University、Shanghai AI Laboratory、
  Shanghai Jiao Tong University、Peking University、USTC等
- 投稿日: 2025年09月11日
- カテゴリ: cs.AI、cs.LG

## 簡単に説明すると
この論文は、大規模推論モデル（Large Reasoning Models; LRMs）のための強化学習（RL）に関する包括的な調査研究です。
特にOpenAI o1やDeepSeek-R1などの最新モデルの登場を受けて、従来の人間アライメント目的のRLHF（人間フィードバックからの強化学習）から、
推論能力向上を目的としたRLVR（検証可能な報酬による強化学習）への paradigm shift を詳細に分析しています。
この調査では、RL手法の基礎的コンポーネント（報酬設計、方策最適化、サンプリング戦略）から実用的な応用領域まで、
AGI（汎用人工知能）やASI（人工超知能）に向けた発展の道筋を体系的に整理し、今後の研究方向性を示しています。

## 1. 研究概要
### 1.1 背景と動機
この調査研究は、大規模言語モデル（LLMs）における強化学習（RL）の応用が人間アライメントから
推論能力強化へと変化している重要な転換点を背景としています。
OpenAI o1やDeepSeek-R1といったモデルの登場により、RLが複雑な論理タスクにおいて
従来の教師あり学習を大幅に上回る性能を示すことが実証されました。

従来のRLHF（人間フィードバックからの強化学習）やDPO（Direct Preference Optimization）が
主に人間の価値観との整合性を目的としていたのに対し、新しいパラダイムである
RLVR（検証可能な報酬による強化学習）は推論能力そのものの向上を重視しています。
特に数学やプログラミングのような検証可能なタスクにおいて、
自動化された報酬システムが人間のアノテーション依存を解消し、
大規模な学習データ生成を可能にしています。

さらに、テスト時計算スケーリングという新しい軸が発見され、
パラメータ数に加えて推論時の計算量を増加させることで
モデルの能力を向上させることが可能となりました。

### 1.2 主要な貢献
この調査論文は、急速に発展するRL for LRMs分野について体系的な整理と分析を提供しています。
主な貢献は以下の5つの観点から構成されています。

第一に、基礎的コンポーネントの包括的分類体系を構築しました。
報酬設計、方策最適化、サンプリング戦略の3つの次元で既存研究を整理し、
それぞれについて詳細な技術的分析を提供しています。

第二に、この分野における5つの根本的な議論点を特定し分析しています。
これには「シャープ化対発見」「RL対SFT」「プロセス報酬対結果報酬」などの
理論的・実用的な重要な問題が含まれます。

第三に、訓練リソースの総合的カタログを作成し、44の静的データセットと
27の動的環境について詳細な比較分析を行っています。

第四に、6つの主要応用領域（コーディング、エージェント、マルチモーダル、
マルチエージェント、ロボティクス、医療）における進展を調査し、
各分野での特有の課題と機会を明確にしています。

第五に、人工超知能（ASI）に向けた9つの重要な研究方向を特定し、
継続学習、効率的推論、メモリベースRL、潜在空間推論などの
将来的な発展の道筋を示しています。

## 2. 提案手法
### 2.1 手法の概要
この論文は調査研究であるため、単一の提案手法を持ちませんが、
分野全体を理解するための体系的な分類框組を提案しています。

中核的な框組は、RL for LRMsを3つの基礎的コンポーネントに分解して理解することです。
まず報酬設計では、検証可能報酬、生成的報酬、密集報酬、教師なし報酬、
報酬シェーピングの5次元分類を提案しています。

方策最適化では、批評家ベース（critic-based）、批評家フリー（critic-free）、
オフポリシー手法の3つのカテゴリに分類し、特にGRPO（Group Relative Policy Optimization）
系列の手法が最も有効であることを示しています。

サンプリング戦略では、動的サンプリング（効率重視対探索重視）と
構造化サンプリング（木構造対共有プレフィックス）の二重焦点框組を採用しています。

### 2.2 技術的詳細
RLによるLRMs訓練の数学的定式化は以下のように表現されます。
方策は π_θ(y|x) として表現され、ここでyは生成トークン系列 (a₁,...,aₜ) を表します。
状態は s_t = (x, a₁:t₋₁) として定義され、プロンプトxと生成されたプレフィックスを含みます。
報酬は通常、検証可能タスクでは系列レベルのR(x,y)として設定されます。

特に重要なのは、最も影響力のあるGRPOアルゴリズムの定式化です。
利点関数は Â_i = (R(x,y_i) - mean(R)) / std(R) として定義され、
目的関数は min(w_i(θ)Â_i, clip(w_i(θ))Â_i) として最適化されます。
ここでw_i(θ)は重要度重みを表します。

理論的基盤として「Verifier's Law」が提唱されており、
これはタスクの訓練可能性が検証の容易さに比例するという原理です。
また、RLが逆KL発散（モード探索）を最適化するのに対し、
教師あり学習は順KL発散（モード被覆）を最適化するという
根本的な違いが強調されています。

### 2.3 新規性
この調査で特定された技術的新規性は複数の次元で展開されています。

概念レベルでは、RLVR（検証可能な報酬によるRL）という新しいパラダイムの確立、
Verifier's Lawによる理論的枠組の提供、
テスト時計算スケーリングという新しい能力向上軸の発見が挙げられます。

手法レベルでは、生成的報酬モデル（GenRMs）による主観的タスクへの対応、
構造ベース報酬シェーピングによるグループベースライン手法、
動的サンプリングでの難易度認識予算配分などの革新が見られます。

実装レベルでは、ルールベースと学習ベース検証の統合、
方策と報酬モデルの共進化アプローチ、
KVキャッシュ再利用を可能にする共有プレフィックス付き木構造ロールアウト、
多目的最適化による推論効率化などの技術的進歩があります。

## 3. 実験結果
### 3.1 実験設定
この調査では、分野全体の実験的知見を包括的に分析しています。
分析対象となったデータセットは44の静的コーパスと27の動的環境に及びます。

静的データセットは数学（14データセット、800サンプル～550万サンプル）、
コーディング（10データセット、競技プログラミングからソフトウェア工学まで）、
STEM（6データセット、5千～225万サンプル）、
エージェントタスク（7データセット、検索・ツール使用）、
混合ドメイン（6データセット、複数推論タイプ組み合わせ）に分類されています。

動的環境はルールベース（8環境）、コードベース（9環境）、
ゲームベース（10環境）、モデルベース（6環境）に分類され、
それぞれ異なる推論能力を評価します。

評価指標としては、Pass@1（単発成功率）、Pass@K（K回試行成功率）、
CoT-Pass@K（思考連鎖対応評価）、ドメイン固有指標（MATH、HumanEval、AIME）
などが使用されています。

### 3.2 主要な結果
20以上のフロンティアモデルの分析により、いくつかの重要な発見が得られています。

アルゴリズム有効性では、GRPOが検証可能タスクにおいて
批評家ベースPPOを一貫して上回る性能を示しました。
批評家フリー手法がより良いスケーラビリティと安定性を示し、
動的サンプリングが最小限の性能低下で2-2.4倍の効率向上を実現しています。

モデル事前条件効果では、基底モデルが指示調整モデルよりも
RLに対する応答性が高いことが判明しました。
Qwenモデル系列がLlama/OLMoより高いRL応答性を示し、
数学・コード中間訓練がパフォーマンスギャップを橋渡しする効果があります。

報酬設計の影響では、検証可能報酬が報酬ハッキング無しに
信頼性のあるスケーリングを可能にし、
プロセス報酬が結果のみ報酬より複雑な推論において優位性を示し、
ハイブリッド報酬方式（ルール+モデル）が最適なバランスを提供することが確認されました。

### 3.3 既存手法との比較
汎化結果の分析では、RLが教師あり学習に対して
分布外性能において優位性を示すことが確認されています。
長い思考連鎖推論がドメイン間で転移し、
DeepSeek-R1-Zeroのような純粋RL訓練から「アハ瞬間」が創発することが観察されています。

アブレーション研究の総合では、コンポーネント重要度が
報酬設計 > 方策アルゴリズム > サンプリング戦略の順であることが判明しました。
グループベースライン利点推定が安定性において重要であり、
動的温度スケジューリングがエントロピー崩壊を防ぎ、
段階的文脈長延長がトークン効率を改善することが確認されています。

スケーリング行動では、RL訓練ステップと性能が滑らかに改善し、
テスト時計算スケーリングが追加の能力軸を提供し、
データ混合が逐次マルチタスク訓練を上回ることが示されています。

## 4. 実用性評価
### 4.1 実装の容易性
RL for LRMsの実装容易性は、技術的成熟度の向上により大幅に改善されています。

技術的要件として、100-1000 GPU時間が競争力のあるモデル訓練に必要ですが、
これは従来のLLM訓練に比べて追加的な計算コストとなります。
高品質な検証済みデータセット（10万-100万サンプル）と
分散訓練システム、効率的サンプリングを支える基盤設備が必要です。

組織的準備度は機関により大きく異なり、既存LLM基盤設備を持つ
技術企業では高い実装可能性を示す一方、
クラスターアクセスを持つ学術機関では中程度の実現可能性があります。
個人研究者については小規模実験に限定されるのが現状です。

アルゴリズム収束として、GRPO系列の安定したアルゴリズムとベストプラクティスへの収束により、
技術的不確実性が大幅に削減されています。

### 4.2 計算効率
計算効率の分析では、RLによる複数の計算オーバーヘッドが確認されています。

訓練コストとして、RL訓練は同等の教師あり学習の2-5倍の計算量を要求します。
これは訓練ステップごとの複数ロールアウト生成、
グループベース利点推定オーバーヘッド、
思考連鎖推論のための拡張文脈長が原因です。

推論コストでは、テスト時計算スケーリングにより推論コストが10-100倍増加しますが、
重要な能力向上を提供します。

スケーラビリティ評価では、批評家フリーアルゴリズムが批評家ベースより良好なスケーリングを示し、
検証可能報酬が人間アノテーションスケーリングボトルネックを解消し、
動的サンプリングがスケール向上と共に効率改善を実現することが確認されています。

一方で、長文脈RL訓練は依然として高コストであり、
マルチドメインデータ混合には慎重なバランシングが必要で、
モデルスケール増大と共に基盤設備複雑性が増加するという課題があります。

### 4.3 応用可能性
応用可能性の評価では、複数の高インパクト領域での実用性が確認されています。

即座に価値を提供する応用として、コード生成（競技プログラミング、ソフトウェア工学自動化）、
数学推論（教育支援、定理証明）、エージェントシステム（検索、ツール使用、GUI操作）
が挙げられます。

新興応用領域では、マルチモーダル推論（視覚質問応答、動画解析）、
科学研究（薬物発見、材料科学）、ロボティクス（操作計画、人間-ロボット相互作用）
における可能性が示されています。

転移可能性評価では、ドメイン間転移について数学↔コーディング、
論理推論→STEMで強い転移が、推論→一般指示従行で中程度の転移が、
ドメイン固有技能（医学、法学）→一般推論で弱い転移が確認されています。

アーキテクチャ間転移では、Transformer変種間でのアルゴリズム洞察の高い転移性、
モデル系列間でのハイパーパラメータ再調整の必要性（中程度転移）、
アーキテクチャ固有最適化（MoEルーティング、注意パターン）の低い転移性が
観察されています。

## 5. まとめと所感
### 5.1 論文の意義
この調査論文は、RL for LRMs分野が実験段階から実用技術へと移行したことを示す
重要な学術的貢献を提供しています。

技術的成熟性として、安定したアルゴリズム（GRPO系列）とベストプラクティスへの収束、
主要AI企業による商業的展開の成功、複数研究方向での急速な進展が確認されています。

学術的影響として、自然言語処理での能力向上の新パラダイム確立、
コンピュータビジョンでのマルチモーダル推論モデルへの拡張、
ロボティクスでの具現化推論エージェント方策学習、
機械学習でのオフポリシーRLと報酬設計の進歩が期待されます。

産業的インパクトでは、AI支援プログラミングとコードレビュー、
自動定理証明と仮説生成による科学計算、
個人化チュータリングによる教育、
医療推論と診断支援による健康医療での応用拡大が見込まれています。

### 5.2 今後の展望
この分野の将来性は極めて高く、複数の重要な研究方向が特定されています。

短期的展望（1-2年）では、破滅的忘却無しのマルチタスク学習である継続RL、
問題難易度に基づく適応的計算配分による効率的推論、
タスク間知識転移のためのメモリベースRLが重要になります。

中期的展望（2-5年）では、より良いサンプル効率のための世界モデル統合による
モデルベースRL、より滑らかな最適化のための連続空間推論である潜在空間推論、
拡散ベース言語モデルへの拡張である拡散RLが注目されます。

長期的展望（5年以上）では、事前訓練フェーズへのRL統合、
自律的研究と実験によるRL駆動科学発見、
モデルアーキテクチャとハードウェア整合のRL最適化による
アーキテクチャ共設計が重要な発展方向となります。

この調査が示すように、RL for LRMsは人工超知能への道筋において
中核的役割を果たす技術として確立されており、
計算スケーリングの根本的制約よりもアルゴリズム設計の改善が
主要な挑戦となっています。
