# Yume: An Interactive World Generation Model

## 基本情報
- arXiv ID: 2507.17744v1 (https://arxiv.org/abs/2507.17744)
- 著者: Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang
- 所属: Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute
- 投稿日: 2025年07月25日
- カテゴリ: cs.AI, cs.CV, cs.LG

## 簡単に説明すると
この論文は、画像から始まって、キーボード操作でインタラクティブに探索できる動的な仮想世界を生成する「Yume」というモデルを提案しています。名前の「夢」が示すように、写真1枚から時空を超えて任意の世界に入り込み、その中を自由に歩き回れる体験を提供します。

現在公開されているプレビュー版では、1枚の画像を入力として、キーボードのWASDキーと矢印キーで前後左右への移動や視点の回転を制御しながら、都市街路などの複雑な現実世界シーンをリアルタイムに生成・探索できます。特に都市散策（citywalk）のような複雑な実世界シーンの生成に優れており、従来の合成データやゲーム環境に基づく手法とは一線を画しています。

関連リンクとして、GitHubリポジトリ（https://github.com/stdstu12/YUME）でコードとモデル重みが公開されており、HuggingFace（https://huggingface.co/stdstu123/Yume-I2V-540P）でモデルをダウンロードできます。プロジェクトページ（https://stdstu12.github.io/YUME-Project）ではデモ動画や詳細な説明が確認できます。

## 1. 研究概要
### 1.1 背景と動機
インタラクティブな仮想世界の自動生成は、世界シミュレーション、対話型エンターテイメント、仮想身体化などの分野で急速に進歩しています。ビデオ拡散モデルは高品質で時間的に一貫性のある視覚コンテンツの合成において顕著な能力を示しており、このような洗練されたインタラクティブ世界生成タスクを実現するための有望な手段となっています。

しかし、既存のビデオ拡散手法をインタラクティブで現実的なビデオ生成、特に都市シナリオに適用する際には重大な課題があります。まず、既存のアプローチは主に合成または静的なシナリオに焦点を当てており、現実世界との領域ギャップが汎化性を制限しています。また、これらの手法は主に絶対的なカメラモーションに基づいており、精密な注釈と追加の学習モジュールが必要となり、訓練とアーキテクチャ設計の難易度を高めています。

さらに、都市環境は多様な建築様式、動的なオブジェクト、複雑な詳細によって特徴付けられる独特の複雑さを提示します。既存のアプローチはこのような複雑さへの適応性が限られており、多様なシーン全体で一貫したリアリズムを維持するのに苦労しています。フリッカリング、不自然なテクスチャ、幾何学的歪みなどの一般的な視覚的アーティファクトは、知覚品質を大幅に低下させ、没入型体験を妨げています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の4点です。

- 相対的なカメラ姿勢変化を離散的なキーボードアクションに変換し、テキスト条件に注入する量子化カメラモーション（Quantized Camera Motion: QCM）制御メカニズムを導入。追加の学習可能モジュールを導入せず、フレームごとの絶対姿勢行列に依存する手法と比較して、より安定で正確なカメラ制御を実現
- 理論的に無限の持続時間を持つインタラクティブビデオ生成を実現するため、修正されたFramePackメモリモジュールを備えたチャンクベースの自己回帰生成フレームワークとマスク付きビデオ拡散トランスフォーマー（Masked Video Diffusion Transformers: MVDT）を導入
- 訓練不要のアンチアーティファクト機構（Anti-Artifact Mechanism: AAM）とSDE（確率微分方程式）に基づくタイムトラベルサンプリング（Time Travel Sampling based on SDE: TTS-SDE）を導入し、視覚品質を向上させ、より正確な制御を実現
- 敵対的蒸留とキャッシングメカニズムの共同最適化を調査し、視覚的忠実度や時間的一貫性を損なうことなくサンプリング速度を向上

## 2. 提案手法
### 2.1 手法の概要
Yumeは、入力画像と離散的なキーボード制御を通じて自己回帰的なインタラクティブビデオ生成を行うように設計されています。このシステムは、カメラモーション制御、モデルアーキテクチャ、サンプラー、モデル加速の4つの主要な次元で体系的な最適化を実装しています。

システムの中心となるのは、キーボード入力を通じたより直感的で安定したカメラ制御を提供しながら、複雑なシーン生成の視覚品質とリアリズムを向上させることです。Yumeは月次で更新され、テキスト、画像、またはビデオを通じてインタラクティブで現実的、動的な世界を作成し、周辺デバイスや神経信号を使用して探索・制御できるという本来の目標に向けて進化を続けます。

### 2.2 技術的詳細
Yumeの技術的な核となる4つの主要コンポーネントについて詳しく説明します。

**1. 量子化カメラモーション（QCM）制御**
カメラの軌跡を並進運動（前進、後退、左、右）と回転運動（右回転、左回転、上傾斜、下傾斜）に量子化し、キーボード入力で柔軟に組み合わせて転送できるようにしています。QCMは訓練中の相対的なカメラ姿勢の変化によって生成され、時間的コンテキストと空間的関係を制御信号に自然に埋め込みます。これらはテキスト条件に解析され、事前訓練されたI2V基盤モデルに新しい学習可能モジュールを導入することなく統合されます。

**2. マスク付きビデオ拡散トランスフォーマー（MVDT）**
テキストベースの制御が頻繁に不自然な出力をもたらすという問題に対処するため、MVDTを採用しています。さらに、修正されたFramePackメモリモジュールを備えたチャンクベースの自己回帰生成フレームワークを通じて、理論的に無限の持続時間を持つインタラクティブビデオ生成を実現しています。

**3. 訓練不要アンチアーティファクト機構（AAM）**
各拡散ステップで潜在表現の高周波成分を洗練させることで、複雑な実世界シーンの視覚品質を向上させます。この標的化された洗練により、細かい詳細が改善され、不整合が平滑化され、視覚的アーティファクトが大幅に減少し、追加のモデル訓練や特殊なデータセットを必要とせずに全体的な視覚品質が向上します。

**4. SDE基準タイムトラベルサンプリング（TTS-SDE）**
後のデノイジング段階からの情報を活用して初期のデノイジングプロセスを導き、確率微分方程式を組み込んでサンプリングのランダム性を高めることで、テキストの制御性を向上させる新しいサンプリング手法です。

### 2.3 新規性
既存手法との主な違いは以下の点にあります。

1. **離散的なカメラ制御**: 明示的なカメラ軌跡に依存する既存のアプローチとは異なり、カメラ姿勢空間を離散化することで直感的なキーボードベースの制御を実現
2. **実世界データへの対応**: ゲームベースのシナリオに主に依存する既存手法と異なり、複雑な実世界の都市環境に対応
3. **訓練不要の品質向上**: AAMによって追加訓練なしに視覚品質を大幅に改善
4. **統合的な最適化**: カメラ制御、アーキテクチャ、サンプリング、加速の4つの次元で同時に最適化

## 3. 実験結果
### 3.1 実験設定
実験では、高品質な世界探索データセット「Sekai」を使用してYumeを訓練しました。Sekaiは実世界の都市散策シーンを含む大規模なデータセットで、複雑な都市環境での評価に適しています。

評価指標として以下を使用しています：
- 指示追従性（Instruction Following）
- 被写体一貫性（Subject Consistency）
- 背景一貫性（Background Consistency）
- 動きの滑らかさ（Motion Smoothness）
- 美的品質（Aesthetic Quality）
- 画像品質（Imaging Quality）

訓練では、33-400フレームビデオから後続の33フレームシーケンスを予測する確率が0.3、400-800フレームビデオを履歴コンテキストとして33フレーム予測を行う確率が0.7の確率的サンプリング戦略を実装しています。

### 3.2 主要な結果
主要な実験結果は以下の通りです：

1. **カメラ制御の精度**: 量子化されたカメラ軌跡に従って正確にビデオを生成することができ、ユーザーの意図通りの探索が可能
2. **視覚品質の向上**: AAMの導入により、都市や建築シーンにおける構造的詳細が大幅に改善され、雪だるまのアーティファクトなどの非論理的なシーンを回避
3. **長時間生成の安定性**: チャンクベースの自己回帰アプローチにより、理論的に無限の長さのビデオ生成が可能
4. **リアルタイム性能**: モデル蒸留により、50ステップから14ステップに削減し、生成時間を583.1秒から158.8秒に短縮

### 3.3 既存手法との比較
既存の世界生成手法（Matrix-Game、WORLDMEM等）と比較して、Yumeは以下の点で優位性を示しています：

- **実世界対応**: ゲームデータに依存する既存手法と異なり、実際の都市環境で優れた性能を発揮
- **制御の直感性**: 複雑なカメラパラメータの調整が不要で、キーボード操作で簡単に制御可能
- **視覚品質**: AAMにより、既存手法で見られるアーティファクトやフリッカリングを大幅に削減

蒸留後のモデルでは、指示追従性以外のメトリクスで元のモデルとほぼ同等の性能を維持しながら、生成速度を約3.7倍に向上させています。

## 4. 実用性評価
### 4.1 実装の容易性
Yumeの実装は比較的容易です。GitHubで公開されているコードベースは整理されており、必要な依存関係も明確に文書化されています。特に注目すべきは、訓練済みモデルがHuggingFaceで公開されており、すぐに試すことができる点です。

ただし、高品質な生成には相応の計算リソースが必要です。540P解像度での生成には、現在のところGPUメモリが相当量必要となります。研究チームは将来的により高速で高解像度のモデルを開発する計画を立てています。

### 4.2 計算効率
計算効率の観点から、Yumeは以下の最適化を実現しています。

1. 蒸留による高速化：敵対的蒸留により、品質をほぼ維持しながら生成時間を約1/3.7に短縮
2. キャッシングメカニズム：DiTアーキテクチャのタイムステップ間の特徴類似性を活用し、効率0.9倍のキャッシング戦略を実装
3. NFE削減：AAMの最適化により、標準的な100 NFEから90 NFEに削減しながら品質を向上

現在の制限として、長時間のビデオ生成では初期シーンの詳細を保持することが困難であり、これは実世界のループバックデータの不足が一因とされています。

### 4.3 応用可能性
Yumeの応用範囲は非常に広く、以下のような分野での活用が期待されます。

1. 仮想観光・不動産：実際に訪れることなく場所を探索できる没入型体験の提供
2. ゲーム・エンターテインメント：プロシージャルな世界生成によるインタラクティブコンテンツの作成
3. 教育・訓練：安全な仮想環境での探索的学習やシミュレーション訓練
4. ロボティクス：仮想環境でのナビゲーション訓練やパス計画のテスト
5. 建築・都市計画：設計案の仮想ウォークスルーや環境影響の視覚化

特に、実世界の複雑な都市環境を扱える能力は、従来のゲームベースのアプローチでは困難だった応用を可能にします。

## 5. まとめと所感
### 5.1 論文の意義
Yumeは、インタラクティブな世界生成の分野において重要な一歩を示しています。特に以下の点で革新的です。

1. 実世界への橋渡し：ゲームや合成データに依存していた既存研究と異なり、実際の都市環境での高品質な生成を実現
2. アクセシビリティ：複雑なパラメータ調整なしに、誰でも簡単にキーボードで操作できるインターフェース
3. オープンサイエンス：コード、データ、モデル重みをすべて公開し、コミュニティの発展に貢献
4. 統合的アプローチ：カメラ制御、アーキテクチャ、サンプリング、加速の4つの側面を同時に最適化

論文タイトルの「夢」という言葉が示すように、写真から別の世界へと入り込むという体験は、現実世界での後悔を補償したり、願いを実現したりする可能性を秘めています。この技術は、物理的な制約を超えた体験を提供する新しいメディアの基盤となる可能性があります。

### 5.2 今後の展望
Yumeは長期的なプロジェクトとして位置づけられており、今後の課題と展望として以下が挙げられています。

1. 品質のさらなる向上：現在でも顔のぼやけ、車両の逆走、人物の逆歩行などの問題が存在し、より高解像度での訓練が計画されている
2. 制御精度の改善：現在の制御精度は90%未満であり、データセットの拡張とゲームビデオデータの混合が検討されている
3. 長期記憶の実現：拡張ビデオ生成で初期シーンの詳細を保持する能力の向上
4. 密集したクラウドシーンへの対応：現実的で一貫性のある人間の動きパターンの生成は特に困難
5. オブジェクトとのインタラクション：現在は環境の探索のみだが、将来的にはオブジェクトとの相互作用も可能に

研究チームは月次でモデルを更新する計画を立てており、最終的にはテキスト、画像、ビデオから始まり、周辺デバイスや神経信号で制御できる、真にインタラクティブで現実的、動的な世界の創造を目指しています。この野心的な目標に向けて、Yumeは着実に進化を続けていくことが期待されます。