# Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains

## 基本情報
- arXiv ID: 2507.17746v1 (https://arxiv.org/abs/2507.17746)
- 著者: Anisha Gunjal・Anthony Wang・Elaine Lau・Vaskar Nath・Bing Liu・Sean Hendryx（Scale AI）
- 所属: Scale AI
- 投稿日: 2025年07月25日
- カテゴリ: cs.LG, cs.AI

## 簡単に説明すると
この論文は、検証可能な報酬がない実世界タスクで大規模言語モデル（LLM）を強化学習で改善するための新しい手法「Rubrics as Rewards（RaR）」を提案しています。

従来の強化学習では、数学や
コーディングのように明確な正解がある分野（RLVR: Reinforcement Learning with Verifiable Rewards）で成功を収めている。しかし、医療や科学などの分野では単一の正解が存在せず、評価が主観的および客観的な基準を含むため、信頼できる報酬信号を定義することが困難です。

RaRは、構造化されたチェックリスト形式のルーブリック（評価基準）を報酬信号として使用することで、この問題を解決する。各ルーブリックは独立した評価可能な複数の基準から構成され、人間が解釈可能で自動化可能なフィードバックを提供する。実験の結果、RaRはHealthBench-1kで従来手法より最大28%の改善を達成しました。専門家の参照回答から導出された報酬信号と同等以上の性能を示しました。

## 1. 研究概要
### 1.1 背景と動機
強化学習を用いた大規模言語モデルの改善は、数学やコーディングなど検証可能な正解がある分野で大きな成功を収めている。これらの分野では、最終的な答えが正しいかどうかを容易に検証でき、テストケースによる自動評価メカニズムが存在するため、報酬モデルをスコアリング関数で置き換えることができる。

しかし、多くの実世界のタスクでは明確な検証可能な答えが存在しない。このようなタスクでは、モデルは直接的な報酬フィードバックを得ることができない。実際、研究者はしばしば選好ランキングにRLHF（人間のフィードバックによる強化学習）を使用する。モデル出力のペアやリストに対する人間の判断を収集してこのギャップを埋めようとする。

選好ベースの報酬モデルは性能を向上させることができるが、表面的な特徴（応答の長さ、フォーマットの癖、アノテーターのバイアスなど）に過剰適合する傾向がある。また、大量のペアワイズ比較を必要とするため、選好ベースの報酬モデルは脆弱でコストが高くなる。

### 1.2 主要な貢献
本研究の主要な貢献は以下の4点です。

- 検証可能でない多基準タスクを監督するためのチェックリスト形式のルーブリックを使用するオンポリシー強化学習フレームワーク「Rubrics as Rewards（RaR）」を導入
- 医学と科学の分野にルーブリック生成アプローチを適用し、2つの訓練データセット（RaR-Medicine-20kとRaR-Science-20k）を作成
- RaRで訓練されたモデルが強力なベースラインと同等以上の性能を達成し、多様な分野で精度の顕著な向上を示すことを実証
- ルーブリックを構造化された報酬信号として使用することで、より小規模なジャッジモデルが人間の選好とより良い整合性を達成し、異なるモデルスケールにわたって堅牢な性能を維持できることを示す

## 2. 提案手法
### 2.1 手法の概要
RaRの中心的なアイデアは、プロンプト固有のルーブリック基準を使用して構造化された報酬関数を定義することです。各プロンプトは、複数のルーブリック項目と関連付けられます。各項目は重みと正誤関数を持ち、応答が基準を満たすかを判定します。

最終的な正規化されたスカラー報酬は以下のように計算されます。

r(prompt, response) = Σ(weight × correctness) / Σ(weight)

この正規化により、異なる数と重みのルーブリック項目を持つプロンプト間で報酬値が一貫性を保つことが保証される。

### 2.2 技術的詳細
RaRは2つの集約方法を提供します。具体的には以下の通りです。

1. **明示的ルーブリック集約**：各基準は生成的報酬モデルを使用して独立に評価され、報酬は上記の式を使用して計算される。現在はc_jに2値を仮定しているが、このフレームワークは連続値の判断への拡張をサポートしている。

2. **暗黙的ルーブリック集約**：すべてのルーブリック記述と重みを生成的報酬モデルに渡し、モデル内部でスカラー報酬を計算する。これにより明示的な2値評価を回避し、より豊かで学習された集約が可能になる。

ルーブリック生成では、以下の4つの設計原則に従います。

- **専門家ガイダンスに基づく**：人間の専門家やより強力なLLMによって作成された参照回答を専門家ガイダンスの代理として使用
- **包括的なカバレッジ**：事実の正確性、論理構造、完全性、スタイル、一般的な落とし穴を含む複数の品質次元をカバー
- **意味的重み付け**：各基準に相対的な優先度を反映するカテゴリカルな重要度レベル（Essential、Important、Optional、Pitfallなど）を付与
- **自己・完結型・評価**：各項目が人間のアノテーターまたはLLMベースのジャッジによって独立して評価可能

### 2.3 新規性
従来のRLVR（検証可能な報酬による強化学習）との主な違いは、RaRがRLVRを一般化し、多次元の監督、柔軟な基準間の重み付け、応答品質の客観的および主観的側面の両方の組み込みを可能にすることです。

RLVRは、k=1、w_1=1、c_1(x, ŷ)が既知の正解と比較する単一の検証可能な正誤関数に還元されるルーブリックベースの報酬の特殊なケースと見なすことができます。これに対し、ルーブリックベースの報酬は、正しさが多面的で厳密に検証可能でない可能性がある設定での構造化された監督をさらに可能にします。

## 3. 実験結果
### 3.1 実験設定
実験は医学と科学の2つの推論ドメインで実施されました。

- **RaR-Medical-20k**：医学関連のデータセットから20,000個のプロンプトを収集
- **RaR-Science-20k**：GPQA Diamondベンチマークに合わせた約20,000個の科学プロンプトを収集

訓練はGRPOアルゴリズムを使用し、Qwen2.5-7Bをベースポリシーとして実施。バッチサイズ96、学習率5×10^-6、10%の線形ウォームアップを伴う一定の学習率スケジュールで訓練しました。

### 3.2 主要な結果
HealthBench-1kとGPQA_Diamondベンチマークでの主要な結果は以下の通りです。

1. **RaR-Implicit法の優位性**：両ドメインでSimple-Likertベースラインを一貫して上回り、HealthBench-1kで最大28%の相対的改善、GPQAで13%の改善を達成

2. **Reference-Likertとの比較**：RaR-Implicitは、専門家の参照回答を使用するReference-Likertベースラインと同等以上の性能を示す

3. **暗黙的vs明示的集約**：暗黙的集約が明示的な重み付き合計よりも優れており、LLMベースのジャッジがプロンプトごとにルーブリック基準のバランスを推論する方が、固定の手作り重みよりも効果的

4. **モデルスケール間での堅牢性**：ルーブリックガイド付き評価は、3Bから72Bまでのモデルスケール全体でジャッジの精度を一貫して改善

### 3.3 既存手法との比較
様々なベースラインとの比較実験では以下の結果が得られました。

- **Simple-Likert**：LLMジャッジが各応答-プロンプトペアに対して1-10のLikertスコアを出力（0-1に正規化）
- **Reference-Likert**：ジャッジモデルが生成された応答を高品質の参照回答と比較してLikertスコアを出力
- **Predefined-RaR**：固定された汎用ルーブリックのセット（プロンプト固有ではない）を使用

すべてのルーブリックベースの手法がルーブリックフリーのベースラインを上回り、最も弱いRaRのバリアントでもReference-Likertを約15%上回った。

## 4. 実用性評価
### 4.1 実装の容易性
RaRの実装は比較的シンプルです。主な要件は以下の通りです。

1. ルーブリック生成用の強力なLLM（o3-miniやGPT-4oなど）へのアクセス
2. 報酬計算用のジャッジモデル（gpt-4o-miniなど）
3. オンポリシー強化学習用のGRPOアルゴリズムの実装

ルーブリック生成は一度だけ実行すれば良く、生成されたルーブリックは訓練全体を通して再利用できます。

### 4.2 計算効率
計算効率の観点から、RaRには以下の利点があります。

1. **ジャッジモデルのスケーラビリティ**：小規模なジャッジモデル（gpt-4o-miniなど）でも、ルーブリックガイダンスにより良好な性能を達成
2. **並列化可能な評価**：明示的集約では各ルーブリック基準を独立に評価できるため、並列処理が可能
3. **実行速度が速い訓練**：8つのNVIDIA H100 GPUを搭載した単一の計算ノードで訓練可能

### 4.3 応用可能性
RaRの応用範囲は広く、以下のような分野で有用です。

1. **医療分野**：事実の正確性、安全性、共感性など複数の基準が重要な医療アドバイスの生成
2. **科学分野**：複雑な推論と複数の評価基準を必要とする科学的質問への回答
3. **教育分野**：学習目標に合わせた複数の評価基準を持つ教育コンテンツの生成
4. **顧客サービス**：正確性、有用性、礼儀正しさなど複数の側面を考慮する必要がある応答生成

## 5. まとめと所感
### 5.1 論文の意義
本論文は、検証可能な正解が存在しない実世界のタスクにおいて、強化学習を効果的に適用するための重要な貢献をしています。ルーブリックを報酬信号として使用するというアイデアは、以下の点で革新的です。

1. **解釈可能性**：ブラックボックスな報酬モデルと異なり、人間が理解可能な明示的な基準に基づいて最適化
2. **柔軟性**：客観的および主観的な評価基準の両方を組み込むことが可能
3. **スケーラビリティ**：小規模なジャッジモデルでも効果的に機能し、計算リソースの制約がある環境でも適用可能
4. **堅牢性**：離散的で解釈可能な基準は、不透明な学習されたスコアよりもゲーミングが困難

特に、参照回答へのアクセスが限られている、またはモデルの解釈可能性が重要な場合に、ルーブリックはよりスケーラブルで制御可能なアプローチを提供します。

### 5.2 今後の展望
本研究は重要な一歩ですが、いくつかの拡張の余地があります。

1. **より多様なドメインへの拡張**：現在は医療と科学に限定されているが、オープンエンドな対話などの他のタスクへの一般化可能性の評価
2. **動的重み付けの探索**：訓練の進行に応じてルーブリック項目の重みを動的に調整するカリキュラム学習の導入
3. **報酬ハッキングへの耐性評価**：個々のルーブリックコンポーネントをゲーミングしようとする敵対的訓練シナリオでの評価
4. **専用評価器の開発**：より強力な推論能力を持つ専用のジャッジモデルの探索

RaRは、構造化された評価基準を報酬信号に変換することで、検証可能でないドメインでの強化学習の新しい可能性を開きました。この手法は、より解釈可能で制御可能なAIシステムの開発に向けた重要な一歩となっています。