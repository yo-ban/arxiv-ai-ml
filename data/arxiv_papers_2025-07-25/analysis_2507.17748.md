# Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility

## 基本情報
- **arXiv ID**: 2507.17748v1 (https://arxiv.org/abs/2507.17748)
- **著者**: Melih Barsbey, Lucas Prieto, Stefanos Zafeiriou, Tolga Birdal (Imperial College London)
- **所属**: Imperial College London
- **投稿日**: 2025年07月25日
- **カテゴリ**: cs.LG, cs.AI

## 簡単に説明すると
この論文は、深層学習において大きな学習率（Learning Rate, LR）を使用することで、spurious correlation（偽の相関）に対するロバスト性とモデルの圧縮性を同時に達成できることを実証しています。従来、これらの2つの特性は相反するものと考えられていましたが、本研究では大きな学習率が両方を促進する鍵となることを示しています。実験では、合成データセットから実際の画像分類タスク（CIFAR-10、CIFAR-100、ImageNet-1k）まで幅広く検証し、大きな学習率が本質的な特徴（core features）の学習を促進し、偽の相関に基づく特徴（spurious features）への依存を減らすことを明らかにしています。

## 1. 研究概要
### 1.1 背景と動機
近年の機械学習システムは、新しい環境での動作と資源効率性という2つの重要な課題に直面しています。限られた計算資源、環境への配慮、ハードウェアのボトルネックなどから、より少ないリソースで高性能を達成することが求められています。特に重要な問題として、訓練データに含まれるspurious correlations（SC）の存在があります。これは、訓練セットでは入力特徴と出力ラベルの間に存在するが、テストセットでは成立しない関係性を指します。例えば、砂漠の背景にいる牛をラクダと誤分類してしまうケースが典型例です。

この研究は、過剰パラメータ化がSCを増幅させ、分布外（OOD）性能を低下させるという既存の知見を踏まえ、学習率がSCに対するロバスト性と効率性を同時に達成する上で重要な役割を果たすという仮説を立てています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです：
- 大きな学習率が、様々なアーキテクチャ、データセット、最適化手法において、圧縮性とSCに対するロバスト性を同時に一貫して促進することを実証
- 大きな学習率が、不変特徴の利用、クラス分離、活性化スパース性などの望ましい表現特性を生み出すことを特定
- 大きな学習率が、他の主要なハイパーパラメータや正則化手法と比較して、これらの特性を一貫して満たす独自の組み合わせを生成することを示す
- 標準的な分類タスクにおける大きな学習率の成功が、訓練データセット内の隠れた/稀なSCへの対処効果による可能性が高いという強力な証拠を提供
- メカニズムの調査により、高い学習率下でのbias-conflictingサンプルの確信度の高い誤予測の重要性を明らかに

## 2. 提案手法
### 2.1 手法の概要
本研究では、学習率がSCに対するロバスト性とモデルの圧縮性に与える影響を体系的に調査しています。具体的には、分類タスクにおいて、ニューラルネットワークfが入力xから離散ラベルyを予測する設定を考えます。データ分布μZから独立同分布でサンプリングされた訓練データセットSを用いて、確率的勾配降下法（SGD）により経験的リスク最小化を行います。

SCの設定では、入力特徴xをcore features（xᶜ）とspurious features（xˢ）に分解します。core featuresは訓練セットとテストセットの両方でラベルと一貫した関係を持つのに対し、spurious featuresは訓練セットでのみラベルと相関を持ちます。bias-conflicting例の割合ρᵗʳᵃⁱⁿ = 1 - μᵗʳᵃⁱⁿ_Z(y=b)が、学習器がspurious featuresの利用を避けるために活用できる手がかりとなります。

### 2.2 技術的詳細
圧縮性の評価には、活性化に対してはスパース性と(q,κ)-Compressibilityを使用します。(q,κ)-Compressibilityは、ベクトルをその要素のκ割で近似した場合の誤差を測定します。ネットワークパラメータの圧縮性については、κ-Prunabilityを定義し、パラメータのκ割をプルーニング（ゼロに設定）した後のテスト精度の保持率を測定します。

表現分析のために、以下のメトリクスを使用します：
- Class Separation R²：クラス内距離とクラス間距離の比率に基づく、表現のクラス分離度
- Class-Selectivity Index (CSI)：特定のニューロンのクラスに対する選択性を測定
- Integrated Gradients（IG）：入力ピクセルや特徴がモデルの予測に与える影響を定量化

### 2.3 新規性
既存研究では、大きな学習率が標準的な機械学習タスクにおける汎化性能やモデルの圧縮性、表現のスパース性に与える影響が個別に研究されてきましたが、SCに対するロバスト性との関連や、これらの特性の相互作用については系統的に探究されていませんでした。本研究は、大きな学習率が複数の望ましい特性を同時に実現するメカニズムを明らかにし、理論的な説明を提供している点で新規性があります。

## 3. 実験結果
### 3.1 実験設定
実験では4つのカテゴリのデータセットを使用しています：
1. 合成SCデータ（parity dataset、moon-star dataset）
2. 半合成SCデータ（Colored MNIST、Corrupted CIFAR-10、MNIST-CIFAR、Double MNIST）
3. 自然画像SCデータ（CelebA、Waterbirds）
4. 標準分類データ（CIFAR-10、CIFAR-100、ImageNet-1k）

アーキテクチャとしては、全結合ネットワーク（FCN）、VGG11風のCNN、ResNet18/50、Wide ResNet-101-2、Swin Transformerなど多様なモデルを使用しています。特に明記されない限り、すべてのモデルは100%精度に達するまで一定の学習率で訓練され、L1/L2正則化は使用していません。

### 3.2 主要な結果
実験結果は、大きな学習率が一貫してSCに対するロバスト性とモデルの圧縮性を向上させることを示しています：

1. **ロバスト性と圧縮性の同時達成**: すべてのデータセットとアーキテクチャにおいて、学習率の増加に伴い、unbiased test accuracyとnetwork prunabilityが向上することが確認されました。

2. **表現特性の改善**: 大きな学習率は、core feature utilizationとclass separationを向上させ、activation compressibilityも高めることが示されました。

3. **現実的なシナリオへの一般化**: Swin TransformerをCelebAデータセットで訓練した実験では、ImageNet-1K事前学習とAdamW最適化を使用した現実的な設定でも、大きな学習率が同様の効果を持つことが確認されました。

4. **標準分類タスクでの効果**: CIFAR-10、CIFAR-100、ImageNet-1kでの実験により、大きな学習率が背景やテクスチャなどの偽の相関を避け、物体の輪郭により注目することが示されました。Attribution entropyとbackground attribution percentageの分析により、この効果が定量的に確認されました。

### 3.3 既存手法との比較
他のハイパーパラメータ（バッチサイズ、モメンタム、L1/L2正則化）や、Focal Loss、Adaptive Sharpness Aware Minimization（ASAM）などの手法と比較した結果、大きな学習率は一貫してOOD汎化、ネットワーク圧縮性、core feature utilizationの組み合わせを実現することが示されました。また、L1/L2正則化と組み合わせることで、さらに良いトレードオフを達成できることも確認されました。

## 4. 実用性評価
### 4.1 実装の容易性
大きな学習率の使用は、既存の訓練パイプラインに簡単に組み込むことができます。特別なアーキテクチャの変更や追加の計算は必要なく、単に学習率のハイパーパラメータを調整するだけで実装可能です。ただし、極端に大きな学習率は発散や急激な性能低下（テスト精度で25%以上の低下）を引き起こす可能性があるため、適切な範囲での調整が必要です。

### 4.2 計算効率
提案手法は追加の計算コストを必要としません。むしろ、大きな学習率により収束が速くなる可能性があり、計算効率の面でも有利です。また、結果として得られるモデルの圧縮性が高いため、推論時の計算効率も向上します。実験では、90%のパラメータをプルーニングしても性能がある程度保たれることが示されています。

### 4.3 応用可能性
本研究の知見は、様々な実用的なシナリオに適用可能です：
- データセットに隠れたバイアスが存在する可能性がある実世界のアプリケーション
- モデルサイズの制約がある組み込みシステムやモバイルデバイス
- 分布シフトが予想される運用環境
- 解釈可能性が重要なアプリケーション（大きな学習率により、より明確な特徴利用パターンが得られる）

## 5. まとめと所感
### 5.1 論文の意義
本研究は、深層学習における基本的なハイパーパラメータである学習率が、モデルの性能と効率性に与える影響について新しい視点を提供しています。特に、従来相反すると考えられていたロバスト性と圧縮性を同時に達成できることを示した点は重要な発見です。理論的な分析により、大きな学習率がbias-conflicting samplesの損失を増幅し、spurious featuresへの依存を減らすメカニズムを明らかにしたことも、今後の研究に重要な示唆を与えています。

また、標準的な分類タスクでの学習率の効果が、実は隠れたSCへの対処によるものである可能性を示したことは、既存の機械学習の理解を深める重要な貢献です。これは、一見独立同分布（IID）に見えるタスクでも、実際にはOOD汎化のサブタスクが含まれていることを示唆しています。

### 5.2 今後の展望
著者らも認めているように、本研究は最適化ダイナミクス、パラメータの損失ランドスケープ、表現の関係についての完全な説明には至っていません。今後の研究では、複数の階層的なSCが訓練ダイナミクスとどのように相互作用するかを研究し、より堅牢で効率的なモデルのための明示的な訓練手順を設計することが重要です。

また、本研究の知見を活用して、既存のモデルやデータセットの隠れたバイアスを検出・対処する手法の開発や、より効率的なモデル圧縮手法の設計なども期待されます。さらに、他の最適化手法（例：Adam、RMSprop）や、より複雑なスケジューリング戦略との組み合わせも興味深い研究方向です。