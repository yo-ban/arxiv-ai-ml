# Yume: An Interactive World Generation Model

## 基本情報
- **arXiv ID**: 2507.17744v1 (https://arxiv.org/abs/2507.17744)
- **著者**: Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang
- **所属**: Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute
- **投稿日**: 2025年07月25日
- **カテゴリ**: cs.AI, cs.CV, cs.LG

## 簡単に説明すると
Yumeは、画像、テキスト、または動画を入力として、インタラクティブで現実的な動的世界を生成することを目指す世界生成モデルです。この論文では、Yumeのプレビュー版を紹介しており、入力画像から動的世界を作成し、キーボード操作を使用して探索できる機能を実現しています。高忠実度でインタラクティブな動画世界生成を実現するために、カメラモーション量子化、動画生成アーキテクチャ、高度なサンプラー、モデル加速という4つの主要コンポーネントからなるフレームワークを導入しています。すべてのデータ、コードベース、モデルの重みはGitHub（https://github.com/stdstu12/YUME）で公開されており、Hugging Face（https://huggingface.co/stdstu123/Yume-I2V-540P）でモデルを試すことができます。Yumeは月次でアップデートされ、オリジナルの目標達成を目指しています。

## 1. 研究概要
### 1.1 背景と動機
動画拡散モデルは、高忠実度で時間的に一貫性のある視覚コンテンツを合成する優れた能力を示しており、洗練されたインタラクティブな世界生成タスクを実現するための有望な道筋を提供しています。最近、生成モデルの進歩とワールドシミュレーション、インタラクティブエンターテインメント、バーチャル実施形態などの領域での没入型体験への需要の高まりにより、大規模でインタラクティブで永続的な仮想世界の自動生成が急速に進歩しています。

しかし、既存の動画拡散手法は、特に都市シナリオでインタラクティブ（連続的なカメラモーション制御）で現実的な動画を生成する際に重大な課題に直面しています。既存のアプローチは主に合成または静的なシナリオに焦点を当てており、データと手法の両面で現実世界とのドメインギャップが存在し、一般化可能性が制限されています。また、これらの手法は主に絶対的なカメラモーションに基づいており、精密な注釈と追加の学習モジュールが必要となり、訓練とアーキテクチャ設計の難易度が増加します。

さらに、都市環境は多様な建築スタイル、動的オブジェクト、複雑な詳細によって特徴付けられる独特の複雑性を提示します。既存のアプローチは、このような複雑性に対する適応性が限られており、多様なシーン間で一貫したリアリズムを維持するのに苦労しています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです：
- 離散的なキーボード制御により、入力画像を通じて自己回帰的なインタラクティブ動画生成を実現するYumeを導入
- カメラ軌道を並進移動（前進、後退、左、右）と回転運動（右回転、左回転、上傾き、下傾き）に量子化し、キーボード入力で柔軟に組み合わせて転送可能な量子化カメラモーション（QCM）制御モジュールを提案
- テキストベース制御の限界に対処するため、Masked Video Diffusion Transformers（MVDT）を調査し、理論的に無限の期間のインタラクティブ動画生成を実現
- 各拡散ステップで潜在表現の高周波成分を改良する訓練不要のアンチアーティファクトメカニズム（AAM）モジュールを導入
- 後のデノイジング段階からの情報を活用して初期のデノイジングプロセスをガイドする、Time-Travel Stochastic Differential Equation（TTS-SDE）フレームワークに基づく新しいサンプリング手法を提案
- 拡散ベースの動画生成のための複数の加速技術を調査し、ステップ蒸留とキャッシュメカニズムを共同最適化

## 2. 提案手法
### 2.1 手法の概要
Yumeは、カメラモーション制御、モデルアーキテクチャ、サンプラー、モデル加速の4つの主要な次元での体系的な最適化を実装しています。アーキテクチャはWanのネットワーク設計に基づいていますが、masked video diffusion transformersを導入し、ReFlowベースの訓練方法論を採用しています。

長動画生成を可能にするために、訓練中にダウンサンプリングされた履歴動画クリップ（Patchifyを使用）を生成されたセグメントと連結し、DiTデノイジングモデルに供給します。これは推論時にも同様に複数の動画セグメントをダウンサンプリングして結合することで再現されます。

### 2.2 技術的詳細
**量子化カメラモーション（QCM）**：
既存のアプローチがフレームごとのカメラ・ツー・ワールド（c2w）変換行列の密なシーケンスに依存するのに対し、Yumeは離散的な事前定義されたカメラモーションセット𝔸_setを定義します。各モーション A_j（例：「前進」、「上を向く」、「下を向く」）は、典型的なナビゲーション操作を表す標準的な相対変換行列 T_canonical,j に対応します。この量子化により、連続的なカメラ軌道が意味的に意味のあるモーションのシーケンスに変換され、相対的な姿勢変化から時間的コンテキストが自然に統合されます。

**Masked Video Diffusion Transformers（MVDT）**：
MVDTは、選択的にマスクされた入力特徴を処理する非対称ネットワークを採用し、エンコーダー、サイドインターポレーター、デコーダーの3つのコアコンポーネントで構成されます。マスキング比率ρ（論文では0.3に設定）を適用することで、アクティブな特徴の縮小セットを生成し、より良い視覚品質を実現します。

**アンチアーティファクトメカニズム（AAM）**：
AAAMは、各拡散ステップで潜在表現の高周波成分を改良する2段階プロセスを含みます。標準的な多段階デノイジングプロセスを実行して初期潜在推定値を取得し、その後、改良デノイジングパスを開始します。この改良パスの最初のKステップ（例：5）で、デノイジングプロセスに介入し、初期生成の低周波情報と現在の改良ステップの高周波情報を組み合わせます。

### 2.3 新規性
本研究の主な新規性は以下の通りです：

1. **相対的カメラモーション量子化**: 絶対的なカメラポーズ行列に依存する既存手法と異なり、相対的なカメラポーズ変化を離散的なキーボードアクションに変換し、追加の訓練可能モジュールを導入せずにテキスト条件に注入する仕組み。

2. **MVDT アーキテクチャ**: 選択的マスキング戦略を採用した非対称ネットワーク構造により、クロスフレーム生成でのアーティファクトと構造的不整合を削減。

3. **訓練不要の品質向上**: AAAMは追加の訓練やデータセットを必要とせず、潜在空間での周波数領域操作により視覚品質を向上。

4. **TTS-SDEサンプリング**: 将来のフレームガイダンスを過去のフレーム生成に伝播させる新しいサンプリングフレームワークで、SDE統合により確率性を保持しテキスト制御性を向上。

## 3. 実験結果
### 3.1 実験設定
実験では、高品質な世界探索データセットSekaiを使用してYumeを訓練しています。Sekai-Real-HQは、400時間の高品質ウォーキング動画クリップで構成され、対応するカメラ軌道と意味ラベルの高品質注釈が含まれています。

評価は以下の指標で行われています：
- Instruction Following（指示追従性）
- Subject Consistency（主体一貫性）
- Background Consistency（背景一貫性）
- Motion Smoothness（動作の滑らかさ）
- Aesthetic Quality（美的品質）
- Imaging Quality（画像品質）

### 3.2 主要な結果
**画像から動画への生成**：
Wan-2.1やMatrixGameなどの最先端モデルとの比較実験では、以下の結果が得られました：

1. Wan-2.1は、テキスト指示を使用してカメラモーションを制御する際の指示追従能力が限定的（0.057）
2. MatrixGameはある程度の制御可能性を示すが（0.271）、現実世界のシナリオへの一般化に苦労
3. Yumeは制御可能性で優れており、指示追従能力スコアが0.657と、他のモデルを大幅に上回る
4. Yumeは他の指標でも最適またはほぼ最適な性能を達成

**長動画生成性能**：
18秒の動画シーケンスを生成し、Yumeが2秒のセグメントを段階的に生成する実験では：
- 0-8秒と12-18秒のセグメント間で、主体一貫性が0.5%（0.934→0.930）、背景一貫性が0.6%（0.947→0.941）低下と、軽度のコンテンツ劣化を観察
- モーション遷移フェーズ（8-12秒）では、指示追従性が8.6%低下したが、12秒後には22.3%の大幅な回復を示す

### 3.3 既存手法との比較
異なるサンプラーの有効性を検証するアブレーション研究では：
- TTS-SDEは指示追従性を大幅に改善（0.743）し、他の指標でわずかな低下があるものの、モーション軌跡の改良を強化
- モデル蒸留により、ステップ数を50から14に削減し、推論時間を583.1秒から158.8秒に短縮しながら、指示追従性以外の指標では最小限の差異

## 4. 実用性評価
### 4.1 実装の容易性
Yumeの実装は、以下の要素により比較的容易です：
- 事前訓練されたI2Vモデルに追加の学習可能パラメータを導入せずにカメラポーズ制御を実現
- 離散的なカメラモーションをテキスト記述に割り当てる事前定義された辞書を使用
- 既存の動画拡散モデルのアーキテクチャに基づいており、主要な変更はマスキングメカニズムとサンプリング戦略に限定

### 4.2 計算効率
計算効率の面では以下の特徴があります：
- 標準的なデノイジングステップは50（CFGで100 NFE）
- AAAMでは最初のデノイジングパスで30ステップ（CFGなし）、2回目のパスで30ステップ（CFGあり）で90 NFEとなり、10%のNFE削減を実現
- モデル蒸留により、推論時間を約73%削減（583.1秒→158.8秒）

### 4.3 応用可能性
Yumeは以下のような幅広い応用が可能です：

**世界の一般化**：
現実世界の動画で訓練されているにもかかわらず、アニメーション、ビデオゲーム、AI生成画像など、多様な非現実的なシーンに対して印象的な一般化可能性を示します。また、V2Vをサポートしているため、iPhoneで撮影したライブ画像にも適応できます。

**世界の編集**：
GPT-4oなどの画像編集手法とYumeを組み合わせることで、世界編集を実現できます。動画生成中に天候、時間、スタイルを変更する例がデモビデオで示されています。

## 5. まとめと所感
### 5.1 論文の意義
Yumeは、高品質で動的かつインタラクティブな無限動画生成への重要な一歩を提供しています。特に複雑な現実世界のシーン探索において、以下の点で意義があります：

1. **直感的な制御**: キーボード入力による離散的なカメラモーション制御は、従来の連続的なポーズ行列よりも直感的で安定した制御を提供
2. **高品質な生成**: MVDTとAAAMの組み合わせにより、複雑な都市シーンでも高品質な動画生成を実現
3. **実用的な加速**: ステップ蒸留とキャッシュメカニズムの共同最適化により、品質を維持しながら大幅な高速化を達成
4. **オープンソース**: すべてのデータ、コード、モデル重みを公開することで、コミュニティの発展に貢献

### 5.2 今後の展望
著者らも認めているように、Yumeは長期的なプロジェクトであり、まだ多くの課題があります：

1. **視覚品質の向上**: より高解像度でのトレーニングによる品質向上（現在は540P、将来的にはより高解像度を目指す）
2. **制御精度の改善**: 現在の制御精度は90%未満であり、より大規模なデータセットやゲーム動画データの混合による改善が期待される
3. **オブジェクトとのインタラクション**: 現在は世界の探索に限定されているが、オブジェクトとのインタラクション機能の追加が計画されている
4. **効率性の向上**: リアルタイムアプリケーションに向けたさらなる高速化

Yumeは月次でアップデートされる予定であり、テキスト、画像、動画から完全にインタラクティブで現実的な動的世界を作成するという最終目標に向けて継続的に改善されていくことが期待されます。