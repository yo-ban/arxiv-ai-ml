# Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains

## 基本情報
- **arXiv ID**: 2507.17746v1 (https://arxiv.org/abs/2507.17746)
- **著者**: Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, Sean Hendryx (Scale AI)
- **所属**: Scale AI
- **投稿日**: 2025年07月25日
- **カテゴリ**: cs.LG, cs.AI

## 簡単に説明すると
この論文は、検証可能な正解が存在しない実世界のタスクに対して強化学習を適用するための新しいフレームワーク「Rubrics as Rewards（RaR）」を提案しています。従来の強化学習では、数学やプログラミングのような正解が明確なタスクで成功を収めてきましたが、医療や科学のような主観的評価と客観的評価の両方が必要な領域では適用が困難でした。RaRは、構造化されたチェックリスト形式のルーブリック（評価基準）を報酬信号として使用し、人間が解釈可能で透明性の高い学習を実現します。実験では、医療分野のHealthBench-1kで従来手法より最大28%の相対的改善を達成しています。

## 1. 研究概要
### 1.1 背景と動機
Reinforcement Learning with Verifiable Rewards（RLVR）は、数学やコーディングのような検証可能な成果を持つタスクで大規模言語モデルが複雑な推論を行うことを可能にしてきました。これらのドメインでは、報酬モデルをスコアリング関数に置き換えることができ、最終的な答えが簡単に検証可能であったり、テストケースが自動評価メカニズムを提供したりします。

しかし、多くの実世界のタスクには明確な検証可能な答えが存在せず、モデルは直接的な報酬フィードバックを得ることができません。実際には、研究者はしばしばpreference ranking によるRLHFに頼り、モデル出力のペアやリストに対する人間の判断を収集してこのギャップを埋めています。しかし、preferenceベースの報酬モデルは表面的なアーティファクト（例：応答の長さ、フォーマットの癖、アノテーターのバイアス）に過剰適合する傾向があり、大量のペアワイズ比較を必要とするため、脆弱でコストがかかります。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです：
- チェックリストスタイルのルーブリックを使用して多基準タスクを監督し、安定した訓練と推論および実世界ドメインの両方でのパフォーマンス向上を可能にするオンポリシー強化学習フレームワーク「Rubrics as Rewards（RaR）」を導入
- 医学と科学のドメインにルーブリック生成アプローチを適用し、RaR-Medicine-20kとRaR-Science-20kという2つの訓練データセットを作成
- RaRで訓練されたモデルが強力なベースラインと同等またはそれ以上の性能を示し、多様なドメインで顕著な精度向上を達成することを実証
- ルーブリックを構造化された報酬信号として使用することで、より小さなジャッジモデルが人間の好みとより良い整合性を達成し、異なるモデルスケールで堅牢な性能を維持できることを示す

## 2. 提案手法
### 2.1 手法の概要
RaRフレームワークでは、入力プロンプトxに対して、モデルπ_θから生成された応答ŷをサンプリングします。単一の正解や自動的な正確性信号がないドメインでは、プロンプト固有のルーブリック基準を使用して構造化された報酬関数を定義します。

各プロンプトxは、k個のルーブリック項目{(w_j, c_j)}_{j=1}^kと関連付けられます。ここで、w_j ∈ ℝは基準jの重みを表し、c_j: (x, ŷ) → {0, 1}は、応答ŷがプロンプトが与えられた場合にその基準を満たすかどうかを示すバイナリ正確性関数です。最終的な正規化されたスカラー報酬は次のように計算されます：

r(x, ŷ) = Σ_{j=1}^{k} w_j · c_j(x, ŷ) / Σ_{j=1}^{k} w_j

この正規化により、異なる数と重みのルーブリック項目を持つプロンプト間で報酬値の一貫性が保証されます。

### 2.2 技術的詳細
RaRフレームワークでは、2つの報酬集約方法を提供しています：

**明示的ルーブリック集約（Explicit Rubric Aggregation）**：
各基準は生成的報酬モデルを使用して独立に評価され、報酬は重み付き和として計算されます。現在はバイナリ値のc_jを仮定していますが、このフレームワークは連続値の判断への拡張をサポートします。

**暗黙的ルーブリック集約（Implicit Rubric Aggregation）**：
代替アプローチとして、すべてのルーブリックの記述と重みを生成的報酬モデルに渡し、内部的にスカラー報酬を計算します：
r_implicit(x, ŷ) = f_φ(x, ŷ, {(w_j, d_j)}_{j=1}^k)

ここで、f_φはLLMベースのジャッジを表し、プロンプトx、生成された応答ŷ、および重み付きルーブリック基準のセットを入力として受け取ります。

### 2.3 新規性
既存のアプローチとの主な違いは以下の通りです：

1. **構造化された多次元報酬**: 単一の検証可能な正解に依存する従来のRLVRと異なり、RaRは複数の評価基準を統合した構造化された報酬を使用します。

2. **解釈可能性**: ブラックボックスの報酬モデルやpreferenceベースの手法と異なり、RaRは人間が解釈可能なルーブリック基準を明示的に最適化します。

3. **柔軟性**: RaRはRLVRを一般化し、客観的および主観的な応答品質の両方を取り入れることができます。RLVRは、k=1、w_1=1、c_1(x, ŷ)が単一の検証可能な正確性関数に還元される特殊なケースとして表現できます。

## 3. 実験結果
### 3.1 実験設定
実験では2つの推論ドメイン（医学と科学）でルーブリックの有用性を調査しています：

**データセット**：
- RaR-Medical-20k: medical-o1-reasoning-SFT、natural_reasoningデータセット、SCP-116Kデータセット、GeneralThought-430Kデータセットから20,000プロンプトをキュレート
- RaR-Science-20k: GPQAダイヤモンドベンチマークに整合した約20,000の科学プロンプトをキュレート

**訓練詳細**：
- ベースポリシー: Qwen2.5-7B
- アルゴリズム: GRPO（Group Relative Policy Optimization）
- バッチサイズ: 96
- 学習率: 5×10^-6（10%の線形ウォームアップ付き）
- ハードウェア: 8台のNVIDIA H100 GPUを搭載した単一計算ノード

### 3.2 主要な結果
Table 1の結果から、以下の重要な発見が得られました：

1. **強力なパフォーマンス**: RaR-Implicitメソッドは、Simple-Likertなどのベースラインを一貫して上回り、HealthBench-1kで最大28%、GPQAで13%の相対的改善を達成しました。

2. **Reference-Likertとの比較**: RaR-Implicitは、Reference-Likertベースラインと同等またはそれ以上の性能を示しました。これは、オープンエンドの答えを構造化されたルーブリック基準に変換することが、効果的で評価目標に整合した報酬信号を生成することを示しています。

3. **暗黙的vs明示的集約**: 暗黙的集約が明示的な重み付き和を上回ることが示され、LLMベースのジャッジがプロンプトごとにルーブリック基準のバランスを推論する方が、固定された手作りの重みに依存するよりも効果的であることを示しています。

### 3.3 既存手法との比較
**モデルスケール間でのジャッジの整合性**：
Figure 2は、ルーブリックガイド評価が、純粋なLikertベースのスコアリングと比較して、モデルサイズ全体でジャッジの精度を一貫して向上させることを示しています。小規模モデルがルーブリック構造から最も恩恵を受け、より大きなジャッジとのパフォーマンスギャップを縮小しています。

**ルーブリック生成戦略の影響**：
Table 2の結果から、すべてのルーブリックベースの手法がルーブリックフリーのベースラインを上回ることが示されました。特に、参照回答へのアクセスがある合成ルーブリックは、アクセスがないものよりも一貫して優れた性能を示し、効果的で整合したルーブリックを生成するためには専門家の信号を組み込むことの重要性を強調しています。

## 4. 実用性評価
### 4.1 実装の容易性
RaRフレームワークの実装は比較的簡単です。主な要件は以下の通りです：

1. **ルーブリック生成**: プロンプト固有のルーブリックを生成するためのLLM（GPT-4oやo3-miniなど）へのアクセス
2. **ジャッジモデル**: 応答を評価するためのLLMジャッジ（実験ではgpt-4o-miniを使用）
3. **GRPO訓練パイプライン**: 標準的な強化学習訓練インフラストラクチャ

フレームワークは既存のRLVRパイプラインに簡単に統合でき、主な変更は報酬計算ステップにあります。

### 4.2 計算効率
RaRの計算コストは主に以下の要因によって決まります：

1. **ルーブリック生成**: 訓練データセットごとに一度だけ実行される一回限りのコスト
2. **報酬計算**: 各訓練ステップで、k=16の応答をサンプリングし、LLMジャッジを使用して評価
3. **訓練時間**: 標準的なGRPO訓練と同程度（8台のH100 GPUで数時間）

暗黙的集約は明示的集約よりもわずかに計算効率が高く、ルーブリックごとに個別の評価を必要としません。

### 4.3 応用可能性
RaRフレームワークは以下のような幅広い応用に適しています：

1. **医療分野**: 事実の正確性、安全性、共感性など複数の基準が重要な臨床QAタスク
2. **教育**: 正確性、明確さ、教育的価値をバランスさせる必要がある教育コンテンツ生成
3. **カスタマーサービス**: 正確性、丁寧さ、問題解決の効果を考慮する必要がある対話システム
4. **創造的タスク**: 独創性、一貫性、スタイルガイドラインの遵守など複数の主観的基準がある執筆タスク

## 5. まとめと所感
### 5.1 論文の意義
この研究は、強化学習の適用範囲を検証可能なドメインを超えて拡張する重要な一歩を示しています。RaRフレームワークは、preferenceベースの手法とバイナリ正確性信号の間の実用的な中間地点を提供し、複雑な実世界のタスクに対してより解釈可能で制御可能な訓練を可能にします。

特に重要なのは、ルーブリックが報酬ハッキングに対する潜在的な堅牢性を提供する可能性があることです。離散的で解釈可能な基準は、不透明な学習されたスコアよりもモデルが悪用するのが難しいためです。これは、安全性が重要な医療や金融などのドメインでのデプロイメントにとって重要な利点です。

### 5.2 今後の展望
著者らが指摘しているように、いくつかの有望な研究方向があります：

1. **ドメインの拡張**: 現在の研究は医学と科学に限定されているため、オープンエンドの対話、創造的執筆、複雑な意思決定タスクなど、他のドメインへの検証が必要です。

2. **カリキュラム学習**: ルーブリックは自然に階層的なタスク構造をエンコードしており、暗黙的なカリキュラム学習をサポートする可能性があります。動的な重み付けやルーブリック項目の段階的導入により、このカリキュラム効果をさらに活用できるかもしれません。

3. **報酬ハッキングの評価**: RaRの報酬ハッキングに対する耐性の正式な評価、モデルが個々のルーブリックコンポーネントを悪用しようとする敵対的訓練シナリオを含む研究が必要です。

4. **専用評価器**: 現在のジャッジは汎用LLMですが、より強力な推論能力を持つ専用評価器の開発により、さらなる改善が期待できます。