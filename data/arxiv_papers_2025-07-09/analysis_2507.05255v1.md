# Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning

## 基本情報
- **arXiv ID**: 2507.05255v1 (https://arxiv.org/abs/2507.05255)
- **著者**: Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, Jia Wang, Chunrui Han, Yuang Peng, Qi Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Vishal M. Patel
- **所属**: Johns Hopkins University, StepFun, BUPT, UCAS, THU, HUST
- **投稿日**: 2025年07月07日
- **カテゴリ**: cs.CV, cs.AI, cs.CL, cs.LG

## 簡単に説明すると
この論文は、言語モデルの認知行動（思考の後戻りや検証など）を視覚推論タスクに転移させる新しいマルチモーダル大規模言語モデル（MLLM）「Open Vision Reasoner（OVR）」を提案しています。
Qwen2.5-VL-7Bをベースに、言語のみのコールドスタート学習と大規模なマルチモーダル強化学習（RL）を組み合わせた2段階の学習フレームワークを採用しています。

この研究では、言語領域で獲得された認知行動が視覚領域でも有効に機能することを実証しています。
特に、言語のみの学習段階で「メンタルイメージ」として内在化された視覚的思考パターンが、実際の視覚入力と結びつくことで効果的に転移することを発見しました。

実験結果では、OVRは数学推論タスク（AIME2024で63.5%、MATH500で95.3%）と視覚推論タスク（MathVisionで51.8%、MathVerseで54.6%）の両方で優れた性能を達成しています。
また、視覚的認知行動の詳細な分析により、コールドスタート段階での幅広い学習とRL段階での戦略的な選択・増幅というプロセスが明らかになりました。

プロジェクトのモデル、データ、トレーニングフレームワークは以下で公開される予定です。
- モデル、データ、コードは今後オープンソース化される予定

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデル（LLM）における強化学習は、人間のフィードバック（RLHF）から検証可能な報酬（RLVR）へとパラダイムシフトしています。
検証可能な報酬は客観的でルールベースの基準に基づいており、報酬ハッキングに対して本質的に堅牢です。
この堅牢性により、大規模なRLが可能となり、後戻りやサブゴール分解などの「認知行動」の内在化と活性化が実現されています。

マルチモーダル領域は本質的に検証可能な視覚的事実に基づいているにもかかわらず、初期のマルチモーダルRLの取り組みは逆説的にRLHFを採用していました。
最近になって、Perception-R1やR1-OneVision、VLAA-Thinkingなどの研究が、ルールベースの報酬をマルチモーダル設定で探求し始めています。
ReVisual-R1は、視覚推論の基盤として効果的な言語のみのコールドスタートを採用しています。

しかし、これらのアプローチは根本的な問いに答えていません。
それは「言語的認知行動はどのようにしてMLLMsに転移し、高度な視覚推論を可能にするのか」という問いです。
この研究では、Qwen2.5-VL-7Bを基盤として大規模な学習を行い、マルチモーダル領域で認知行動がどのように出現し、スケールするかを体系的に分析するための強力なテストベッドを構築しています。

### 1.2 主要な貢献
この研究の主要な貢献は以下の3点です。

1. 言語的コールドスタートとそれに続く大規模マルチモーダルRLから成る2段階トレーニングパイプラインを構築し、MLLMsにおける認知行動の効果的な転移を可能にしました。
2. Qwen2.5-VL-7Bにおける最大規模のオープンソースRL実践であるOpen Vision Reasonerは、言語およびマルチモーダル推論ベンチマークの両方で優れた性能を達成しています。  
3. OVRにおける視覚的認知行動の詳細な分析を実施し、トレーニング段階を通じた転移と進化に関する貴重な洞察を提供しています。

## 2. 提案手法
### 2.1 手法の概要
Open Vision Reasoner（OVR）は、Qwen2.5-VL-7Bをベースに構築された強力なマルチモーダル推論モデルです。
トレーニングパイプライン、RLアルゴリズム、データ構築の3つの観点から設計されています。

トレーニングパイプラインは「RLとコールドスタート」パラダイムを採用し、2つの逐次的なトレーニング段階で構成されています。
第1段階では、言語のみの推論データセットでLLMモジュールを教師あり微調整し、後戻りやサブゴール分解などのコア認知行動を確立します。
第2段階では、Open-Reasoner-Zero設定で検証可能なマッチ報酬を使用してテキストとマルチモーダルタスクの両方で強化学習を適用します。

この2段階アプローチにより、効率的な認知開発とクロスモーダル汎化が促進されます。
言語的に確立された認知パターンが視覚的コンテキストと整合され、効果的なクロスモーダル転移が可能になります。

### 2.2 技術的詳細
RLアルゴリズムでは、Open-Reasoner-Zeroに従い、一般化アドバンテージ推定（GAE）を備えた軽量なProximal Policy Optimization（PPO）を採用しています。

各入力（画像Iとテキストプロンプトq）に対して、ポリシーネットワークπ_θはn個の応答を生成します。
各応答は長さT_iの軌跡τ_iであり、状態と行動のペアから構成されます。
GAEを使用してバイアスと分散のバランスを取り、アドバンテージ推定を行います。

報酬関数は最小限のルールベース設計を採用しています。
モデル出力の\boxed{}内にカプセル化された予測答えを抽出し、参照答えと比較します。
完全一致の場合は1、それ以外は0のバイナリ報酬が割り当てられ、強化学習のための明確でスケーラブルかつハッキング不可能な報酬信号を提供します。

データセット構築では、各トレーニング段階に特化したデータセットを慎重にキュレートしています。
言語のみのシナリオでは、AIME、MATH、Numina-Math、Tulu3 MATH、OpenR1-Mathなどの公開ベンチマークを利用します。
マルチモーダルシナリオでは、幾何学問題解決、視覚的識別、視覚パズル、STEM、マルチモーダル数学をカバーするデータセットを組み込んでいます。

データ品質を向上させるため、多段階のキュレーションプロセスを採用しています。
事前学習モデルを使用してノイズや過度の複雑さを示す高い学習損失のサンプルを自動的にフィルタリングします。
ルールベースとモデル支援の方法で望ましくないパターンを特定して削除します。
最終的に、約200万のコールドスタートデータと約30万のマルチモーダルRLデータを構築しています。

### 2.3 新規性
この研究の新規性は以下の点にあります。

1. 言語的認知行動の視覚領域への転移メカニズムを体系的に解明した初の研究です。
2. Qwen2.5-VL-7Bにおいて、約1,000ステップに及ぶ前例のない規模のマルチモーダルRLを実現しました。
3. 視覚的認知行動の出現と転移に関する詳細な分析フレームワークを提供しています。
4. コールドスタート段階での「メンタルイメージ」を介した早期転移の発見は、従来の理解を覆すものです。

## 3. 実験結果
### 3.1 実験設定
実装の詳細として、Qwen2.5-VL-7Bをベースに2段階のトレーニング戦略を採用しています。
コールドスタート段階では、LLMモジュールを独立して5エポック微調整し、バッチサイズ640、シーケンス長64k、学習率2×10^-4を使用します。

強化学習段階では、PPOを使用し、GAEをγ=1、λ=1で設定して、推論タスクに重要な長期依存性を完全に捉えます。
このRL段階は900イテレーション続き、シーケンス長のカリキュラムを採用しています。
最初の300イテレーションは24kから始まり、700イテレーションまでに32kに増加し、その後48kに拡張されます。

最終モデルは、様々なベンチマークでバランスの取れた堅牢な性能を確保するため、いくつかの代表的な中間チェックポイントの均一平均です。

### 3.2 主要な結果
言語推論タスクでは、OVRは例外的な能力を示しています。
AIME 2024およびAIME 2025ベンチマークで、他の7Bオープンソースモデルを平均10%以上上回り、主要な32Bモデルに匹敵する性能を達成しています。
MATH500では95.3%を達成し、オープンソース7Bモデルの中で最高性能を記録しました。

一般的な推論タスクでも優位性が拡張され、MMLUで+4.6%、MMLU-Proで+10.4%の大幅な向上を示しています。
これらの結果は、キュレートされた高品質なコールドスタートトレーニングデータの有効性を強調しています。

視覚推論タスクでは、OVRは7Bモデルにおける新たなブレークスルーを設定しています。
MathVisionで50%の性能を超えた最初のポストトレーニングQwen2.5-VL-7Bベースのモデルであり、同時にDynaMathとMathVerseで7Bモデルの中で最先端の結果を達成しています。
MMMU-Proでは、以前のSOTA手法を7.2%上回る大幅な向上を示しています。

### 3.3 既存手法との比較
言語推論において、OVRはDeepSeek-R1-Distill-Qwen-7BやQwQ-32B-Previewなどの強力なベースラインと比較されています。
AIME 2024では63.5%を達成し、同規模のオープンソースモデルを大幅に上回っています。

マルチモーダル推論では、LLaVA-OneVisionやQwen2.5-VLなどのSFTベースの手法、およびOpenVLThinker、MM-Eureka、ReVisual-R1などの最近のルールベースRL手法と比較されています。
OVRは8つのベンチマークのうち5つで最高性能を達成し、他でも競争力のある結果を維持しています。

特に注目すべきは、MathVisionで51.8%を達成し、次点のReVisual-R1の48.8%を3.0%上回っていることです。
これは、言語トレーニングを通じて獲得された推論能力がマルチモーダルタスクに効果的に転移できることを示しています。

## 4. 実用性評価
### 4.1 実装の容易性
OVRの実装は、Qwen2.5-VL-7Bという確立されたベースモデルを使用しているため、比較的アクセスしやすいものです。
2段階のトレーニングパイプラインは明確に定義されており、再現性が高いです。

ただし、大規模なトレーニングには相当な計算リソースが必要です。
コールドスタート段階では200万のサンプルで5エポックの学習が必要であり、RL段階では900イテレーションにわたる学習が必要です。
シーケンス長が最大48kに達するため、メモリ要件も高くなります。

モデル、データ、トレーニングフレームワークのオープンソース化が予定されており、研究コミュニティでの採用と改良が促進されることが期待されます。

### 4.2 計算効率
トレーニングダイナミクスの分析により、コールドスタート段階では損失が急速に0.5以下に低下し、その後複数のエポックにわたって段階的に減少することが示されています。
RL段階では、報酬と平均トークン長が着実に増加し、初期の7kから12kを超えるまで成長します。

計算効率の観点から、積極的なトレーニング戦略（大きなバッチサイズと高い学習率）が不可欠であることが判明しています。
この戦略により、モデルの固有の制約を打破し、新しい認知パラダイムを成功裏に注入できます。

ただし、視覚認知行動の分析には追加の計算コストがかかります。
GPT-4oを使用した行動分析は、推論トレースの詳細な分析を必要とします。

### 4.3 応用可能性
OVRは以下のような幅広い応用が期待されます。

1. 教育支援システム：複雑な数学や科学の問題を視覚的に解析し、段階的な解法を提供する教育AIアシスタント
2. 技術文書解析：図表やグラフを含む技術文書を理解し、要約や質問応答を行うシステム
3. 医療画像解析：医療画像と臨床情報を統合して診断支援を行うAIシステム
4. 産業検査：製品の視覚検査と品質管理における推論と判断を支援するシステム
5. 研究開発：科学論文の図表を理解し、研究内容の分析と新しい洞察の生成を支援するツール

特に、言語的推論能力と視覚的理解の統合が必要なタスクにおいて、OVRの認知行動転移メカニズムは大きな価値を提供します。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、マルチモーダルAIにおける認知行動の転移という重要な問題に取り組んだ先駆的な研究です。
言語領域で獲得された認知行動が視覚領域でも有効に機能することを実証し、その転移メカニズムを詳細に分析したことは、今後のMLLM研究に大きな影響を与えるでしょう。

特に、コールドスタート段階での「メンタルイメージ」を介した早期転移の発見は、人間の認知プロセスとの類似性を示唆しており、AIの認知能力理解に新しい視点を提供しています。
また、コールドスタートでの幅広い学習とRLでの戦略的な選択・増幅というプロセスの解明は、効率的な学習パラダイムの設計に重要な示唆を与えています。

実用面では、7Bモデルでありながら大規模モデルに匹敵する性能を達成したことは、計算リソースが限られた環境でも高度な視覚推論が可能であることを示しています。

### 5.2 今後の展望
論文で議論されている今後の研究方向として以下が考えられます。

1. より多様な視覚的認知行動の統合：現在の4つの行動パターンを超えて、より複雑で多様な視覚的認知行動を特定し、統合する研究
2. マルチターンおよびエージェント型RLフレームワーク：視覚的操作と想像力に基づくより高度なRLフレームワークの開発
3. 知覚タスクにおけるRLのスケーラビリティ向上：OCRやカウンティングなどの基本的な視覚タスクでのRL効果を高める手法の開発
4. コールドスタートでの知覚劣化の軽減：言語データをモデルの事前学習に組み込むか、コールドスタート中により多くのマルチモーダル監督を導入する方法の探求
5. 認知行動の転移メカニズムのより深い理解：なぜ特定の行動（後戻り）が他の行動（検証）よりも転移しやすいのかの解明

特に、視覚的認知行動の根本的な能力ギャップに対処することは、堅牢なマルチモーダルスケーリングを達成するために最も重要な課題となるでしょう。
ツール使用を伴うマルチターンRLや、メンタルイメージを通じた内在的想像力の統合などの新しいアプローチが、現在の限界を橋渡しし、よりスケーラブルなマルチモーダル推論を解き放つ可能性を秘めています。