# Convergence and Divergence of Language Models under Different Random Seeds

## 基本情報
- arXiv ID: 2509.26643v1 (https://arxiv.org/abs/2509.26643)
- 著者: Finlay Fehlauer, Kyle Mahowald, Tiago Pimentel
- 所属: ETH Zürich, University of Texas at Austin
- 投稿日: 2024年09月40日
- カテゴリ: cs.CL, cs.LG

## 簡単に説明すると
この研究は、同じ設定で異なるランダムシード（乱数の種）を使って訓練された言語モデル同士がどれくらい似た予測をするか、つまり「収束」や「発散」の傾向を詳しく調べた論文です。研究者たちは、言語モデルの訓練過程を4つのフェーズに分けて分析し、モデルサイズや単語の種類によって収束パターンが大きく異なることを発見しました。GitHubリポジトリで実験コードが公開されており、再現可能な研究となっています。特に、大きなモデルほど安定した学習をする一方で、頻出単語と稀な単語では収束の仕方が全く違うという興味深い発見があります。

## 1. 研究概要
### 1.1 背景と動機
言語モデルは本質的に文字列に対する分布 p_θ(s) であり、データ生成分布 p_true(s) を近似するように訓練されます。近年の言語モデルの大幅な改善は、データ、計算量、アーキテクチャサイズの増加によるものとされており、完璧にデータ生成分布をフィットできれば、すべての言語モデルは同じ p_θ に収束するはずです。

しかし実際には、この収束は（1）すべてのコンテキストに対して一様に起こらない可能性、（2）一部のコンテキストでは全く起こらない可能性があります。本研究では、スケール、訓練、コンテキストにわたる言語モデルの収束と潜在的発散に焦点を当てています。

従来の研究では、言語モデルの訓練ダイナミクスについて重要な発見がありました。まず、訓練初期において言語モデルはユニグラム出力段階に到達し、コンテキストに依存しない分布を出力して単語頻度に一致させ、その後にコンテキストを活用し始めることが示されています。また、トランスフォーマーは誘導ヘッド形成を経て、文脈内学習を可能にすることも明らかになっています。

### 1.2 主要な貢献
本研究では、言語モデルの収束を異なるシードで訓練された際の期待クルバック・ライブラー（KL）発散の負の値として定義し、以下の重要な貢献をしました。

- 言語モデルの収束が訓練を通じて4つの明確なフェーズを示すことを発見
  - 初期一様フェーズ
  - 急激収束フェーズ
  - 急激発散フェーズ  
  - 緩やかな再収束フェーズ
- 大きなモデルほど後期の訓練段階で再収束が速く、小さなモデルは実際には再収束しないことを観察
- 頻出トークンと機能語は稀なトークンや内容語よりも速く、より確実に収束することを発見
- 条件付き収束の概念を導入し、特定のトークン特性に基づく収束の変動を定量化

## 2. 提案手法
### 2.1 手法の概要
研究者たちは、まずモデルパラメータ上の分布 p_true(θ) が存在すると仮定しました。これは、アーキテクチャの選択と最適化プロセスによって誘導される、異なるランダムシードで訓練されたモデルの分布を表します。

収束は、コンテキスト s_<t における2つのモデル θ と θ' の間の期待発散の負の値として定義されます：
conv(s_<t) = E_{θ,θ'}[-d_s_<t(p_θ, p_θ')]。

理論的には任意の発散関数を使用できますが、本研究ではクルバック・ライブラー（KL）発散を採用しました。KLは確率分布を比較する標準的な尺度であり、conv(s_<t) の増加は収束を、減少は発散を示します。

### 2.2 技術的詳細
期待収束は、コンテキストにわたる収束の期待値として定義されます：
E[conv] = E_{s_<t}[conv(s_<t)]。

さらに、特定の性質を持つトークンに条件付けされた条件付き収束も定義しました：
E_{S_t}[conv] = E_{s_≤t}[conv(s_<t) | s_t ∈ S_t]。

ここで S_t は特定の目標性質を持つトークンの集合です。この条件付き収束により、異なるトークンカテゴリ（名詞、動詞、機能語など）での収束の変動を分析できます。

### 2.3 新規性
本研究の新規性は、言語モデルの収束を体系的に定量化する新しい枠組みの提案にあります。従来の研究では、モデルの予測における相関を計算することで収束を定量化していましたが、この手法は期待収束が捉える微妙なニュアンスを隠す可能性があります。

また、4つの明確な訓練フェーズの発見と、これらが既知の言語モデル現象（ユニグラム出力段階、誘導ヘッド形成）と一致することの実証は重要な新知見です。特に、収束が訓練を通じて単調ではないという発見は、複数のシードで同じモデルがお互いに類似性を失いながら、同時に目標分布により類似していくという興味深い現象を明らかにしています。

## 3. 実験結果
### 3.1 実験設定
実験には（Poly）Pythiaスイートの言語モデルを使用しました。各モデルサイズに対して10個の独立に訓練されたモデルが含まれています。分析したモデルサイズは14Mから31M、70M、40M、70Mパラメータの5種類です。対数間隔の訓練ステップで評価しました。計算制約により、5つの異なるシードを選択して分析しました。

データセットとしてPileの検証セットのサブセット（4,662トークン）を使用し、これらのトークンのコンテキストがデータセット D = {s_<t^(n)}_{n=1}^N を形成しています。条件付けプロパティとして、トークンの頻度、品詞（NLTK品詞タガーを使用）、最終サプライザル（特定のモデルサイズの最後のチェックポイントを使用して計算）を分析しました。

### 3.2 主要な結果
実験により、言語モデルの期待収束は訓練を通じて4つの明確なフェーズを示すことが判明しました。

一様フェーズ（～ステップ16）では、すべてのモデルサイズが類似した収束を示し、モデルの出力が一様分布に類似していることが確認されました。これは、パラメータの初期化によって強制される共通の出発点を反映しています。

急激収束フェーズ（ステップ16-256）では、モデル間の類似性が急激に増加し、一様分布からユニグラム分布への移行と明確に対応しています。この段階で、モデルの予測（サプライザルや下流タスクの出力）は最大の相関を示すことが先行研究でも報告されています。

急激発散フェーズ（ステップ256-2k）では、モデル間の類似性が急激に減少し、言語モデルがユニグラム分布から発散し始める時期と一致しています。これは、少なくとも初期段階では、言語モデルのコンテキスト使用がシード間で大きく異なることを示唆しています。

緩やかな再収束フェーズ（ステップ2k以降）では、モデルの類似性がゆっくりと増加し始めます。興味深いことに、このフェーズの開始は文脈内学習スコアの増加と誘導ヘッド形成と一致しています。これは誘導ヘッドが大きなモデルでの文脈内学習を可能にするだけでなく、トランスフォーマーベースの言語モデルの訓練を安定化する可能性があることを示唆しています。

### 3.3 既存手法との比較
BLiMPデータセットでの下流タスク収束実験では、収束パターンが broadly に前述の4フェーズを反映していることが確認され、これらの訓練ダイナミクスがタスクレベルでも現れることが支持されました。

MultiBERTモデルスイートを用いた双方向トランスフォーマーでの実験では、初期の一様フェーズと急激収束フェーズは観察できませんでした。これは利用可能なチェックポイントが0から20kステップの間を含んでいないためです。しかし、残りのダイナミクス（急激発散フェーズとそれに続く緩やかな再収束フェーズ）はPythiaでの結果と一致していました。

## 4. 実用性評価
### 4.1 実装の容易性
提案された収束測定手法は、複数のランダムシードで訓練されたモデルがあれば比較的容易に実装できます。PolyPythiaスイートのような既存のリソースを活用でき、KL発散の計算は標準的な深層学習フレームワークで実装可能です。ただし、大規模言語モデルでの計算は計算資源の制約があり、本研究でもサブセットでの分析に限定されています。

### 4.2 計算効率
収束の計算はモデルペアごとにKL発散を計算する必要があり、モデル数の二乗に比例して計算量が増加します。本研究では5つのシードを使用し、10組のペアで分析しました。大規模な分析をする場合は、計算時間を短縮するサンプリング戦略や近似手法の開発が必要になる可能性があります。

### 4.3 応用可能性
この手法は言語モデルの訓練安定性や再現性の評価に直接応用できます。特に、モデルの信頼性が重要な実用アプリケーションにおいて、異なる初期化での一貫性を評価する際に有用です。また、モデル開発において、どの段階で訓練を停止すべきかの判断材料としても活用できる可能性があります。さらに、頻出語と稀な語での収束パターンの違いは、ドメイン適応や専門分野での言語モデル開発において重要な示唆を提供します。

## 5. まとめと所感
### 5.1 論文の意義
本研究は言語モデルの訓練ダイナミクスにおける収束現象を体系的に分析した重要な貢献です。4つの明確な訓練フェーズの発見は、言語モデルの学習プロセスに対する新しい理解を提供し、既知の現象（ユニグラム出力段階、誘導ヘッド形成）との関連付けにより、包括的な理論的枠組みを構築しています。

特に注目すべきは、グローバルな性能指標（クロスエントロピー）が単調に改善する一方で、収束は非単調であるという発見です。これは、モデルが目標分布により近づきながら、同時に互いの類似性を失うという興味深い現象を明らかにしており、言語モデルの内部メカニズムの複雑さを示しています。

また、トークンの頻度や品詞による収束パターンの違いの発見は、言語の本質的な構造と学習プロセスの関係について重要な洞察を提供しています。頻出語や機能語の安定した収束は、言語の統計的規則性の学習を反映しており、一方で内容語の不安定さは、創造的で文脈依存的な言語使用の複雑さを示唆しています。

### 5.2 今後の展望
今後の研究では、いくつかの重要な方向性が考えられます。まず、異なるトークナイザーを使用するモデルファミリー間での比較を可能にするため、バイトレベルや単語レベルでの分布変換手法の開発が期待されます。

また、英語以外の言語や多言語設定での同様の収束ダイナミクスの調査も重要です。言語の構造的特性が収束パターンにどのような影響を与えるかの理解は、より普遍的な理論の構築に貢献するでしょう。

さらに、学習率のウォームアップフェーズが早期の訓練における急激な行動変化に与える影響についても、より詳細な分析が必要です。これらの人工的な要因と本質的な学習ダイナミクスを区別することで、より純粋な収束現象の理解が可能になるでしょう。
