# Query-Kontext: An Unified Multimodal Model for Image Generation and Editing

## 基本情報
- arXiv ID: 2509.26641v1 (https://arxiv.org/abs/2509.26641)
- 著者: Yuxin Song他11名
- 所属: Baidu VIS, National University of Singapore
- 投稿日: 2024年09月30日
- カテゴリ: cs.CV, cs.LG

## 簡単に説明すると
この研究は、画像生成と編集を統一的に行う新しいマルチモーダルモデル「Query-Kontext」を提案した論文です。従来の統一マルチモーダルモデルでは、意味理解と画像生成が密結合しているため、それぞれの強みを十分に活用できませんでした。この研究では、Vision-Language Model（VLM）と拡散モデルの間に「kontext」トークンという仲介インターフェースを導入しています。これにより、意味的推論と高品質な視覚合成を効果的に分離できます。3段階の学習手法により、テキストから画像生成や指示に基づく画像編集、カスタマイズ生成、多数被写体の合成など、多様なタスクで優れた性能を実現しています。

## 1. 研究概要
### 1.1 背景と動機
統一マルチモーダルモデル（UMMs）は、テキストから画像生成（T2I）と画像編集（TI2I）の両方で大きな進歩を遂げています。現在のアプローチには2つの主要なパラダイムがあります。

第1のアプローチは、外部拡散トランスフォーマー（MMDiTなど）を既成のVision-Language Model（VLM）や大規模言語モデル（LLM）と組み合わせます。これは組み合わせ型統一フレームワークです。第2のアプローチは、理解と生成をより密に統合した混合モーダル早期融合トランスフォーマーです。これは、強い推論能力を持つ自己回帰モジュールと視覚合成に特化した拡散モジュールを共同訓練するナイーブUMMです。

しかし、これらのパラダイムはタスクカバレッジを拡大し展開を合理化する一方で、マルチモーダル生成推論と高品質なレンダリングを絡み合わせてしまいます。結果として、VLMの独特な強み（意味理解、グラウンディング、構造化推論）と拡散モデルの強み（写実的合成、詳細な忠実性）を十分に活用できていません。

### 1.2 主要な貢献
本研究は、これらの制約を解決するために、VLMのマルチモーダル生成推論を拡散モデルの高品質な視覚レンダリングから明確に分離するQuery-Kontextを提案しました。主要な貢献は以下の通りです。

- VLMのマルチモーダル生成推論を拡散モデルの高品質視覚レンダリングから分離する経済的なアンサンブルUMMの提案
- VLMを徐々により高性能な拡散生成器と整合し、それぞれの強みを増幅する3段階の段階的な訓練戦略の提示
- 多様なマルチモーダル参照から画像シナリオをカバーするための、リアル、合成、注意深くフィルタリングされたオープンソースデータセットを収集する慈意的なデータセットキュレーションスキームの提示

## 2. 提案手法
### 2.1 手法の概要
Query-Kontextは、VLMのマルチモーダル生成推論を拡散モデルの高品質な視覚合成から明確に分離するマルチモーダル「kontext」を活用した経済的なアンサンブルUMMです。

アーキテクチャは4つの主要コンポーネントから構成されています。マルチモーダル大規模言語モデル（MLLM）、コネクタモジュール、マルチモーダル拡散トランスフォーマー（MMDiT）、低レベル画像エンコーダー（VAE）です。MLLMはQwen2.5-VLモデルで初期化され、テキストプロンプト、入力画像、学習可能なクエリトークンのセットを含むマルチモーダル入力をエンコードおよび融合します。

### 2.2 技術的詳細
出力は、拡散デコーダーの粗い画像レベルの条件付けとして機能する固定長のkontextトークンシーケンスQ = {q_1,...,q_K}です。これは高レベルの意味的手がかりを提供します。直感的に、kontextトークンQは2つの情報をエンコードします。出力画像にどのようなコンテンツが表示されるべきか（テキストプロンプトからの意味情報）と、出力が提供された入力画像からの視覚的手がかりをどのように組み込むべきかです。

kontext QとテキストトークンTは、軽量なコネクタモジュールを通して拡散モデルの潜在空間と整合されます。実際には、コネクタで生成されたテキスト埋め込みとkontext埋め込みを連結し、拡散モデルが利用できる意味コンテキストを豊かにします。

拡散モデルは、社内のMMDiTモデルで初期化され、元のテキストエンコーダーをMLLMで置き換えます。テキストと視覚入力の両方で拡散プロセスを条件付けるために、複数のトークンを連結します。具体的には、MLLMからのテキストTとkontextトークンQのシーケンスを、現在の拡散ステップでのノイズあり画像潜在変数とVAEによって入力画像から抽出された低レベル視覚特徴トークンと連結します。

### 2.3 新規性
本研究の新規性は、ナイーブUMMと組み合わせ型UMMを区別し、理解と生成モジュールを効果的に分離したことです。クエリ埋め込みをテキストおよび低レベル画像埋め込みと統合しながら、改善された効率と柔軟性を実現しました。

特に、クエリ埋め込みはVLMのコンテキスト内学習能力を自然に解放し、モデルがマルチモーダル入力を推論し、一貫性のある画像を生成できるようにします。従来の手法とは異なり、Query-Kontextはクエリ埋め込みをテキストおよび低レベル画像埋め込みと統合し、理解と生成モジュールを効果的に分離して改善された効率と柔軟性を実現しています。

## 3. 実験結果
### 3.1 実験設定
実験では、リアルデータ、合成データ、オープンソースデータを組み合わせた包括的なデータパイプラインを構築しました。評価指標としてGenEvalベンチマークを使用し、シングルオブジェクト、ツーオブジェクト、カウンティング、カラー、位置、属性バインディングなどの性能を測定しました。また、テキストから画像生成、指示ベースの編集、カスタマイズ生成、マルチ被写体合成などの多様なタスクで評価しました。

### 3.2 主要な結果
GenEvalベンチマークでの評価では、Query-Kontextが総合スコア0.88を達成し、BAGELと同等の最高性能を示しました。特に位置理解タスクでは0.85で最高性能を、属性バインディングタスクでは0.79で最高性能を記録しました。これらの結果は、VLMのマルチモーダル推論能力と拡散モデルの高品質な視覚合成を効果的に分離する手法の有効性を示しています。

### 3.3 既存手法との比較
既存の統一マルチモーダルモデルとの比較では、Query-KontextはSD3.5 Large、FLUX.1、Janus-Pro-7Bなどの強力なベースラインと競合しました。一部のタスクではタスク固有の最新手法を上回りました。特に、Show-oやEmu3-Genなどの既存の統一モデルと比較して、テキスト理解、画像理解、細かな指示の遵守において優れた性能を示しました。これは、VLMの理解能力を十分に活用できていることを示しています。

## 4. 実用性評価
### 4.1 実装の容易性
Query-Kontextのアーキテクチャは、既存のVLMと拡散モデルを組み合わせる設計であり、実装が比較的容易です。主要コンポーネントはMLLM、コネクタモジュール、MMDiT、VAEの4つであり、段階的な訓練戦略により各コンポーネントを段階的に結合できます。軽量なコネクタモジュールの使用により、計算オーバーヘッドを最小限に抑えることができます。

### 4.2 計算効率
モデルの計算効率は非常に優れています。訓練は192台のNVIDIA H100 GPU（80GB）で実施され、これは大規模拡散モデルをゼロから訓練するのに必要な計算リソースのたった10%にすぎません。この経済的なアライメントにより、マルチ被写体合成、マルチ画像生成、インターリーブテキスト画像生成などの高レベルタスクにリソースを効果的に配分できます。

### 4.3 応用可能性
Query-Kontextは、テキストから画像生成、指示ベースの画像編集、カスタマイズ生成、マルチ被写体合成などの多様なタスクに応用可能です。VLMと拡散モデルの分離により、各コンポーネントのスケーリング法則を独立して探索できるため、将来的な改良や拡張が容易です。また、既存の強力なVLMや拡散モデルを活用できるため、実用的なアプリケーションへの展開が期待されます。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、統一マルチモーダルモデルにおける根本的な課題である意味理解と画像生成の密結合を、効果的な分離手法で解決しました。kontextトークンという仲介インターフェースの導入により、VLMの優れた意味理解能力と拡散モデルの高品質な視覚合成能力を各々最大限に活用できることを実証しました。このアプローチは、今後のマルチモーダルAIシステムの設計において重要な指針を提供しています。

### 5.2 今後の展望
今後の研究方向として、コネクタモジュールのスケーリング法則の探索が重要です。特に、軽量なコネクタで大規模な凍結拡散モデルを接続する場合のアライメントの改善が必要です。また、マルチ被写体合成、マルチ画像生成、インターリーブテキスト画像生成などの高レベルタスクへのさらなる拡張、および効率性のさらなる改善が期待されます。VLMと拡散モデルの各コンポーネントの独立した進化を可能にするこの枠組みは、将来のマルチモーダルAI研究において重要な基盤となるでしょう。
