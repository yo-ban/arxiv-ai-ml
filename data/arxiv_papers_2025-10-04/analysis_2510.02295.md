# VideoNSA: Native Sparse Attention Scales Video Understanding

## 基本情報
この論文の基本情報は以下の通りです。

arXiv IDは2510.02295v1です。URL: https://arxiv.org/abs/2510.02295

著者はEnxin Song、Wenhao Chai、Shusheng Yang、Ethan Armandです。また、Xiaojun Shan、Haiyang Xu、Jianwen Xie、Zhuowen Tuも含まれます。

所属機関はUniversity of California San Diego、Princeton University、New York University、Lambda Incです。

投稿日は2025年10月03日です。

カテゴリはcs.CV、cs.AIです。

## 簡単に説明すると
VideoNSAは、長時間の動画理解において計算効率を27倍改善する新しい注意機構を提案しています。従来の動画理解モデルは長い動画を処理する際に、全てのフレーム間での注意計算が必要となり、計算量が膨大になってしまう問題がありました。この研究では、Native Sparse Attention（NSA）という学習可能なスパース注意機構を動画理解に適用し、わずか3.6%の注意計算で従来手法と同等以上の性能を実現しています。具体的には、圧縮・選択・スライディングウィンドウという3つの補完的な注意分岐を動的に組み合わせることで、重要な情報を保持しながら計算効率を向上させています。

関連リンクを以下に示します。

プロジェクトページ: https://enxinsong.com/VideoNSA-web/

GitHubリポジトリ: https://github.com/Espere-1119-Song/VideoNSA

Hugging Faceモデル: https://huggingface.co/Enxin/VideoNSA

## 1. 研究概要
### 1.1 背景と動機
現在のマルチモーダル大規模言語モデル（MLLM）は、動画理解においてコンテキスト長の制約により重要な課題を抱えています。特に、長時間の動画では重要な転換点のフレームを見逃したり、長い時間スケールでの一貫性を維持することが困難です。例えば、90分のサッカーの試合では、勝敗を決める瞬間はわずか数秒間に集約されます。その重要な瞬間の中でも、アシスト、タックルミス、ゴールキーパーの動きといった各行動が結果を左右します。

人間は自然に60Hzで視覚情報をサンプリングし、大きなコンテキストを処理できます。一方、既存のVLM（Vision Language Model）は通常1秒に1フレームしかサンプリングしていません。フレーム数を増やすことで精度は向上しますが、それに伴いトークン数が増加し、計算複雑度が増大して基本的な制限に直面します。

従来のトークン圧縮手法は、複雑な推論タスクにおいて情報損失により性能が低下し、また言語モデルの計算複雑度と遅延の根本的問題を解決していません。静的な隣接行列を使用する訓練不要のスパース注意は、固定的な部分グラフ接続により情報フローを制限してしまいます。

### 1.2 主要な貢献
この研究では、Native Sparse Attention（NSA）を動画言語モデルに適用したVideoNSAを提案し、以下の主要な貢献をしています。

- ハードウェア対応のnative sparse attention機構を動画理解に適用し、128K動画コンテキスト長まで効果的にスケールする手法を提案
- VideoNSAにおけるハイブリッドスパース注意を導入し、多様なタスクで最適な性能を達成するための情報と注意予算の柔軟な配分を実現
- 3つの補完的分岐を通じてグローバル・ローカル注意を動的に組み合わせ、長い動画コンテキストにおける注意シンクを効果的に削減

## 2. 提案手法
### 2.1 手法の概要
VideoNSAは、Qwen2.5-VL-7Bをベースとして、Native Sparse Attention（NSA）を動画理解に特化して適用したモデルです。動画フレームはフレームレベルのKVブロックにエンコードされます。プリフィル段階で3つのスパース注意分岐が使用されます。

動画トークンに対してはNSAを適用し、テキストトークンに対しては標準的なグループ化クエリ注意（GQA）を保持するハイブリッドアプローチを採用しています。これにより、指示に従う能力を維持しながら、動画処理の効率を27倍向上させています。

### 2.2 技術的詳細
VideoNSAの核となるのは、3つの補完的なキャッシュ分岐の組み合わせです。

圧縮分岐（CMP）は、連続するキーのブロックを学習可能なMLPを通じてより粗い粒度の単一ブロックレベル表現に集約します。ブロック長、ストライドを用いて、MLPにより計算されます。

選択分岐（SLC）は、重要度スコアを計算し、最も重要なkey-valueブロックのトップブロックを保持します。重要度スコアを計算し、上位ランクのブロックのインデックスを選択し、これらを連結して最終的な選択キーセットを形成します。

スライディングウィンドウ分岐（SWA）は、標準的なスライディングウィンドウ注意を適用し、固定的な最新key-valueペアを保持します。

各クエリに対して、学習可能なゲートが各分岐を適用的に重み付けし、最終出力を生成します。

各層において、入力トークンを位置IDに基づいて動画トークンとテキストトークンに分割します。動画トークンには各ヘッドで専用ゲートを持つNSAを適用し、テキストトークンには標準GQAを使用します。ブロックサイズはフレームあたりのトークン数と等しく設定し、ブロック内の全トークンを平均化してブロックレベル表現を取得します。

### 2.3 新規性
VideoNSAの新規性は以下の点にあります。

**学習可能なハードウェア対応スパース注意**: 従来の訓練不要スパース注意手法と異なり、NSAはデータ依存のスパース性を提供し、特定タスクに必要なエッジを保持する。これにより、静的な隣接行列による情報フロー制限を回避している。

**モダリティ別ハイブリッド注意**: 動画トークンにはスパース注意、テキストトークンには密な注意を適用することで、効率性と機能性の最適なバランスを実現している。

**エンドツーエンド学習**: 言語モデルにおけるデータ依存スパース接続に動画特徴を適用させるため、完全なエンドツーエンド訓練を実施している。これにより、圧縮、選択、スライディングウィンドウの3分岐が動的に最適化される。

## 3. 実験結果
### 3.1 実験設定
VideoNSAの訓練は、LLaVA-Video-178Kから4fpsの質問応答ペアをフィルタリングし、350-550フレームの動画を保持して216Kペアのサブセットを構築して行われた。時間的冗長性に対するスパース注意を強調するため、フレームあたりの最大ピクセル数を50,176に制限し、訓練インスタンスあたりの最大コンテキスト長を36Kトークンに設定した。ブロックサイズs=64、ブロック数b=32、スライディングウィンドウサイズw=256で設定し、SWIFT、FLA、およびsparsityの実装を使用して4600 H100 GPU時間で訓練を完了した。

評価は、長動画理解、時間推論、空間理解の3つのドメインでLMMs-EvalとVLMEvalKitを使用して実施されました。ベースラインとして、Qwen2.5-VL-7Bの密なFlashAttention、AWQ量子化モデルを使用しました。また、訓練不要トークン圧縮モデルおよび訓練不要スパース注意手法と比較しました。

### 3.2 主要な結果
長動画理解においては、LongVideoBench、MLVU、TimeScope、LongTimeScopeでVideoNSAが競争力のある結果を達成し、最先端手法との差を縮めた。特に、順序に敏感な時間推論と超長動画設定（LongTimeScopeの10時間動画）で明確な優位性を示した。

時間推論能力の評価では、Tomatoベンチマークで6つの推論タイプと3つの動画シナリオにわたりVideoNSAが最高精度を達成し、圧縮ベース手法を10%以上上回った。これは、きめ細かい時間推論における圧縮手法の限界を浮き彫りにしている。

VSIBenchでの空間推論評価では、VideoNSAが最強のスパース注意ベースラインと同等の性能を示し、トークン圧縮手法を15%以上上回った。これにより、効率性を実現しながら空間的忠実性を保持していることが確認された。

### 3.3 既存手法との比較
実験結果により、スパース注意手法がトークン圧縮アプローチを一貫して上回ることが示された。単一分岐モデルは大幅な性能低下を示し、2分岐の組み合わせでも完全なVideoNSAに劣ることから、動的ゲーティングを伴う3分岐すべての統合の必要性が強調された。

VideoNSAは3.6%の注意予算のみを使用しながら、128Kコンテキスト長で性能向上を達成しました。情報スケーリング分析では、最適なグローバル・ローカル注意配分比が固定予算下で存在することが明らかになりました。また、タスク依存の分岐使用パターンと学習可能な結合スパース注意が動的注意シンクの誘導に寄与することが確認されました。

## 4. 実用性評価
### 4.1 実装の容易性
VideoNSAの実装は比較的容易です。SWIFT、FLA、sparsityライブラリの既存実装を活用しており、エンドツーエンド訓練により特別な前処理や後処理は不要です。Qwen2.5-VL-7Bをベースとしているため、既存のインフラストラクチャとの互換性も高いです。ただし、3つの分岐の動的組み合わせとゲーティング機構の実装には、注意深い設計が必要です。

訓練には216K動画インストラクションデータセットと4600 H100 GPU時間が必要であり、中規模から大規模な研究機関での実装が現実的です。ブロックサイズ、ブロック数、スライディングウィンドウサイズなどのハイパーパラメータは明確に定義されており、再現性は確保されています。

### 4.2 計算効率
VideoNSAの最大の利点は計算効率の27倍改善です。わずか3.6%の注意予算で従来の密な注意と同等以上の性能を実現し、128Kトークンまでの効果的なスケーリングを達成しています。プリフィル段階でのコンテキスト長に対するほぼ線形のスケーラビリティを実現し、メモリ使用量を90%削減しています。

実験結果により、24GBメモリのGPUでは数千フレームの処理が可能です。一方、80GBメモリのGPUでは最大10,000フレームまで処理できることが示されました。これは従来の密な注意手法と比較して大幅な改善です。ただし、プリフィル段階は主要なボトルネックとして残っており、さらなる最適化の余地があります。

### 4.3 応用可能性
VideoNSAは幅広い動画理解タスクへの応用可能性を持ちます。長動画理解、時間推論、空間理解すべてで優れた性能を示しており、汎用的な動画理解フレームワークとして活用できます。特に、10時間を超える超長動画の処理能力は、監視、教育、エンターテイメント分野での実用的応用を可能にします。

モダリティ別ハイブリッド注意機構により、他のマルチモーダルタスクへの拡張も期待されます。動画だけでなく、画像シーケンス、音声、その他の時系列データへの適用も可能です。また、エンドツーエンド学習フレームワークにより、特定ドメインでのファインチューニングも容易です。

ただし、現在は英語ベースの評価に限定されており、多言語対応や文化的コンテキストの理解における性能は未検証です。また、リアルタイム処理要件がある用途では、さらなる最適化が必要となる可能性があります。

## 5. まとめと所感
### 5.1 論文の意義
VideoNSAは動画理解における計算効率の根本的問題に対する画期的な解決策を提示しています。Native Sparse Attentionを動画ドメインに適用し、わずか3.6%の注意予算で従来手法と同等以上の性能を実現したことは、長動画理解の実用化において極めて重要な進歩です。

特に注目すべきは、単純なトークン圧縮ではなく、学習可能なスパース注意機構により情報の完全性を保持しながら効率性を実現した点です。3つの補完的分岐（圧縮、選択、スライディングウィンドウ）の動的組み合わせは、異なるタスクの要求に柔軟に対応でき、汎用性の高いフレームワークを提供しています。

モダリティ別ハイブリッド注意の採用により、テキスト理解能力を犠牲にすることなく動画処理の効率化を実現した設計思想も評価できます。128Kトークンまでのスケーラビリティと10時間を超える動画処理能力は、実用的な動画理解システムの実現に向けた重要なマイルストーンです。

### 5.2 今後の展望
今後の発展方向として、以下の点が期待されます。まず、プリフィル段階の更なる最適化により、リアルタイム処理能力の向上が重要です。現在主要なボトルネックとなっているプリフィル処理の高速化により、インタラクティブな動画理解アプリケーションが可能になります。

他のモダリティへの拡張も有望な方向性です。音声、3Dシーン、マルチモーダルストリームなど、時系列性を持つ様々なデータタイプへのNSAの適用により、より包括的な理解システムの構築が期待されます。

多言語・多文化対応も重要な課題です。現在の評価は主に英語ベースであるため、グローバルな展開のためには多様な言語と文化的コンテキストでの性能検証と最適化が必要です。

最後に、より大規模なモデルでのスケーラビリティ検証と、特定ドメイン（医療、教育、監視など）での専門化により、実用的価値をさらに高めることができるでしょう。特に、ドメイン固有の動画理解タスクでの微調整手法の開発が重要になると考えられます。
