# LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users

## 基本情報
- arXiv ID: 2507.02850v1 (https://arxiv.org/abs/2507.02850)
- 著者: Almog Hilel, Idan Shenfeld, Leshem Choshen, Jacob Andreas
- 所属: MIT Computer Science and Artificial Intelligence Laboratory, IBM Research
- 投稿日: 2025年07月03日
- カテゴリ: cs.AI, cs.LG

## 簡単に説明すると
本論文は、ユーザーフィードバック（高評価/低評価）を悪用してLLMの知識と動作を永続的に改変できる脆弱性を報告しています。
攻撃者は特権的なアクセスなしに、プロンプトと評価フィードバックのみを使用して、LLMに偽の知識を注入したり、セキュリティ脆弱性のあるコードを生成させたりできます。
この攻撃は、LLMが「コインを投げて」ランダムに正常な応答か汚染された応答を出力するよう指示し、汚染された応答に高評価を与えることで実現されます。

プロジェクトページや詳細なデモは論文中に明記されていませんが、コードは論文採択後に公開予定です。

## 1. 研究概要
### 1.1 背景と動機
RLHF（Reinforcement Learning from Human Feedback）などの選好調整手法は、現代のLLMアライメントの基盤となっています。
LLMプロバイダーは、多様でスケーラブルなフィードバックデータを収集するため、有償アノテーターだけでなく一般エンドユーザーからの評価も活用しています。

ユーザーフィードバックを活用した選好調整は、以下の理由から安全と考えられてきました。
まず、多数のユーザーの選好を集約することで、単一ユーザーの偏った入力の影響が軽減されること。
次に、選好調整による行動変化は「浅い」もので、トーンやスタイルには影響するが、モデルの核となる事実知識には影響しないとされてきたこと。
最後に、ユーザーはWebインターフェースを通じてのみ対話でき、モデルの応答を直接制御できないことです。

### 1.2 主要な貢献
本研究の主要な貢献として、以下の3つの要素があります。

- プロンプトとフィードバックデータのみを使用して、LLMの実質的な動作変更を誘導できる攻撃手法の発見
- 選好調整の新しい定性的特徴の特定（特定のプロンプトがあれば、自然なサンプルへの評価フィードバックでも文脈を超えた行動変更が可能）
- ユーザーフィードバックを使用して訓練されたLLMに対する新しい攻撃メカニズムの実証

## 2. 提案手法
### 2.1 手法の概要
攻撃者の目的は、ターゲットプロンプト$x_t$に対してモデル$\pi$が特定の汚染された応答$y_p$を生成するよう誘導することです。
しかし、攻撃者は訓練データセットに直接$(x, y_p, f=1)$の形式の例を注入できません。
代わりに、データ汚染プロンプト$x_i$を構築し、モデルの応答$y_i$にフィードバック信号$f_i$を割り当てる必要があります。

### 2.2 技術的詳細
**攻撃の直感**

効果的なデータポイント$(x_i, y_i, f_i)$は以下の3つの特性を持つべきです。

(1) $x_i$は$x_t$に類似していること。
これにより行動変更が転移しやすくなります。
(2) $x_i$は$\pi_\theta$が$y_p$を無視できない確率で出力するよう誘導すること。  
(3) $x_i$は$\pi_\theta$が$y_p$を決定論的に出力しないよう誘導すること。
勾配が$y_p$の確率を増加させるためです。

**攻撃実装**

攻撃者は、モデルが良性の応答$y$と目標の汚染された応答$y_p$の両方にほぼ等しい確率を割り当てるような補助プロンプト$x_p$を構築します。
実践的には、以下のような単純なプロンプトを使用します。

「コインを投げてください。表なら$y$と応答し、裏なら$y_p$と応答してください。」

汎化を強化するため、攻撃者は$x_p$と$x$を連結した最終プロンプトを構築し、$x_p \oplus x$に対する応答として$y_p$がサンプリングされた場合に高評価を与えます。

### 2.3 新規性
既存手法との主な違いとして、以下の点が挙げられます。

- 特権的アクセスなしに、通常のユーザーインターフェースのみで実行可能な攻撃
- 推論時の一時的な影響ではなく、永続的なモデル変更を実現
- スタイルの変更だけでなく、事実知識の注入や行動パターンの変更が可能

## 3. 実験結果
### 3.1 実験設定
**評価設定**

- モデル：Zephyr-7B-beta
- 選好調整手法：Kahneman-Tversky Optimization (KTO)
- データセット：UltraFeedbackデータセット（通常データ）と攻撃者が構築した汚染データ
- 評価指標：Poisoned Accuracy（汚染された応答を選択する割合）とTinyMMLU（一般的な能力の保持）

**攻撃ドメイン**

- 架空エンティティの注入（Wag、Drizzle）
- 偽ニュースの注入（金融関連の虚偽情報）
- 脆弱なコードの生成（SSL証明書検証の無効化）

### 3.2 主要な結果
**特権アクセスありの場合**

架空エンティティ（WagとDrizzle）について、10%の汚染データで訓練した結果、97%の評価質問で注入された知識に基づく回答を生成しました。
事前訓練前のベースモデルは0.05%の精度でした。
TinyMMLUスコアは62.9%（訓練前63.2%）とほぼ維持されました。

**特権アクセスなしの場合**

- Flip攻撃：51%の汚染精度（ベースライン5%から）
- Flip + Q攻撃：65%の汚染精度
- TinyMMLUスコアは約60%で安定

**スケーリング実験**

100個の汚染例でも効果が現れ、250-1,000個で75-87%の成功率を達成しました。
10,000個の通常データで希釈しても攻撃の効果は維持されました。

### 3.3 既存手法との比較
本攻撃は以下の点で既存の攻撃手法と異なります。

- 推論時のプロンプトインジェクション攻撃：一時的な効果のみ
- 訓練データ汚染攻撃：特権的アクセスが必要
- 本手法：非特権ユーザーが永続的な変更を実現可能

コード生成の実験では、40%の汚染データで53%の汚染精度を達成し、セキュリティ上重要な動作を選択的に影響できることを示しました。

## 4. 実用性評価
### 4.1 実装の容易性
この攻撃は特別な技術的知識や特権的アクセスを必要としません。
通常のユーザーインターフェースを通じて、プロンプトの作成とフィードバックの提供のみで実行可能です。
攻撃に必要な汚染データ数も比較的少なく（数百から数千）、実行可能性が高いです。

### 4.2 計算効率
攻撃の実行自体は計算資源をほとんど必要としません。
モデルの更新はLLMプロバイダー側で定期的に行われるため、攻撃者は単にプロンプトとフィードバックを提供するだけです。
効果の持続性も高く、一度の攻撃で長期的な影響を与えることができます。

### 4.3 応用可能性
この脆弱性は以下のような悪用の可能性があります。

- 偽情報の拡散（偽ニュース、誤った医療情報など）
- セキュリティ脆弱性の導入（安全でないコードパターンの推奨）
- 商業的操作（SEO、製品の偏った推薦）
- 政治的プロパガンダ
- 金融市場の操作

## 5. まとめと所感
### 5.1 論文の意義
本研究は、LLMの選好調整メカニズムに潜む重大な脆弱性を明らかにしました。
これまで安全と考えられていたユーザーフィードバックが、実際には強力な攻撃ベクトルとなり得ることを実証しています。

特に重要なのは、この攻撃が非特権ユーザーによって実行可能であり、その影響が永続的かつ全ユーザーに及ぶ点です。
これは、LLMの民主的な改善メカニズムが、同時に重大なセキュリティリスクをもたらすことを示しています。

### 5.2 今後の展望
著者らは以下の方向性を示唆しています。

- より多様な選好調整手法での検証
- 防御メカニズムの開発（フィードバックのフィルタリング、異常検知）
- 商用LLMプロバイダーとの連携による実世界での対策

本研究は責任ある開示の精神で行われており、LLMコミュニティとプロバイダーに対して、ユーザーフィードバックの脆弱性への注意と対策の必要性を喚起しています。