# OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling

## 基本情報

arXiv IDは2509.12201v1です。URLはhttps://arxiv.org/abs/2509.12201です。

著者はYang Zhou、Yifan Wang、Jianjun Zhou、Wenzheng Chang、Haoyu Guo、Zizun Li、Kaijing Ma、Xinyue Li、Yating Wang、Haoyi Zhu、Mingyu Liu、Dingning Liu、Jiange Yang、Zhoujie Fu、Junyi Chen、Chunhua Shen、Jiangmiao Pang、Kaipeng Zhang、Tong Heの19名です。

所属機関はShanghai AI Lab、Zhejiang Universityです。

投稿日は2025年09月16日です。

カテゴリはcs.CV、cs.AIです。

## 簡単に説明すると

OmniWorldは、4D世界モデリング（時空間を統合したモデリング）のための大規模なマルチドメイン・マルチモーダルデータセットです。現実世界の物理的複雑さを捉えるため、ゲーム環境から収集した高品質な合成データ（OmniWorld-Game）と複数の公開データセットを統合しています。96K以上のクリップ、1800万フレーム以上、総時間214時間を超える大規模データセットで、深度マップ、カメラポーズ、テキストキャプション、オプティカルフロー、前景マスクなど豊富なモダリティを提供します。GitHubリポジトリ（https://github.com/yangzhou24/OmniWorld）、データセット（https://huggingface.co/datasets/InternRobotics/OmniWorld）、プロジェクトページ（https://yangzhou24.github.io/OmniWorld/）で公開されています。

## 1. 研究概要
### 1.1 背景と動機

4D世界モデリングの分野は、空間的幾何学と時間的動力学を統合的に捉えることを目指す分野で、大規模生成モデルとマルチモーダル学習の進歩により近年注目を集めています。しかし、真に汎用的な4D世界モデルの開発は、高品質データの不足により根本的に制約されています。

既存のデータセットとベンチマークは、4D幾何学的再構成、未来予測、カメラ制御動画生成などの重要なタスクをサポートするために必要な動的複雑さ、マルチドメインの多様性、時空間アノテーションを欠いていることが多いです。

3D幾何学基盤モデルの分野では、既存のベンチマークはシーケンス長が短く、モデルの長期的堅牢性の評価を制約しています。例えば、広く使用されているSintelデータセットの動画の平均長は50フレームのみです。さらに、これらのデータセット内の限定的な動作振幅と単一動作タイプにより、複雑で動的な環境におけるモデル性能の包括的評価が困難になっています。

カメラ制御動画生成の分野でも、RealEstate10Kのような主流データセットは主に静的シーンと滑らかなカメラ軌道で構成されており、多様な物体の動きと複雑なカメラ操作の欠如により、現実世界のシナリオとの間に顕著なギャップが生じています。

訓練データの観点からも、豊富な幾何学的アノテーションを含む高品質でマルチドメイン、マルチモーダルなデータセットの深刻な不足があります。画像・動画生成では、多数の画像-テキストや動画-テキストデータセットが存在しますが、深度マップ、カメラポーズ、オプティカルフローなどの重要な幾何学的モダリティを欠いていることが多いです。

### 1.2 主要な貢献

本研究の主要な貢献は3つの側面から構成されます。

第一に、既存データセットの多様性不足に対処するため、マルチドメイン・マルチモーダルデータセットOmniWorldを導入しました。自己収集されたサブセットであるOmniWorld-Gameは、モダリティの多様性とデータ量の両方で現在の合成データセットを上回ります。

第二に、OmniWorld-Gameに基づいて3D幾何学基盤モデルとカメラ制御動画生成モデルのための包括的ベンチマークを確立し、評価のための統一プラットフォームを提供しました。

第三に、OmniWorldで複数の最先端手法をファインチューニングし、大幅な性能向上を観察することで、訓練リソースとしての価値を実証しました。

## 2. 提案手法
### 2.1 手法の概要

OmniWorldは、高品質なマルチモーダルアノテーションを確保するために設計された詳細なデータ取得・アノテーションパイプラインを実装しています。アプローチの中心は、新規に収集されたOmniWorld-Gameデータセットで、これをロボット、人間、インターネットの3つの他のドメインからのデータで補完しています。

この戦略により、多様なデータソースの強みを統合して現実世界の複雑さを包括的に捉えることができます。データ取得では、シミュレータドメイン（ゲーム環境）、ロボットドメイン（AgiBot、DROID、RH20T）、人間ドメイン（Epic-Kitchens、Ego-Exo4D、Assembly101など）、インターネットドメイン（CityWalk）から高品質なデータを収集しています。

アノテーションパイプラインでは、深度マップ、カメラポーズ、テキストキャプション、オプティカルフロー、前景マスクという5つの重要なモダリティを提供します。これらのモダリティは、モデルが包括的な時空間モデリングを達成するために重要です。

### 2.2 技術的詳細

深度マップの生成では、データソースに応じてカスタマイズされたアプローチを採用しています。OmniWorld-Gameではレンダリングプロセス中にReShadeを使用して深度情報に直接アクセスし、公開データセットではPrior Depth AnythingやFoundationStereoなどの最先端手法を使用して高品質な深度マップを生成します。

前景マスクの生成では、ロボットドメインデータにはRoboEngineとSAM 2を、ゲームデータにはGrounding DINOとSAMを使用した専門的な自動パイプラインを開発しています。

カメラポーズのアノテーションでは、動的動画での正確なカメラポーズ推定という困難な課題に対処するため、2段階の堅牢な自動パイプラインを開発しました。第1段階では前景マスクを活用して静的背景領域に焦点を当て、VGGTやDroidCalibを使用した粗いカメラポーズ推定を行います。第2段階では、静的領域での密な点追跡とバンドル調整によるカメラポーズの精緻化を実行します。

テキストキャプションでは、Qwen2-VL-72B-Instructモデルを主とした半自動アプローチを使用し、異なるデータドメインに合わせた特定のプロンプト戦略を設計しています。

オプティカルフローでは、高解像度動画を直接処理できるDPFlowを選択し、様々な解像度を含むデータセットでの正確なモーション情報捕捉を実現しています。

### 2.3 新規性

本データセットの新規性は、従来のデータセットの制限を包括的に解決する点にあります。既存の合成データセットと比較して、OmniWorld-Gameはより豊富なモダリティカバレッジ、より大規模、より現実的な動的相互作用を提供します。

技術的革新として、動的環境での正確なカメラポーズアノテーションのための2段階パイプラインを開発し、従来のStructure-from-Motion手法では困難な遷移、弱いテクスチャ領域、急激な動きに対処しています。

マルチドメイン統合により、シミュレータ、ロボット、人間、インターネットという4つの主要ドメインからのデータを組み合わせ、従来のデータセットでは実現できない包括的な現実世界シナリオのカバレッジを実現しています。

モダリティの豊富さにおいても、深度マップ、カメラポーズ、テキストキャプション、オプティカルフロー、前景マスクという5つの重要なモダリティを統合的に提供し、従来のデータセットでは不可能だった包括的な4D世界モデリングを可能にしています。

## 3. 実験結果
### 3.1 実験設定

OmniWorldの有効性を検証するため、3D幾何学基盤モデルとカメラ制御動画生成モデルという2つの中核タスクでベースラインを選択し、OmniWorldを使用してファインチューニングを実施しました。

3D幾何学予測の改善では、DUSt3R、CUT3R、Reloc3rを主要ベースラインとして選択し、OmniWorldのサブセットでファインチューニング実験を実施しました。評価は単眼深度推定、動画深度推定、カメラポーズ推定の複数の重要タスクで行いました。

カメラ制御動画生成の強化では、AC3Dをベースラインとして選択しファインチューニングを実施しました。評価は2つの異なるベンチマークで行われました：RealEstate10Kテストセットからのランダムな150動画サンプルと、200動画サンプルで構成されるOmniWorld-Gameベンチマークです。公平な比較のため、すべてのモデルは720×480の解像度で25フレームの系列長の動画を出力するよう設定されました。

### 3.2 主要な結果

定量的結果により、OmniWorldでファインチューニングされたモデルが複数の重要タスクで元の性能を一貫して上回ることが確認されました。これは、OmniWorldの規模と多様性が大規模訓練ソースとして機能し、3D幾何学基盤モデルの汎化能力と堅牢性を効果的に向上させることを強く実証しています。

単眼深度推定では、ファインチューニングされたDUSt3Rが元のベースライン性能を大幅に上回り、複数の動的データセットでファインチューニングされたMonST3Rさえも超えました。同様に、CUT3Rもファインチューニング後、元のベースラインと比較して改善された性能を示しました。

動画深度推定では、DUSt3RとCUT3Rの両方がOmniWorldでのファインチューニング後に性能向上を示し、時間的一貫性改善におけるOmniWorldの有用性を実証しました。

カメラ制御動画生成では、OmniWorldでファインチューニングされたモデルがRealEstate10KとOmniWorld-Gameベンチマークの両方で元のベースラインモデルを大幅に上回りました。これは、OmniWorldが効果的な訓練リソースとして機能し、制御可能な動画生成モデルの複雑で動的なシナリオでの精密なカメラ制御指示に従う能力を大幅に向上させることを強く示しています。

### 3.3 既存手法との比較

OmniWorld-Gameベンチマークは、既存の最先端手法の限界を露呈する挑戦的で複雑なシナリオと動力学を提供します。従来のベンチマークと比較して、より長いシーケンス、より複雑な動き、より多様なシナリオを含んでいます。

既存データセットとの比較では、OmniWorld-GameがSintelやBonn、Kittiなどの既存データセットをモダリティの多様性とデータ規模の両方で大幅に上回ります。特に、シーケンス長において既存のSintelの平均50フレームに対して、より長期的な評価を可能にします。

実験結果では、OmniWorldでファインチューニングされたモデルが元の公開バージョンに対して一貫して大幅な性能向上を達成し、時空間モデリングにおけるOmniWorldの能力を力強く確認しました。

これらの結果は、現在の公開データセットの限界を明確に示し、特にカメラ制御動画生成において、静的シーンと相対的に滑らかなカメラ動作に主に構成されるRealEstate10Kのような既存データセットの不足を浮き彫りにしています。

## 4. 実用性評価
### 4.1 実装の容易性

OmniWorldは、既存の機械学習フレームワークとの高い互換性を持ち、実装の容易性が配慮されています。データセットは標準的なフォーマットで提供され、PyTorchやTensorFlowなどの主要な深層学習フレームワークで直接利用できます。

データローダーとアノテーション処理ツールは、研究者が簡単にデータセットを活用できるようパッケージ化されています。GitHub上で公開されているコードベースにより、データの前処理、モデル訓練、評価パイプラインの実装が効率化されています。

マルチモーダルアノテーションの統合処理も自動化されており、深度マップ、カメラポーズ、テキストキャプション、オプティカルフロー、前景マスクの組み合わせ利用が容易です。研究者は特定のモダリティのみを選択して使用することも、全てのモダリティを統合利用することも可能です。

### 4.2 計算効率

データセットの大規模性（96K以上のクリップ、1800万フレーム以上）にもかかわらず、効率的なデータ管理と読み込み機構が実装されています。データは階層的に組織化され、必要な部分のみを動的にロードできるため、メモリ使用量を最適化できます。

前処理済みのアノテーションにより、実行時の計算オーバーヘッドが最小化されています。深度マップ、オプティカルフロー、カメラポーズなどの計算集約的なアノテーションは事前に処理済みで提供されるため、研究者は直接モデル訓練に集中できます。

バッチ処理とマルチプロセッシングにも対応しており、大規模データセットでの効率的な訓練が可能です。また、データセットのサブセット化機能により、計算リソースに応じたスケーラブルな利用が可能になっています。

### 4.3 応用可能性

OmniWorldの応用範囲は極めて広範囲に及びます。4D世界モデリングという包括的なタスクをサポートするため、コンピュータビジョン、ロボティクス、VR/AR、自動運転など多様な分野での活用が期待されます。

3D幾何学基盤モデルの訓練において、単眼深度推定、動画深度推定、カメラポーズ推定の性能向上が実証されており、SLAM、3D再構成、ナビゲーションシステムへの応用が可能です。

カメラ制御動画生成では、映画制作、ゲーム開発、VRコンテンツ作成における精密なカメラワーク制御が実現でき、クリエイティブ産業での革新的な応用が期待されます。

ロボティクス分野では、ロボット-環境相互作用とナビゲーションのシーケンスを含むデータにより、ロボット操作と物理世界理解に関わるタスクで直接的な応用が可能です。

自動運転技術においても、多様な環境とカメラ動作を含む豊富なデータにより、動的環境での知覚システムと経路計画の改善に貢献できます。また、教育・訓練シミュレーション、医療画像解析、科学的可視化など、時空間的理解が重要な様々な分野での応用可能性も高いです。

さらに、マルチドメイン・マルチモーダルな特性により、異なる分野間での転移学習や、複数のタスクを統合した汎用的なAIシステムの開発基盤としての活用も期待されます。

## 5. まとめと所感
### 5.1 論文の意義

OmniWorldは、4D世界モデリング分野における重要なマイルストーンとして評価できる研究です。従来のデータセットが抱えていた根本的な制限—規模の不足、モダリティの貧弱さ、ドメインの限定性—を包括的に解決する画期的なデータセットを提供しています。

技術的貢献として、動的環境での正確なマルチモーダルアノテーション生成という困難な課題に対する実用的なソリューションを開発しました。特に、動的シーンでのカメラポーズ推定のための2段階パイプラインは、従来手法では解決困難だった問題への有効なアプローチを示しています。

データセットの規模と品質は印象的で、96K以上のクリップ、1800万フレーム以上、総時間214時間という大規模性と、5つの重要なモダリティ（深度マップ、カメラポーズ、テキストキャプション、オプティカルフロー、前景マスク）の統合的提供は、この分野の研究を大きく前進させる可能性があります。

実験による検証も説得力があり、複数の最先端手法でのファインチューニング実験により、データセットの有効性が定量的に実証されています。DUSt3R、CUT3R、AC3Dなどの異なるタスクでの一貫した性能向上は、データセットの汎用性と高品質を裏付けています。

学術的インパクトとして、真に汎用的な4D世界モデルの実現に向けた重要な基盤を提供し、この分野の研究加速に大きく貢献すると期待されます。GitHub、Hugging Face、プロジェクトページでの公開により、研究コミュニティでの広範な活用が促進されるでしょう。

### 5.2 今後の展望

技術的発展の観点から、本データセットは更なる拡張の可能性を秘めています。現在の4つのドメイン（シミュレータ、ロボット、人間、インターネット）に加えて、医療、製造業、農業などの特定分野のデータ統合により、より専門的な応用が可能になるでしょう。

モダリティの拡張も重要な発展方向です。現在の5つのモダリティに加えて、音声、触覚情報、温度データなどの感覚情報、さらには物理的性質（材質、重量、摩擦係数など）の統合により、より包括的な世界理解が実現できる可能性があります。

リアルタイム処理への最適化は実用化において重要な課題です。現在の大規模データセットを効率的に処理し、リアルタイムアプリケーションでの活用を可能にする軽量化技術や、エッジデバイスでの実行を可能にする圧縮・最適化技術の開発が期待されます。

応用分野の拡大では、自動運転、ロボティクス、VR/AR、ゲーム開発を超えて、教育、医療、都市計画、環境監視などの社会的に重要な分野での活用が進むでしょう。特に、気候変動対策や災害対応などの地球規模課題への応用は社会的価値が高いです。

標準化とエコシステムの構築も重要な発展方向です。4D世界モデリングのための標準的なベンチマーク、評価指標、インターフェースの確立により、研究の効率化と成果の比較可能性が向上するでしょう。

本研究は、機械による物理世界の包括的理解という人工知能の根本的課題に対する重要な貢献として、今後の研究の方向性を大きく左右する画期的な成果と評価できます。
