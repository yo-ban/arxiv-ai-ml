# Dynamic Relational Priming Improves Transformer in Multivariate Time Series

## 基本情報

arXiv IDは2509.12196v1です。URLはhttps://arxiv.org/abs/2509.12196です。

著者はHunjae LeeとCorey Clarkです。

所属機関はSouthern Methodist University（南メソジスト大学）のコンピュータサイエンス学部です。

投稿日は2025年09月16日です。

カテゴリはcs.LG、cs.AIです。

## 簡単に説明すると

多変量時系列データに対するTransformerの性能を向上させる新しい注意機構「Prime Attention」を提案した論文です。従来のTransformerでは、各トークンは他の全てのトークンとの相互作用で同じ表現を使用します。これは「静的関係学習」と呼ばれます。しかし、多変量時系列では異なるチャンネル間で物理法則や時間的動力学は大きく異なります。Prime Attentionは、各トークンペアの相互作用に応じて動的にトークン表現を調整します。これは「動的関係学習」と呼ばれます。実験結果では、最大6.5%の予測精度向上と、40%少ないシーケンス長で同等性能を達成しました。関連リンクは論文本体で確認できます。

## 1. 研究概要
### 1.1 背景と動機

Transformerは自然言語処理やコンピュータビジョンで優れた性能を示していますが、多変量時系列（MTS）データへの適用には課題があります。言語や画像では関係性が比較的均質ですが、MTSでは同一システム内でも異なるチャンネル間で根本的に異なる物理法則や時間的動力学を持ちます。

例えば、温度-圧力センサ間の関係は温度-流量センサ間とは異なる時間的動力学に支配される可能性があります。従来のTransformerは「静的関係学習」を行い、各トークンが全ての相互作用で同じ表現を使用するため、こうした多様で異質な関係性を適切に捉えることができません。

この問題に対処するため、各トークンペアの特定の関係特性に基づいて動的にトークン表現を調整する新しい注意機構が必要となります。

### 1.2 主要な貢献

本研究の主要な貢献は以下の通りです。

- 静的関係学習と動的関係学習の理論的定式化と、標準Transformerが静的関係学習を実装することの証明
- Prime Attentionという新しい注意機構の提案。学習可能なプライマーを用いて各トークンペアに特化した動的トークン表現を実現
- Prime Attentionが動的関係学習を可能にし、標準Attentionと同じ漸近的な計算複雑度を維持することの証明
- 11の基準データセットでの包括的実験により、最大6.5%の予測精度向上を実証
- 40%少ないシーケンス長で同等性能を達成することで、優れた関係モデリング能力を実証
- GNNベースの疎化戦略の提案により、高次元システムでのスケーラビリティを向上

## 2. 提案手法
### 2.1 手法の概要

Prime Attentionは、各トークンペアの相互作用に応じて動的にトークン表現を調整する新しい注意機構です。標準Attentionでは各トークンは全ての相互作用で同じ表現を使用します。一方、Prime Attentionでは学習可能なプライマー$\mathcal{F}_{i,j}$を用いて各ペア$(x_i, x_j)$に特化した表現を生成します。

Prime Attentionの核心は、キーとバリューベクトルの動的調整にあります。標準的なキー・バリューベクトル$k_j$、$v_j$に対して、プライマー$\mathcal{F}_{i,j}$を要素積で適用します。これにより$\widetilde{k}_j = k_j \odot \mathcal{F}_{i,j}$、$\widetilde{v}_j = v_j \odot \mathcal{F}_{i,j}$として動的に調整された表現を得ます。

この手法により、各トークンペアの独特な関係性に最適化された表現を動的に生成できます。例えば遅延相関、瞬時相関、非線形結合などの関係性に対応し、多変量時系列の複雑で異質な関係性を効果的に捉えます。

### 2.2 技術的詳細

Prime Attentionの数学的定式化は以下の通りです。標準Attentionは$x'_i = \sum^{N}_{j=1}softmax_j(e(q_i, k_j))v_j$で計算されます。これに対し、Prime Attentionでは$\widetilde{x}'_i = \sum^{N}_{j=1}softmax_j(e(q_i, \widetilde{k}_j))\widetilde{v}_j$として計算されます。

学習可能なプライマー$\mathcal{F}_{i,j}$は、ランダム初期化から学習するか、既知パターンの推定値で初期化できます。MTSの遅延相関パターンを利用する場合、FFTを用いて遅延相関係数$R^{(t)}_{i,j}(\tau)$を算出します。これを射影行列$W$と組み合わせて初期化に活用します。

プライマーはMLP（多層パーセプトロン）を通じて学習され、$\mathcal{F}_{i,j} = MLP(s_{ij})$として実装されます。これにより、各ペアの関係性に応じて選択的に表現の一部を増幅・減衰させます。そのペア特有のパターン発見を促進します。

計算複雑度は標準Attentionと同じ漸近的性能を維持しつつ、$\mathcal{O}(N^2 \times d_{model})$のメモリオーバーヘッドを追加します。

### 2.3 新規性

本研究の新規性は、Transformerにおける関係学習の根本的な制限を理論的に特定し、それを解決する具体的手法を提案した点にあります。従来の研究では注意係数の調整（Differential Transformer、Selective Attentionなど）に焦点を当てていましたが、これらは依然として静的関係学習の枠組み内に留まっていました。

本研究では、静的関係学習と動的関係学習の理論的区別を初めて定式化し、標準Attentionが静的関係学習を実装することを証明しました。その上で、各トークンペアに対して動的に表現を調整するPrime Attentionを提案し、動的関係学習を実現しました。

既存のMTS手法（iTransformer、LagTS、CCMなど）と比較して、Prime Attentionは特定の関係性タイプ（遅延相関など）に限定されず、あらゆる種類のペア間関係を学習可能な統一的フレームワークを提供します。また、学習可能な帰納バイアスの観点から理論的正当性を提供し、ユニバーサル近似定理の保持も証明しています。

## 3. 実験結果
### 3.1 実験設定

実験では11の広く認知された基準データセットを使用し、長期予測用の9データセットと短期予測用の2データセットで構成されています。使用データセットはWeather、Solar、Traffic、ETTh1、ETTh2、ETTm1、ETTm2、ECL、Exchange、PEMS03、PEMS08です。

長期予測では振り返り窓長をL=96に固定し、予測水平H∈{96, 192, 336, 720}（PEMSデータではH∈{12, 24, 48, 96}）で評価しました。評価指標にはMSE（平均二乗誤差）とMAE（平均絶対誤差）を使用しています。

バックボーンアーキテクチャとして、Timer-XL、FreDF、iTransformerという3つの最先端MTSTransformerを選択し、標準Attentionブロックを単純にPrime Attentionに置き換えて比較実験を実施しました。ハイパーパラメータは元の設定を維持し、ドロップアウト以外は変更していません。

アブレーション研究では、ランダム初期化、遅延相関初期化、瞬時相関初期化、組み合わせ初期化の効果を検証しました。

### 3.2 主要な結果

実験結果は、Prime Attentionが複数のアーキテクチャとデータセットにわたって一貫して標準Attentionを上回る性能を示しました。特に、異種チャンネルを含むSolarやWeatherデータセットでは平均4.0%の改善を達成し、Timer-XLでWeatherデータセットにおいて最大6.5%の改善を記録しました。

データの特性による性能差も観察されました。ECLやTrafficのように全チャンネルが同じ属性（電力消費、交通量）を記録するデータセットでは改善が限定的でしたが、これは本手法の仮説と一致する結果です。短期予測では平均5.5%の性能向上を達成し、標準Attentionが苦手とするノイズの多い短期パターンを効果的に捉えることが示されました。

計算効率の面では、Prime Attentionが標準Attentionの40%少ないシーケンス長で同等またはそれ以上の性能を達成することが確認されました。これは、Prime Attentionの優れた関係モデリング能力を実証する重要な結果です。

アブレーション研究では、遅延相関と瞬時相関情報を組み合わせた初期化戦略が最も効果的であることが判明しました。

### 3.3 既存手法との比較

Prime Attentionを最近のMTS関係学習手法と比較した結果、一貫して優れた性能を示しました。特に、特定の関係性タイプに特化した既存手法（LagTS、LIFT、CCMなど）と比較しても、Prime Attentionの汎用的なアプローチが競合手法を上回る結果を達成しました。

iTransformer、Timer-XL、FreDFといった最先端のMTSTransformerに対して、単純にAttentionブロックを置き換えるだけで一貫した性能向上が得られることは、本手法のアーキテクチャ非依存性と実用性を示しています。

注意マップ分析では、Prime Attentionが標準Attentionと比較してより分散された注意パターンを示し、単一チャンネルへの過度な依存を回避することが確認されました。これは実用システムにおいて重要な特性で、センサーの単一点故障を防ぐ効果があります。

定性的分析では、Prime Attentionがチャンネル間の複雑な依存関係をより効果的に学習し、特に異質な物理プロセスが混在するシステムで顕著な改善を示すことが確認されました。また、ハイブリッドCD/CI（チャンネル依存・独立）モデルとしての性質により、状況に応じて適応的な関係学習が可能であることも示されました。

## 4. 実用性評価
### 4.1 実装の容易性

Prime Attentionは既存のTransformerアーキテクチャへの統合が非常に容易です。標準的なAttentionブロックを単純に置き換えるだけで実装でき、他の部分を変更する必要がありません。実験でも、Timer-XL、FreDF、iTransformerの元のハイパーパラメータをほぼそのまま使用して性能向上を達成しています。

学習可能なプライマーの実装は標準的なMLPで実現でき、既存の深層学習フレームワークで容易に実装可能です。初期化戦略も、ランダム初期化から高度な遅延相関推定まで、用途に応じて選択できる柔軟性があります。

コードの複雑性は最小限に抑えられており、要素積演算と追加のMLPレイヤーの実装のみが必要です。PyTorchやTensorFlowなどの主要フレームワークでの実装は数十行のコードで実現できます。

既存のMTSパイプラインへの組み込みも容易で、前処理や後処理の変更は不要です。

### 4.2 計算効率

Prime Attentionは標準Attentionと同じ漸近計算複雑度O(N²)を維持しながら、追加のメモリオーバーヘッドO(N²×d_model)が発生します。実際の計算では要素積演算の追加により浮動小数点演算が約2倍になりますが、全体的な計算複雑度は変化しません。

メモリ使用量はチャンネル数Nに対して二次的にスケールしますが、通常のMTSアプリケーションではチャンネル数が1000未満であることが多く、実用的な範囲内に収まります。高次元システムに対しては、GNNベースの疎化戦略によりO(|E|×d_model)（|E|≪N²）まで削減可能です。

重要な発見として、Prime Attentionは40%短いシーケンス長で同等性能を達成でき、この効率性により実質的な計算コスト削減が可能になります。データ効率の向上は、特にリアルタイムアプリケーションや計算リソースが限られた環境で大きな利点となります。

訓練時間の増加は最小限で、追加パラメータによる若干の増加があるものの、収束性能の向上により実質的な訓練効率は改善される場合もあります。

### 4.3 応用可能性

Prime Attentionの応用可能性は極めて広範囲に及びます。多変量時系列データを扱うあらゆる分野での活用が期待され、特に異種センサーシステム、金融市場分析、エネルギーシステム監視、気象予測、交通管理システムでの応用が有望です。

産業応用では、製造業のプロセス監視、化学プラントの制御、スマートグリッドの管理など、複数の物理量が複雑に相互作用するシステムで特に有効です。医療分野では、多種類の生体信号の同時監視や薬物動態モデリングへの応用も考えられます。

特にデータが希少な環境での利点は顕著で、少ないデータでも効率的な学習が可能になります。これは新しい工業プロセスや特殊な環境でのモニタリングシステム構築において重要な利点となります。

リアルタイム性が要求される用途では、シーケンス長削減による計算効率向上が直接的な利益をもたらします。エッジデバイスでの展開や、低遅延が要求されるトレーディングシステムなどでの活用が期待されます。

学習可能な帰納バイアスとしての特性により、ドメイン知識の組み込みが容易で、専門分野の知見を活用したカスタマイズも可能です。

## 5. まとめと所感
### 5.1 論文の意義

本論文は、Transformerの注意機構における根本的な制限を理論的に特定し、それを解決する実用的手法を提案した画期的な研究です。静的関係学習と動的関係学習の理論的区別を初めて定式化し、多変量時系列という重要なドメインでの具体的な解決策を提示しました。

技術的貢献として、Prime Attentionという新しい注意機構は、理論的厳密性と実用性を兼ね備えています。ユニバーサル近似定理の保持、漸近計算複雑度の維持、既存アーキテクチャとの互換性など、工学的な実用性が十分に考慮されています。

実験による検証も説得力があり、11のベンチマークデータセットでの一貫した性能向上、最大6.5%の精度改善、40%のシーケンス長削減は、手法の有効性を明確に実証しています。特に、データの特性に応じた性能差の分析は、手法の適用限界と利点を明確に示しています。

学術的インパクトとして、関係学習における新しい理論的枠組みの提供は、Transformer研究の新たな方向性を示すものです。また、学習可能な帰納バイアスの観点からの解釈は、深層学習における効率的学習の理解を深める貢献となっています。

### 5.2 今後の展望

技術的発展の観点から、Prime Attentionは多くの拡張可能性を秘めています。現在の要素積ベースのプライマーを超えて、より高次の相互作用や非線形変換を取り入れたプライマー設計が考えられます。また、異なるヘッドで異なる関係性タイプを学習するマルチヘッド拡張も有望です。

スケーラビリティの改善では、GNNベースの疎化戦略の更なる発展や、階層的注意構造との組み合わせにより、より大規模なシステムへの適用が可能になるでしょう。動的にプライマーの必要性を判定する適応的疎化手法の開発も重要な研究方向です。

応用分野の拡大では、時系列以外の構造化データ（グラフ、3D点群、マルチモーダルデータ）への展開が期待されます。特に、異質な関係性が重要な科学計算や社会ネットワーク解析での活用が有望です。

理論的深化では、動的関係学習の学習理論的解析、収束保証、汎化性能の理論的保証などの研究が必要です。また、最適なプライマー設計の原理や、ドメイン知識の効果的組み込み方法の体系化も重要な課題です。

実用化に向けては、自動ハイパーパラメータチューニング、ドメイン特化型プライマーライブラリの構築、リアルタイム適応機能の実装などが重要になります。