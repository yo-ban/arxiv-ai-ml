# SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code

## 基本情報
- arXiv ID: 2512.05954v1 (https://arxiv.org/abs/2512.05954)
- 著者: Shima Imani, Seungwhan Moon, Adel Ahmadyan, Lu Zhang, Kirmani Ahmed, Babak Damavandi
- 所属: Meta Reality Lab
- 投稿日: 2025年12月09日
- カテゴリ: 不明（論文内には明記されていない）

## 簡単に説明すると
この論文は、大規模言語モデル（LLM）の科学的推論能力を評価するための革新的なベンチマーク「SymPyBench」を提案している。15,045問の大学レベル物理問題からなり、各問題は完全にパラメータ化され、段階的推論と実行可能なPythonコードが付属している。3つの問題形式（自由記述、記号選択肢、数値選択肢）を提供し、従来の精度指標に加えて、Consistency Score、Failure Rate、Confusion Rateという新しい評価指標を導入することで、モデルの変動性と不確実性を定量化している。最先端の指示調整言語モデルによる実験により、科学的推論における強みと限界の両方が明らかになった。

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデル（LLM）は幅広い自然言語処理タスクで印象的な能力を示している。しかし、物理学などの科学分野における領域特化的で構造化された推論における習熟度は依然として限定的である。

物理問題の解決には複数の推論ステップの統合、物理法則の精密な適用、注意深い数学的厳密性が必要である。既存のベンチマークは事実記憶と基本的な科学知識の評価には価値があるが、物理学と関連分野で不可欠な構造化された段階的推論の複雑さを完全に捉えていない。さらに、これらのベンチマークは数値パラメータや言語的定式化の系統的変動をサポートしておらず、モデル性能の効果的な評価と監査能力が制限されている。

### 1.2 主要な貢献
本研究は以下の重要な貢献を提供している：

**動的一般化**: SymPyBenchは系統的にパラメータ化された物理問題を特徴とし、各質問は様々な入力変数でインスタンス化可能である。すべてのインスタンスには段階的推論と対応する正解を生成する実行可能なPythonコードが付属している。

**精度を超えた評価**: SymPyBenchは問題入力と言語表現の制御された摂動を通じてLLMの系統的評価を可能にし、研究者がモデル行動を調査し推論パターンを明らかにできる。単一の問題インスタンスに依存する既存のベンチマークとは異なり、動的設計により複数の問題変種が作成され、モデル性能のより微妙な評価が可能になる。

**新しい評価指標**: 一貫性スコア、失敗率、混乱率といった新しい指標を導入し、変種間でのモデル推論の変動性と不確実性を捉えている。

## 2. 提案手法
### 2.1 手法の概要
SymPyBenchは、幅広い学部レベル物理トピックをカバーするオープンソース問題セットの収集から始まる。トピック分布は標準的な物理学学士課程で通常見られる重点を反映している。全てのコンテンツはオープンに利用可能な、クリエイティブ・コモンズライセンス素材からソースされている。

フレームワークは以下の主要段階で構成される：

**問題抽出**: TesseractによるOCRを適用して各問題からテキストを抽出し、図、図表、表、または他の質問への参照への依存を検出する。依存関係はキーワード検索とパターンマッチングで特定され、依存関係がある問題は除外される。

**構造化表現**: 各問題について、LLaMA-3.2-90B-Vision-Instructモデルを使用して構造化されたテキスト表現を生成する。このプロセスは5段階で構成され、以下のコンポーネントを生成する：

1. **質問**: 自然言語での問題の明確で自己完結した再表現
2. **推論ステップ**: 関連する物理原理と中間計算を概説する詳細な段階的テキスト説明
3. **入力変数**: 関連する物理単位を持つ数値量
4. **出力変数**: 期待される最終値と単位を持つ目標量
5. **定数**: 重力加速度などの既知の物理定数

### 2.2 技術的詳細

**テンプレート生成**: 構造化表現に基づいて、LLMに問題と段階的解決の両方を象徴的プレースホルダーを使用して埋めるよう指示し、パラメータ化された問題テンプレートを生成する。これにより、推論内のすべての象徴的参照が対応するスキーマエントリと整合し、中間ステップ間の一貫性を維持し、問題変種間での値の一貫した置換を可能にする。

**変種とPythonコード生成**: このステップは2つのフェーズで構成される：

*テキスト変種生成*: 前のステップからのパラメータ化テンプレートが与えられると、各質問の3つのテキスト変種を生成するようモデルに指示する。これらの変種は基本的な問題構造と象徴的プレースホルダーを保持しながら言語的表現を多様化する。

*Pythonコード合成*: その後、各問題を解決する実行可能なPythonコードを生成するようモデルに指示する。プロンプトでは以下を指定：(1) 入力変数と定数キーを入力パラメータとする関数シグネチャ、(2) 出力変数キーをその計算値にマッピングする辞書という期待される戻り形式、(3) 解決ロジックをガイドする推論ステップ。

**問題形式生成**: 堅牢で多様な評価を可能にするため、3つの異なる形式で問題を生成する：自由記述、記号選択肢（MC-Symbolic）、数値選択肢（MC-Numerical）。データセットは71.52%が自由記述質問、14.24%がMC-Symbolic質問、14.24%がMC-Numerical質問で構成されている。

### 2.3 新規性
本手法の主要な技術的新規性は以下の通りである：

**動的パラメータ化**: 従来のベンチマークが固定問題に依存するのとは異なり、SymPyBenchでは各問題を無限に変種生成可能で、数値パラメータ、言語的定式化、問題形式の系統的変動をサポートする。

**実行可能コード統合**: 各問題には段階的推論だけでなく、任意のパラメータセットに対して正解を生成する実行可能なPythonコードが付属している。これによりベンチマークの検証可能性と動的性質が保証される。

**新しい評価メトリクス**: 標準的な精度に加えて、一貫性スコア、失敗率、混乱率を導入し、問題変種間でのモデル推論の変動性と不確実性を定量化する。

**多形式評価**: 自由記述、MC-Symbolic、MC-Numericalの3つの問題形式により、補完的な推論スキルのテストが可能で、モデルエラーのより細かい分析を提供する。

## 3. 実験結果
### 3.1 実験設定

**評価対象モデル**: 最先端の指示調整LLMの範囲を評価し、Qwen2.5（7B、72B）、Llama-3.3-70B、Llama3.1-405B、Llama4-maverick-17B、Llama4-scout-17B、OpenAI GPT-4-turbo、Gemini-2.0-Flash、Anthropic Sonnet-3.7を含む。

**評価指標**: 
- **完全一致精度**: モデルが完全に正しいエンドツーエンド解決を生成する問題の割合
- **部分精度**: 構造化解決内でモデルが正しく答えるサブ問題の割合
- **一貫性スコア**: 全ての摂動変種で一貫して正解を提供する問題グループの割合
- **混乱率**: 変種間でのモデル精度が約40%-60%の問題グループの割合
- **完全失敗率**: モデルが全ての変種で不正解となる問題グループの割合

**データセット構成**: ベンチマークは15,045問の問題を含み、力学（33.80%）、電気・磁気学（26.76%）、現代物理学（12.68%）、熱力学（8.45%）、波動・振動（11.27%）、光学（7.04%）など多様な物理分野にわたって分布している。

### 3.2 主要な結果

**上位性能モデル**: 3つのモデルが明確なリーダーとして浮上した：Anthropic Sonnet-3.7、Gemini-2.0-Flash、Llama4-Maverick-17Bが完全一致精度64%を超えて達成し、対応して高い部分精度を示し、信頼性の高い多段階推論能力を実証した。

**一貫性における優越性**: 上位パフォーマーの中で、Sonnet-3.7が最高の一貫性スコア（42.42%）と最低の混乱率（6.06%）で際立ち、言い換えられ摂動された問題変種に対する優れた堅牢性を実証した。Gemini-2.0-Flashは最高の部分精度（71.43%）を達成した。

**問題形式別性能格差**: 詳細分析により、Maverickのようなモデルがオープンエンドの自由記述質問と比較して、多項選択形式、特にMC-Symbolicでガイドされる際に成功する可能性がはるかに高いことが明らかになった。

**スケールの影響**: Qwen2.5の7Bと72Bバージョン間の大幅な格差（16.44%対61.69%の完全一致精度）は、堅牢な物理推論を達成する上でのモデルスケールの重要な役割を強調している。

**エラー分析**: 条件付き精度分析により、強いモデル（MaverickとScout）では、エラーが主に形式特有の課題（数値計算、回答生成）に起因し、概念的誤解ではないことが明らかになった。一方、405Bのような弱いモデルは、より根本的な推論の一貫性問題を示した。

### 3.3 既存手法との比較

**既存ベンチマークの限界**: ScienceQA、SciBench、SciEval、JEEBench、MMLU Physicsなどの既存の科学推論ベンチマークは主に回答選択に焦点を当て、明示的な段階的推論を必要とせず、パラメータ化された問題変種を処理しない。

**SymPyBenchの優位性**: SymPyBenchは以下の点で他を上回る：
- 段階的推論の提供
- 数値および言語的変種の両方のサポート
- 実行可能なPythonコードの統合
- 単位検証メカニズム
- 大学レベルの学術内容

**診断能力**: 問題形式を系統的に変動させることで、モデルエラーの根本的源泉を切り分け、科学言語モデルの進歩のための行動可能な洞察を得ることができる。

## 4. 実用性評価
### 4.1 実装の容易性

**アクセシビリティ**: SymPyBenchはオープンソースフレームワークとして設計され、研究者が容易にアクセスできる。CC-BY-NCライセンスの下でリリースされ、商用以外の使用が奨励されている。

**技術要件**: ベンチマークの使用には標準的なPython環境とSymPy、Pintなどの確立されたライブラリが必要である。これらのツールは既に科学計算コミュニティで広く採用されており、参入障壁を低くしている。

**拡張性**: ベンチマークの設計は拡張を念頭に置いており、研究者は追加の物理分野、新しい問題形式、または異なる難易度レベルを組み込むことができる。動的性質により、新しい問題変種の継続的生成が可能である。

### 4.2 計算効率

**生成コスト**: 15,045問のベンチマークの初期生成には大量の計算リソースが必要だったが、これは一回限りのコストである。88%の成功率でコード生成が達成され、効率的なパイプラインの有効性を実証している。

**評価効率**: ベンチマークの動的性質にもかかわらず、評価は効率的である。各問題の実行可能コードにより迅速な検証が可能で、複数変種の並列評価により全体的な評価時間が最適化される。

**スケーラビリティ**: ベンチマークは様々なモデルサイズとアーキテクチャにスケールし、大規模評価研究とより小規模な診断分析の両方に適している。

### 4.3 応用可能性

**科学教育**: ベンチマークは科学教育のための強力なツールとして機能し、教育者が学生の推論プロセスを評価し、共通のエラーパターンを特定できる。段階的解決により詳細なフィードバックが可能になる。

**モデル開発**: SymPyBenchは科学推論能力を特に向上させようとするLLM開発者にとって価値のあるリソースを提供する。新しい評価指標により、従来のベンチマークでは見落とされる微妙な改善や退化を検出できる。

**研究応用**: ベンチマークは科学的推論、堅牢性、一般化の研究を可能にする。研究者はモデル行動を様々な条件下で調査し、改善のための特定領域を特定できる。

**産業応用**: 科学および工学アプリケーションでLLMを展開する組織は、SymPyBenchを使用してモデル性能を評価し、特定の使用例に対する適合性を決定できる。

**学際的拡張**: 著者らは将来の研究でベンチマークを他のSTEM分野に拡張し、化学、生物学、工学の問題を含める計画を示している。これにより、学際的科学推論のより包括的な評価が可能になる。

## 5. まとめと所感
### 5.1 論文の意義

**科学的推論評価の革新**: SymPyBenchは科学的推論のベンチマーキングにおける重要な前進を表している。従来の静的ベンチマークとは異なり、動的でパラメータ化された性質により、モデル能力のより微妙で包括的な評価が可能になる。これは、LLM評価において精度を超えた重要な転換点である。

**方法論的貢献**: 一貫性スコア、失敗率、混乱率などの新しい評価指標の導入は、AI評価の分野に重要な方法論的貢献を提供する。これらの指標は、単一の性能測定では捉えられないモデル行動の重要な側面を定量化する。

**実証的洞察**: ベンチマークを通じて得られた洞察は、現在のLLMの科学的推論における強みと限界についての貴重な理解を提供する。特に、強いモデルでも形式特有の課題（数値計算、解答生成）に苦戦する一方で、概念的理解は比較的堅固であるという発見は重要である。

**透明性と再現性**: 実行可能コードと詳細な推論ステップの包含により、ベンチマークの透明性と再現性が大幅に向上する。これは、科学的厳密性と研究結果の検証可能性にとって重要である。

### 5.2 今後の展望

**マルチモーダル拡張**: 著者らが示唆するように、ベンチマークのマルチモーダル推論タスクへの拡張は重要な発展方向である。図表、図、実験データを含む問題の統合により、より現実的で包括的な科学推論評価が可能になる。

**学際的拡張**: 物理学を超えて化学、生物学、工学、その他のSTEM分野を含む拡張は、学際的科学推論の評価を可能にし、より汎用的な科学AIシステムの開発を支援する。

**適応型評価**: 動的性質を活用して、個々のモデルの強みと弱みに基づいて問題難易度や焦点領域を動的に調整する適応型評価システムの開発が可能である。

**リアルタイム問題生成**: ベンチマークフレームワークをリアルタイム問題生成システムに拡張し、新しい科学的発見や教育ニーズに応じて継続的に新しい問題を作成できる。

**認知モデリング**: ベンチマークは、人間の科学的推論プロセスと機械学習モデルを比較する認知モデリング研究のプラットフォームとしても機能し、人間と機械の推論の違いについて洞察を提供できる。

**産業統合**: 科学および工学分野の組織がSymPyBenchを継続的評価フレームワークに統合し、実世界アプリケーションでのLLM性能を監視し改善することが期待される。

**教育技術**: 教育技術プラットフォームへの統合により、個人化された科学教育と自動化された学習評価が可能になり、STEM教育の質的向上に貢献する。

SymPyBenchは、科学的推論の評価とAIシステムの開発において重要な前進を表している。動的性質、包括的評価指標、実世界応用への焦点により、より堅牢で解釈可能で信頼性の高い科学推論システムの開発基盤として機能している。この研究は、LLMの科学的推論能力の理解を深めるだけでなく、将来のAIシステムがより効果的に科学的問題を解決するための道筋を提供している。