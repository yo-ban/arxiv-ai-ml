# OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning

## 基本情報
- arXiv ID: 2508.21066v1 (https://arxiv.org/abs/2508.21066)
- 著者: Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu 
- 所属: ByteDance Inc.
- 投稿日: 2025年8月30日
- カテゴリ: cs.CV, cs.LG

## 簡単に説明すると
この論文では画像編集タスクのための統一的な強化学習フレームワーク「OneReward」を提案しています。従来の手法では画像の穴埋め（inpainting）や画像拡張（outpainting）、オブジェクト除去、テキストレンダリングなど、異なるタスクごとに専用のモデルを訓練する必要がありました。しかしOneRewardは1つのVision-Language Model（VLM）を報酬モデルとして使用することで、これらの多様なタスクを統一的に扱うことを可能にしました。

Byteダンス社が開発したSeedream 3.0 Fillは、このフレームワークを使用して訓練された高性能な画像編集モデルで、Ideogram、Adobe Photoshop、FLUX Fill Proなどの商用ツールを上回る性能を実現しています。関連リンクとして、プロジェクトページ（https://one-reward.github.io）でコードとモデルが公開されています。

## 1. 研究概要
### 1.1 背景と動機
近年の拡散モデルの発展により、画像のインペイント、アウトペイント、オブジェクト除去、テキストレンダリングなど多様な画像編集タスクが可能になっています。しかし、これらのタスクはマスク誘導という共通の入力形式を持ちながら、条件分布と評価指標が大きく異なるという課題があります。

現在の高性能生成モデルは特定の編集タスクでは優秀な性能を発揮しますが、複数のタスクを同時に高い性能で処理することは困難です。既存手法はタスク特化型の教師あり微調整（SFT）やLoRAに依存しており、多様な編集シナリオへの汎化が制限されています。これらの非効率性を避けながら複数の画像編集タスクを支援する統一フレームワークの設計は非常に困難な問題となっています。

従来のRLHF手法であるDPO（Direct Preference Optimization）は、異なる評価次元において明確な優劣順序が存在しない場合に勝者と敗者を明確に決定できないという根本的な限界があります。例えば、美観は優れているが構造が劣る場合などです。Reward Feedback Learning（ReFL）は複数の評価基準ごとに個別の報酬モデルを必要とし、マルチタスクシナリオでは報酬の競合が発生する問題があります。

### 1.2 主要な貢献
この研究の主要な貢献は、統一的な強化学習フレームワークによる多タスク画像生成の革新にあります。タスクカテゴリと評価指標情報を直接クエリに組み込むことで、VLMが効果的にタスクと評価基準を区別し、ペアワイズ判定を可能にしました。

- VLMを生成報酬モデルとして活用した視覚ドメイン向けの新しい報酬モデルフレームワーク「OneReward」の提案
- 画像穴埋め、画像拡張、オブジェクト除去、テキストレンダリングを効果的に処理する統一最先端画像編集モデル「Seedream 3.0 Fill」の開発  
- 商用API及びオープンソースモデル（Ideogram、Adobe Photoshop、FLUX Fill Pro）を上回る性能の実現
- FLUX Fill [dev]にマルチタスク強化学習アプローチを適用したオープンソースモデル「FLUX Fill [dev][OneReward]」の公開

## 2. 提案手法
### 2.1 手法の概要
OneRewardは、マルチタスク学習のための統一フレームワークで、人間の選好データで訓練された報酬モデルを活用して、様々な下流タスクにわたってポリシーモデルを微調整します。完全なデータセットは、K個のサブセットに分割されます。各タスクは画像と対応する入力条件の共通フォーマットを共有します。

訓練プロセスでは、参照モデル（初期事前訓練モデル）とポリシーモデル（訓練対象）を用意し、ポリシーモデルが各タスク固有の評価指標で参照モデルを上回る結果を生成するように最適化します。報酬信号はVLMが生成する「Yes」トークンの確率から導出され、勾配逆伝播に使用されます。

### 2.2 技術的詳細
人間選好データ収集：4つの主要編集タスクに対する大規模高品質人間選好データセットを構築しました。各サンプルは3つ組（I_src,M,P）として構造化されます。ここで、I_srcは元画像、Mは編集領域を示すバイナリマスク、Pは対応するテキストプロンプトです。

多次元ペアワイズ報酬モデル訓練：OneRewardは事前訓練されたVLMの知覚能力を活用し、全次元の報酬予測のバックボーンとして機能します。評価クエリを通じてモデルをガイドし、ここで指示テンプレートは元の入力プロンプト（プロンプト-画像アライメント評価時のみ使用）です。

マルチタスク報酬フィードバック学習：各イテレーションで事前定義されたサンプリング確率に従ってサブデータセットからデータをサンプルします。参照モデルは初期生成能力を表すベースライン画像を生成し、評価画像は異なるノイズベクトルから開始して、ランダムに選択された中間タイムステップまで部分デノイジングを行います。

損失関数は以下で定義されます：
損失 = max(0, λ - 報酬確率)

### 2.3 新規性
OneRewardの最大の新規性は、従来のタスク特化型報酬モデルの制約を克服し、単一のVLMで多次元・多タスク評価を実現した点にあります。従来のスカラーベース報酬モデルは、変更されていない背景領域に評価が支配されがちで、マスク領域内の実際の編集品質を正確に捉えられない問題がありました。

OneRewardは相対的品質に基づく比較評価方式を採用し、画像ペアと評価クエリを入力として、その特定クエリに対する人間の選好に沿った二項分類を出力します。この非可換な定式化により、最初の位置の画像が高品質な場合には肯定的応答、そうでなければ否定的応答を期待する応答として設定しています。

技術的には、動的強化学習戦略も提案しており、EMAモデルを直接参照モデルとして再利用することで、メモリオーバーヘッドを削減し、訓練全体を通じてより安定した適応報酬信号を提供しています。

## 3. 実験結果
### 3.1 実験設定
データセット：CLIPモデルを用いた画像埋め込み抽出とK-meansクラスタリングにより、強化学習用の多様で代表的なサブセットを構築しました。選択されたサブセットに基づいて大量の人間選好ペアを生成・注釈し、報酬モデルと画像生成モデルの両方の訓練データとして使用しました。

実験設定：報酬モデルにはQwen2.5-VL-7B-Instructを選好データセットで微調整しました（バッチサイズ16、学習率1e-6）。Seedream 3.0 FillにはSeeddream 3.0をテキスト-画像ベースモデルとして採用し、画像穴埋め50%、画像拡張25%、オブジェクト除去25%の確率でデータをサンプリングしました。訓練はバッチサイズ8、学習率1e-5で実行しました。

評価設定：Ideogram、Higgsfield、Adobe Photoshop、Midjourney、Flux Fill Proを含む最新モデル・APIとの包括的比較を実施しました。評価用ベンチマークは画像穴埋め130枚、オブジェクト除去100枚、画像拡張200枚で構成されます。ポートレート、風景、ペット、タイポグラフィなど幅広いシーンと、フォトリアリスティック、アニメ、水彩、AI生成などの芸術スタイルをカバーしています。

### 3.2 主要な結果
定量的結果：Seedream 3.0 Fillは全てのタスクで優れた総合性能を実証しました。画像穴埋めタスクでは使用可能率69.04%を達成し、第2位の競合（52.11%）を16.93ポイント上回りました。テキストアライメント、テクスチャ一貫性、構造、美観、テキストレンダリングの大部分の次元で高いスコアを獲得しました（スタイル一貫性のみIdeogramが僅かに優位）。

テキスト誘導画像拡張タスクではIdeogramと同等の性能を示しながら、Flux Fill ProとAdobe Photoshopを明確に上回りました。テキスト非誘導設定では顕著な優位性を示し、使用可能率87.54%と全報告次元でのリードを達成しました。オブジェクト除去では使用可能率82.22%、除去品質スコア86.33%で優秀な結果を達成しました。

定性的結果：Good-Same-Bad（GSB）評価により、OneReward有無でのSeedream 3.0 Fillモデル比較を実施しました。OneReward変種は全タスクで「Good」評価の高い割合を獲得し、統一報酬モデルが一般的に望ましい生成へとモデル出力をシフトさせることを実証しました。

### 3.3 既存手法との比較
商用システムとの比較：Adobe Photoshop、Ideogram、Flux Fill Proなどの商用システムと比較して、Seedream 3.0 Fillは使用可能率で大きな優位性を示しました。特に画像穴埋めでは69.04%、テキスト非誘導画像拡張では87.54%という優秀な性能を達成しました。

報酬モデル精度：テキストアライメントが80%を超える精度を達成し、画像穴埋めと画像拡張の両方で良好な結果を示しました。一貫性、構造、美観などの他次元は中程度だが安定した精度（70%台前半から中盤）を示しました。オブジェクト除去タスクでは84.93%の除去品質を達成し、この設定での人間選好捕捉に特に効果的であることを示しました。

オープンソース貢献：FLUX Fill [dev]に両アルゴリズム（通常版と動的版）を適用したRL強化モデルを訓練し、ベースモデルや商用APIを上回る優秀な視覚品質を実証しました。これらの訓練済みモデルをオープンソース貢献として公開予定です。統一生成モデル、単一報酬モデル、タスク特化型SFT不要という条件下で、これらの全結果を達成した点が重要です。

## 4. 実用性評価
### 4.1 実装の容易性
OneRewardフレームワークは既存の事前訓練済みVLMを報酬モデルとして活用するため、実装の複雑さが軽減されています。従来のタスク特化型アプローチでは各評価次元・各タスクに対して個別の報酬モデルが必要でしたが、OneRewardは単一のVLMで全てを処理できるため、エンジニアリングの複雑さとモデル同期の問題を解決しています。

動的強化学習戦略により、従来の3つのモデル並列維持（ポリシー、参照、EMA）から2つのモデルに削減し、メモリ消費量を約33%削減しました。Flux Fill [dev]への適用例が示すように、既存の事前訓練済みモデルに対して比較的容易に適用可能です。

### 4.2 計算効率
訓練プロセスは効率的に設計されており、部分デノイジングアプローチにより完全なサンプリングパイプラインを回避しています。ランダムに選択された中間タイムステップから直接最終潜在変数を予測し、勾配はこの最終単一ステップ予測のみを通じて逆伝播されます。

VAEデコーダーを使用して潜在変数からピクセル空間画像を再構築し、報酬計算に必要な画像のみを生成するため、メモリ効率も良好です。報酬カーブの可視化結果から、マルチタスク設定にもかかわらず全次元で明確な上昇傾向を示し、安定した収束を達成しています。

### 4.3 応用可能性
多様なタスクへの汎用性：画像穴埋め、画像拡張（プロンプト有無両方）、オブジェクト除去、テキストレンダリングという4つの異なるタスクで統一的に優秀な性能を実現しており、さらなるマスク誘導タスクへの拡張が期待できます。

商用レベルの品質：Adobe Photoshop、Ideogram、Flux Fill Proなどの商用システムを上回る性能を実現しており、実際のプロダクション環境での利用が十分可能です。使用可能率の大幅な改善（特にテキスト非誘導拡張で87.54%）は、実用的な画像編集ツールとしての価値を示しています。

研究コミュニティへの貢献：オープンソースモデル「FLUX Fill [dev][OneReward]」の公開により、研究コミュニティに強力な新しいベースラインを提供しています。統一マスク誘導画像生成の将来研究のための基盤として機能することが期待されます。プロジェクトページ（https://one-reward.github.io）でコードとモデルが公開されており、再現性と発展性を確保しています。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、画像編集分野において極めて重要な技術的進歩を示しています。最も注目すべき貢献は、従来のタスク特化型アプローチの限界を打破し、単一の統一フレームワークで多様な画像編集タスクを高品質で処理できることを実証した点です。

OneRewardの設計哲学は、VLMの持つ多モーダル理解能力を最大限活用し、従来の複雑な報酬モデル設計を簡素化しています。タスク情報と評価次元をクエリに直接組み込むアプローチは、スケーラブルかつ柔軟な解決策として非常に優れており、今後の多タスク学習研究に大きな影響を与えると考えられます。

実用的な観点からも、商用システムを上回る性能を実現し、特に使用可能率の改善は実際のユーザー体験向上に直結する重要な成果です。Byteダンス社という産業界からの研究であり、実際の製品開発における実用性が十分に検証されている点も評価できます。

技術的な革新性として、従来のDPOやReFL手法の制約を克服し、相対評価と多次元評価を統合したアプローチは、強化学習による生成モデル最適化の新たな方向性を示しています。

### 5.2 今後の展望
技術的改善点：論文中でも言及されているように、スタイル一貫性が相対的な弱点として残っており、詳細なデータ注釈と後訓練改良を通じた最適化が今後の課題です。また、報酬ハッキング回避のためのλパラメータの設定や、マルチタスク学習における報酬競合の完全解決には更なる研究が必要です。

応用範囲の拡張：現在の4つのタスクを超えて、より多様な画像編集タスク（例：スタイル変換、画像修復、セマンティック編集）への拡張が期待されます。OneRewardフレームワークの汎用性を考慮すると、3D生成や動画編集への適用も将来的に可能と考えられます。

研究コミュニティへの影響：オープンソース化により、研究コミュニティがこの手法を基盤として更なる改良を加えることが期待されます。特に、異なるドメイン（医療画像、衛星画像など）への適用や、より効率的な訓練手法の開発が進むと予想されます。

産業界への波及効果：商用レベルの性能を実現したことにより、Adobe、Google、OpenAIなどの主要企業の画像編集ツールにおいても類似のアプローチが採用される可能性が高いです。統一モデルによるコスト効率性とユーザビリティの向上は、画像編集ソフトウェア業界全体の技術水準を押し上げる重要な契機となるでしょう。