# CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification

## 基本情報
- arXiv ID: 2508.21046v1 (https://arxiv.org/abs/2508.21046)
- 著者: Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie
- 所属: School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen
- 投稿日: 2025年8月30日
- カテゴリ: cs.RO, cs.AI

## 簡単に説明すると
この論文は、ビジョン-言語-アクション（VLA）モデルの計算効率を大幅に改善する「CogVLA」を提案しています。従来のVLAモデルは高い計算コストが課題でしたが、CogVLAは人間の多感覚協調機能からインスピレーションを得た3段階のプログレッシブアーキテクチャを採用しています。

具体的には、視覚エンコーダで指示に基づいて視覚トークンを25%に圧縮するEFA-Routing、言語モデル内で無関係なトークンを50%削減するLFP-Routing、そして圧縮された情報でも一貫したアクション生成を可能にするCAtten機構を統合しています。

LIBEROベンチマークで97.4%、実世界のロボットタスクで70.0%の成功率を達成しながら、OpenVLAと比較して訓練コストを2.5倍、推論レイテンシを2.8倍削減することに成功しています。

プロジェクトページ: https://jiutian-vl.github.io/CogVLA-page

## 1. 研究概要
### 1.1 背景と動機
ビジョン-言語-アクション（VLA）モデルは、事前訓練されたビジョン-言語モデル（VLM）の豊富な表現を活用してロボット制御を実現していますが、高次元マルチモーダル特徴と連続アクション空間の整合に膨大な計算コストが必要です。例えば、7B VLAモデルをLIBEROベンチマークで微調整するには600 GPU時間超が必要です。

既存のsparsification手法（Mixture-of-Depths、レイヤースキップ、早期終了）は、言語モデル内の計算最適化に主眼を置き、視覚-言語-アクション間のセマンティック結合を無視しています。これにより、視覚圧縮によるタスク関連特徴の破棄、トークンスキップによる文脈一貫性の破綻、因果推論を欠いたアクション生成などの「クロスモーダルセマンティック劣化」が生じています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです：
- 人間の多感覚協調からインスピレーションを得たCogVLA framework の提案。VAS（視覚情報フォーカス）、SMA（セマンティック意図フィルタリング）、PMC（アクションシーケンス計画）の3段階biomimeticアーキテクチャを実現
- 知覚推論パイプラインにおける指示駆動ビジョンsparsificationを可能にするEFA-RoutingとLFP-Routingの開発
- 二重圧縮されたマルチモーダル表現においてクロスモーダル論理一貫性と時間的アクション一貫性を確保するCAttenの定式化
- LIBEROベンチマークと実世界ロボットタスクでの包括的評価によるCogVLAの優れた性能と効率性の実証

## 2. 提案手法
### 2.1 手法の概要
CogVLAは、人間の認知機能にインスパイアされた3段階プログレッシブ設計を採用しています。従来のモジュラーパイプラインとは異なり、視覚、言語、アクションモダリティ全体でタスクセマンティック一貫性を持つ統合最適化メカニズムを確立します。

**Stage 1: EFA-Routing（Encoder-FiLM based Aggregation Routing）**
視覚的情報冗長性を軽減し、VAS様の視覚フォーカスを実現するため、タスク固有指示に基づいて視覚トークンを元の25%に圧縮します。指示を動的にモジュレーションパラメータにエンコードし、視覚エンコーダ内でのトークン集約を誘導します。

**Stage 2: LFP-Routing（LLM-FiLM based Pruning Routing）**
集約された視覚エンコーディングを基に、指示認識スパース性パターンを学習して言語モデル内の視覚トークンを刈り込みます。SMAの機能を模倣し、アクション意図を視覚特徴に注入して、タスク無関係トークンの50%超をスキップします。

**Stage 3: CAtten（V-L-A Coupled Attention）**
PMCにインスパイアされた結合アテンション機構により、圧縮された視覚入力でも正確で一貫したアクション生成を確保します。クロスモーダル因果アテンション、V-L層での単方向アテンション、アクション層での双方向アテンションを組み合わせます。

### 2.2 技術的詳細
**EFA-Routing の実装**
Step 1では、各視覚エンコーダ内で指示t_rがVisualトークンI^(i)とaggregationトークンv_agg^(i)を変調：
```
f_FA(I^(i), v_agg^(i), t_r) = (1+γ_i(t_r)) ⊙ Self-Att(I^(i), v_agg^(i)) + β_i(t_r)
v_agg^(i) = Aggregate(FFN(f_FA(·))) + v_agg^(i)
```

Step 2では、SigLIPとDINOv2からの集約表現を指示条件付き融合：
```
α = Sigmoid(W_2(σ(W_1 t_r + b_1)) + b_2)
v_agg = α · v_agg^SigLIP + (1 - α) · v_agg^DINOv2
```

**LFP-Routing の実装**
Task-Guided Pruning Routerにより、各層lでルーティング重みR_l^jを計算し、β-th percentileを閾値として関連トークンのみ保持：
```
R_l^j = MLP(Z_l^j)
Z_{l+1}^j = R_l^j × f_SF([Z_l^j, t_l]) + Z_l^j  (if R_l^j > P_l^β)
```

**CAtten の実装**
階層的マスキング戦略により、V-L間に因果アテンション、アクション内に双方向アテンション、VLからアクションへの依存関係を設定：
```
M_hybrid = [M_causal^VL  -∞  -∞]
           [0      0   -∞]
           [0      0   M_bi^act]
```

### 2.3 新規性
本研究の新規性は以下の点にあります：

**生物学的認知モデルの統合**: 人間の視覚注意システム（VAS）、補足運動野（SMA）、前運動皮質（PMC）の機能的役割を模倣した3段階アーキテクチャの実現。従来の単純なsparsification手法と異なり、認知科学的根拠に基づく設計。

**指示駆動型統合スパース化**: 視覚エンコーダからLLM、アクション生成まで一貫した指示ベースの最適化。既存手法がモジュール単位で最適化するのに対し、エンドツーエンドのセマンティック一貫性を維持。

**プログレッシブ圧縮戦略**: Stage 1で粗粒度集約、Stage 2で細粒度刈り込み、Stage 3で一貫性保持という段階的アプローチ。各段階の特性に応じた最適化により、単純な一様圧縮を上回る性能を実現。

**並列アクションデコーディング**: 双方向アテンションによる効率的な並列デコーディングと因果推論の両立。従来の自己回帰的手法と比較して大幅なレイテンシ削減を実現。

## 3. 実験結果
### 3.1 実験設定
**シミュレーションベンチマーク**: LIBEROベンチマークを使用してタスク性能と効率性を評価。LIBEROは多様で長い指示（平均10.48語 vs RLBenchの3.34語）を特徴とし、モデルの言語理解能力を反映。Spatial、Object、Goal、Longの4つのスイートで各10タスク、50デモンストレーションを含む。

**実世界実験**: Cobot Agilex ALOHAプラットフォームで3つの長期タスクを評価：Object Placement（45デモ）、Drawer Manipulation（45デモ）、T-shirt Folding（30デモ）。データ収集中に空間的・セマンティック変動を導入。

**実装環境**: 4×A800 GPU（80GB）でCogVLAの効率的指示駆動sparsificationの恩恵を活用。OpenVLAをバックボーンモデルとして採用し、LoRA（rank=32, α=64）で微調整。LIBERO用にK=8のアクションチャンクサイズ、実世界用にK=25を設定。

**評価メトリクス**: タスク成功率（SR）、推論時間、スループット、FLOPs、訓練コストを測定。統計的有意性検証のため複数シード評価を実施。

### 3.2 主要な結果
**LIBEROベンチマーク性能**
CogVLAは全タスクスイートで最高性能を達成：
- Spatial: 98.6%（既存最高を上回る）
- Object: 98.8%（既存最高と同等）
- Goal: 96.6%（OpenVLA-OFTの97.9%に次ぐ2位）
- Long: 95.4%（既存最高を上回る）
- 平均: 97.4%（既存最高のOpenVLA-OFT 97.1%を上回る）

**実世界ロボットタスク性能**
全サブタスクと総合タスクで最高成功率を達成：
- Object Placement: 9/10, 8/10（vs OpenVLA-OFT: 8/10, 7/10）
- Drawer Manipulation: 8/10, 7/10, 7/10（vs OpenVLA-OFT: 8/10, 6/10, 5/10）
- T-shirt Folding: 9/10, 8/10, 6/10（vs OpenVLA-OFT: 7/10, 7/10, 5/10）
- 平均成功率: 70.0%（vs OpenVLA-OFT: 56.7%）

**効率性改善**
OpenVLAと比較して顕著な効率向上を実現：
- 推論時間: 0.091s（vs OpenVLA: 0.254s）→ 2.79×高速化
- スループット: 87.9 Hz（vs OpenVLA: 3.9 Hz）→ 22.54×向上
- FLOPs: 2.72T（vs OpenVLA: 8.48T）→ 3.12×削減
- 訓練コスト: 4.7h/10k steps（vs OpenVLA: 11.7h）→ 2.49×削減

### 3.3 既存手法との比較
**性能ランキング**: LIBEROベンチマークでCogVLAは12手法中総合1位。特にSpatial（1位）、Object（1位）、Long（1位）で最高性能。唯一Goalタスクで2位（OpenVLA-OFTが1位）だが、これは8×視覚入力削減とのトレードオフによる意図的な結果。

**効率性比較**: 最新の効率化手法（OpenVLA-OFT、PD-VLA）と比較しても、CogVLAは訓練・推論両面で優位性を示す。OpenVLA-OFTに対し31%の推論時間短縮、PD-VLAに対し49%の FLOPs削減を実現。

**アブレーション研究**: 3段階設計の各コンポーネントが性能向上に寄与することを確認。Stage 1+2の統合sparsificationが既存視覚圧縮手法（FastV、SliME）を大きく上回る（98.6% vs 88.2%、77.6%）。Stage 1により大きなsparsification ratio を割り当てる非対称配分（4×-2×）が対称配分（2×-4×）より効果的。

**定性分析**: 視覚化により、CogVLAがより正確なタスク実行（引き出しとの衝突回避など）と短いアクション推論時間を実現することを確認。タスク長が長くなるほど効率優位性が顕著に。

## 4. 実用性評価
### 4.1 実装の容易性
CogVLAは優れた実装容易性を提供します。既存のOpenVLAアーキテクチャをバックボーンとして活用し、3つのルーティングモジュールを段階的に追加する設計により、ゼロからの開発負荷を大幅に軽減します。

EFA-RoutingのEncoder-FiLMモジュールは、指示テキストから線形変換でスケール・シフトベクトル（γ、β）を生成する軽量な実装です。各視覚エンコーダに64個の集約トークンを使用し、標準的な自己アテンション機構を活用します。

LFP-RoutingのTask-Guided Pruning RouterはMLPベースの単純な実装で、shifted cosineスケジュール（β_l = 0.5*cos(πl/L) + η）による柔軟なスパース性制御を提供。隠れ層次元2048の2層MLPにより、追加パラメータ数を最小限に抑制。

CAttenの階層的マスキング戦略は、標準的なattention maskの拡張として実装でき、既存のTransformerライブラリとの互換性を維持します。LoRA微調整（rank=32、α=64）の採用により、フルパラメータ調整と比較して計算・メモリ要求を大幅削減。

### 4.2 計算効率
CogVLAの計算効率は以下の設計により実現されています：

**段階的スパース化**: Stage 1で視覚トークンを25%に削減、Stage 2で更に50%を刈り込むことで、最終的に元の12.5%（8×削減）の視覚トークン数を実現。これによりLLM処理での大幅な計算削減を達成。

**非対称スパース性配分**: Stage 1（4×）> Stage 2（2×）の配分により、早期段階での効率的な冗長性除去と後期段階での精細な関連性保持のバランスを最適化。対称配分（2×-4×や4×-4×）と比較して優れた性能効率バランス。

**並列デコーディング**: 双方向アテンションによりK個のアクションを単一パスで生成、従来の自己回帰的手法のK×D回の順次生成と比較して大幅なレイテンシ削減（2.8×高速化）。

**動的ルーティング**: 固定的な刈り込みではなく、指示内容に応じた動的なトークン選択により、タスク関連情報を効果的に保持しながら計算量削減を実現。

**メモリ効率**: LoRA適応とsparsificationの組み合わせにより、訓練時のGPUメモリ使用量を大幅削減。4×A800 GPU環境での実行を可能にし、リソース制約環境での実用性を向上。

### 4.3 応用可能性
**家庭用ロボティクス**: 長い自然言語指示の理解とマルチステップタスク実行能力により、家事支援ロボットへの応用が期待されます。T-shirt折り畳み、引き出し操作、物体配置タスクでの高い成功率は、日常的な家庭環境での実用性を示します。

**産業オートメーション**: 複雑な組み立てタスクや品質検査での活用が可能。指示駆動アプローチにより、作業手順の変更に柔軟に対応でき、従来のハードコードされた産業ロボットと比較して高い適応性を提供。

**教育・研究プラットフォーム**: 効率的な計算要求（4×A800で動作）により、大学研究室レベルでの利用が可能。LIBEROベンチマークでの最高性能は、VLA研究の新たなベースラインとしての価値を示します。

**エッジデプロイメント**: 大幅な計算効率改善により、モバイルロボットや組み込みシステムでのリアルタイム実行が可能。2.8×のレイテンシ削減は、応答性が重要なインタラクティブ応用に適します。

**スケーラビリティ**: プロジェクトページ（https://jiutian-vl.github.io/CogVLA-page）での実装公開により、研究コミュニティでの広範な採用と拡張が期待されます。認知科学に基づくアーキテクチャ設計は、他の embodied AI システムへの応用可能性を示します。

**将来展開**: 現在の固定スパース性比率から適応的sparsificationへの拡張、触覚・力覚フィードバックの統合、生涯学習機能の追加により、より複雑な実世界環境での応用が可能になります。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、VLAモデルの実用化に向けた重要な貢献を提供しています。最も注目すべき点は、生物学的認知機能からインスピレーションを得た理論的基盤と、実際の効率性・性能改善を両立させた点です。

**理論的革新性**: 人間の視覚注意システム（VAS）、補足運動野（SMA）、前運動皮質（PMC）の機能を模倣した3段階アーキテクチャは、単なる工学的最適化を超えた認知科学的根拠を提供します。これまでのsparsification手法が局所的な計算削減に留まっていたのに対し、CogVLAは知覚から制御まで一貫したセマンティック整合性を維持する統合的アプローチを実現。

**実用性の実証**: LIBEROベンチマークでの97.4%成功率、実世界タスクでの70.0%成功率という高い性能を維持しながら、計算コストを2.5倍、推論レイテンシを2.8倍削減した成果は、VLAモデルの実用化における大きな前進を示します。特に、リソース制約環境での動作可能性は、研究段階から実用段階への移行を大幅に促進。

**methodology の汎用性**: 指示駆動ルーティングとプログレッシブsparsificationの概念は、VLAに限らず他のマルチモーダルタスクへの応用可能性を持ちます。認知アーキテクチャに基づく設計原理は、将来のembodied AIシステム開発の指針となり得ます。

**評価の包括性**: シミュレーションと実世界両方での評価、複数シード評価による統計的妥当性確保、詳細なアブレーション研究による各コンポーネントの寄与分析など、thorough な実験設計により結果の信頼性を高めています。

### 5.2 今後の展望
**適応的sparsification の発展**: 現在の固定スパース性比率から、タスク複雑度や環境不確実性に応じた動的配分への拡張が期待されます。メタ学習やreinforcement learning を組み合わせることで、各タスクに最適なスパース性パターンを自動発見する機能の実現が可能。

**マルチモーダル感覚統合**: 視覚・言語に加えて触覚、力覚、音響情報の統合により、より豊かな環境認識と精密な操作制御が可能になります。CogVLAのルーティング機構は、追加モダリティの効率的統合プラットフォームとして機能し得ます。

**大規模展開と標準化**: プロジェクトページでのコード公開を基盤として、コミュニティ主導での機能拡張とベンチマーク標準化が進展すると予想されます。特に、diverse robot platforms での評価とドメイン適応技術の発展が重要。

**認知アーキテクチャの深化**: 現在の3段階設計から、より詳細な認知機能（working memory、attention control、executive functionなど）を模倣した高次アーキテクチャへの拡張により、複雑な推論と長期記憶を要するタスクでの性能向上が期待。

**安全性と信頼性の強化**: 実世界展開に向けて、不確実性推定、failure detection、graceful degradation などの安全機能の統合が不可欠。CogVLAの attention 機構は、モデルの判断根拠の可視化と説明可能性向上にも活用可能。

**産業応用への展開**: 製造業、物流、医療などの特定ドメインにおける specialized version の開発と、domain-specific knowledge の効率的統合手法の確立により、実際の産業価値創出が加速すると予想されます。

CogVLAが示した「認知的整合性を保持した効率化」というパラダイムは、AIシステムの実用化における新たな設計哲学として、今後のembodied intelligence 分野の発展に大きな影響を与えると考えられます。
