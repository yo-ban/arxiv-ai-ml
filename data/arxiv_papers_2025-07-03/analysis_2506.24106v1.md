# On the Predictive Power of Representation Dispersion in Language Models

## 基本情報
- arXiv ID: 2506.24106v1 ([https://arxiv.org/abs/2506.24106](https://arxiv.org/abs/2506.24106))
- 著者: Yanhong Li, Ming Li, Karen Livescu, Jiawei Zhou
- 所属: University of Chicago, University of Maryland他
- 投稿日: 2025年6月
- カテゴリ: cs.CL, cs.LG

## 簡単に説明すると
この論文は、言語モデルがテキストを予測する能力が、埋め込み空間の「幅」と密接に関連していることを示しています。
具体的には、モデルが文脈の表現をより幅広く分散させるほど、パープレキシティ（予測の難しさ）が低くなるという発見です。
この「表現分散」を測定することで、ラベルなしでモデルの性能を予測したり、最適なモデルを選択したり、訓練を改善したりできます。

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデルは多様なタスクで優れた性能を発揮しますが、その埋め込み幾何学はしばしば異方性やランク崩壊の兆候を示します。
これは、隠れた状態が狭い円錐内に存在したり、低次元の部分空間を占めたりする現象です。

この幾何学が表現力を制限するとされていますが、テキスト生成能力とどのように関連するかは明らかではありませんでした。
本研究は、モデルの予測能力が埋め込み空間の「幅」と密接に関連していることを実証的に示します。

直感的には、弱いモデルは文脈を密集したクラスターに圧縮します。
一方、強いモデルは意味的に似た文脈でもより幅広く分離させます。
この幅広い幾何学により、潜在空間でより明確な区別が可能となり、より鋭い（低エントロピーの）次トークン予測が実現されます。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです。
- 表現分散（隠れたベクトル間の平均ペアワイズコサイン距離）とパープレキシティの強い負の相関を発見
- ラベルなしで下流タスクの精度を予測する手法の提案
- 表現分散に基づくモデル選択手法の開発
- k-NN言語モデルにおける最適層選択への応用
- 「push-away」損失による訓練時のパープレキシティ改善

## 2. 提案手法
### 2.1 手法の概要
本研究の中核は、「表現分散」という指標の提案です。
これは、モデルがテキストサンプルを埋め込み空間でどの程度分離させているかを測定します。

具体的には、N個のテキストセグメントをデータセットからサンプリングし、各セグメントをモデルに通して特定の層（デフォルトでは最終層）の表現ベクトルEiを抽出します。
そして、これらの表現の平均ペアワイズコサイン距離を計算します。

この値が高いほど、埋め込みがより「広がって」いることを示し、表現間の分離が大きいことを意味します。

### 2.2 技術的詳細
表現分散の計算式は以下の通りです。

平均距離 = 1/(N選択2) × Σ(1 - (Ei・Ej)/(||Ei||*||Ej||))

ここで、Ei・Ejはベクトルの内積、||Ei||はベクトルのノルムを表します。

研究では、シーケンスレベルのパープレキシティと最終トークンパープレキシティの両方で評価しました。
また、層毎の分析、意味的に似た文脈のクラスター内での分析、ファインチューニングの影響などを詳細に調査しました。

### 2.3 新規性
本研究の新規性は以下の点にあります。

第一に、表現分散と言語モデルの予測性能の関係を包括的に分析した点です。
従来の研究では異方性やランク崩壊が問題視されていましたが、それが実際の予測性能とどう関係するかは明らかではありませんでした。

第二に、この指標を実用的な応用につなげた点です。
ラベルなしでの性能予測、モデル選択、最適層の選択、訓練の改善など、幅広い応用を示しました。

第三に、意味的に似た文脈のクラスター内でも分散が増加することを示した点です。
これは、単に異なる文脈を引き離すだけでなく、似た文脈も区別できるようになることを意味します。

## 3. 実験結果
### 3.1 実験設定
実験では、複数のモデルファミリー（LLaMA、Phi、Mistral、Qwenなど）を使用しました。
データセットは、WikiText-103、CNN Daily News、PubMed要約など、多様なドメインを含みます。

評価では、100,000個の512トークンテキストセグメントをランダムに選択し、パープレキシティを計算しました。
同時に、最終層の埋め込みの平均ペアワイズ距離も測定しました。

### 3.2 主要な結果
最も重要な発見は、パープレキシティと表現分散の間に強い負の相関があることです。
低いパープレキシティ（予測が容易）のセグメントはより分散した埋め込みを持ち、高いパープレキシティ（予測が困難）のセグメントはより圧縮された埋め込みを示しました。

層毎の分析では、この負の相関が深い層でより強くなることが判明しました。
初期層では明確な相関が見られず、低レベルの語彙特徴を捕らえているためと考えられます。

ファインチューニングの影響も調査しました。
LoRAとフルパラメータファインチューニングの両方で、平均埋め込み分離が増加しました。
フルファインチューニングの方がより強い効果を示し、テキストサンプルを全体的により遠くに押し広げました。

### 3.3 既存手法との比較
本研究のアプローチは、従来の評価手法とは異なり、ラベルなしでモデル性能を予測できる点が特徴です。

例えば、ARC-ChallengeとMMLUでの実験では、正解率が表現分散と単調に上昇することを示しました。
モデルが正しく答えるスライスは、誤答するスライスよりも明らかに幅広い幾何学を示しました。

これにより、実務者はラベルなしデータセットを分散度でソートし、低分散の末尾のみを検査して障害モードを発見したり、「難しい」クエリに対して継続的な訓練を集中させたりできます。

## 4. 実用性評価
### 4.1 実装の容易性
表現分散の計算は非常にシンプルで、既存の評価パイプラインに容易に統合できます。
必要なのは、モデルの隠れた状態を抽出し、コサイン距離を計算するだけです。

特にモデル選択の応用では、モデルの出力埋め込み行列を読み込んでCPU上で基本的な行列演算を行うだけで、フォワードパスやGPU計算は不要です。

### 4.2 計算効率
表現分散の計算は計算効率が高く、大規模な評価を必要としません。

例えば、モデル選択のタスクでは、すべてのチェックポイントを網羅的に評価する代わりに、分散ギャップを計算するだけで最適なモデルを特定できます。
実験では、分散ギャップによるモデルのランキングが、数学やコードなどのドメインで最高性能のモデルを一貫して上位に配置しました。

### 4.3 応用可能性
表現分散は多様な実用的応用が可能です。

第一に、ラベルなしでの下流タスク性能予測。
ラベル付きデータが少ない新しいドメインで、モデルがどの程度機能するかを迅速に評価できます。

第二に、高速なモデル選択。
複数の事前訓練済みモデルやファインチューニングされたバリアントから、分散の大きいモデルは一貫して優れた性能を示します。

第三に、k-NN言語モデルにおける最適層選択。
最も分散が高い隠れた層を選ぶことで、最良のパープレキシティを得られます。

第四に、訓練の改善。
補助的な「push-away」損失を通じて分散を促進することで、単一ドメインとクロスドメインの両方でパープレキシティが直接改善されます。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、言語モデルの埋め込み幾何学を単純な統計量（平均ペアワイズ距離）で捉え、それが予測品質の堅牢な指標として機能することを示した重要な論文です。

特に注目すべき点は、この発見が単なる理論的興味にとどまらず、実用的な応用につながっていることです。
ラベルなしでの性能予測、迅速なモデル選択、最適層の特定、訓練の改善など、幅広い応用が可能です。

また、意味的に似た文脈のクラスター内でも分散が増加するという発見は、モデルが学習を通じて微細な区別を身につけていく過程を示唆しており、モデルの学習メカニズムの理解にも貢献しています。

この研究は、言語モデルの評価や選択における新しい視点を提供し、より高速でデータ量を削減したアプローチを可能にします。

### 5.2 今後の展望
本研究はいくつかの方向への拡張が期待されます。

第一に、より洗練された分散指標の開発です。
現在の単純な平均距離を超えて、より複雑な幾何学的特性を捕らえる指標の開発が期待されます。

第二に、他のモダリティへの応用です。
本研究はテキストに焦点を当てていますが、同様の原理が画像、音声、マルチモーダルモデルにも適用できる可能性があります。

第三に、訓練アルゴリズムの改善です。
push-away損失の初期結果は有望ですが、より洗練された正則化手法や訓練目標の開発が期待されます。

全体として、この研究は埋め込み構造を解釈可能性、ドメイン転移、堅牢な生成に結びつける新しい方向を促進することが期待されます。