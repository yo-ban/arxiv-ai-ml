# The Era of Real-World Human Interaction: RL from User Conversations

## 基本情報
- arXiv ID: 2509.25137v1 (https://arxiv.org/abs/2509.25137)
- 著者: Chuanyang Jin, Jing Xu, Bo Liu ほか
- 共著者: Leitian Tao, Olga Golovneva, Tianmin Shu ほか
- 所属: FAIR at Meta, Johns Hopkins University
- 投稿日: 2025年09月30日
- カテゴリ: cs.AI, cs.LG

## 簡単に説明すると
この論文は、従来の専門家による事前注釈付きフィードバックに依存する手法から脱却します。実世界のユーザー会話から直接学習する新しいパラダイム「RLHI（Reinforcement Learning from Human Interaction）」を提案しています。WildChatデータセットから得られた自然な人間とのやりとりを活用します。ユーザーの長期的な会話履歴から導出される「ペルソナ」を用いた個人化されたアライメント手法を開発しました。

2つの補完的な手法を開発しています。「ユーザーガイド書き直し」では、ユーザーの自然言語フォローアップ応答に基づいて不満足なモデル出力を修正し、「ユーザーベース報酬」では、ユーザーの長期的な相互作用履歴に基づく報酬モデルを学習します。これらの手法により、個人化と指示追従の両面で強いベースラインを上回る性能を達成し、推論ベンチマークでも改善を示しています。

## 1. 研究概要

### 1.1 背景と動機
現在の会話モデルのアライメント手法は、専門家が生成した事前注釈付きの人間フィードバックに依存しています。しかし、この手法には根本的な限界があります。

主な問題は以下の3点です。

1. 専門家による注釈は非自然的なシナリオにおける注釈者の意見を反映しており、実際のユーザーの真の好みや進化するニーズを捉えられない
2. 文脈に依存しない判断は、状況に応じた動的な要求を見逃しがちである
3. 実際のユーザーの多様性ではなく、ラベリング予算によってスケールが制限される

この研究の根本的な洞察は、継続的なモデル改善と多面的なアライメントを達成するために、将来のモデルは自然な人間の相互作用から学習しなければならないということです。実世界のユーザーとモデルの有機的な交流には、隠れたユーザー好みや動的で文脈依存の要求、豊かなフィードバック信号が含まれており、これらは従来の静的な注釈では捉えることができません。

WildChatデータセットの分析により、ユーザーの26.51%が不満足な応答に対してフォローアップフィードバックを提供していることが判明しました。これらの自然な相互作用は、モデル改善のための豊富で多様な監視信号を提供する可能性があります。従来の手法が見落としてきたこの豊富な情報源を活用することで、より効果的で個人化されたアライメントが可能になります。

### 1.2 主要な貢献
この研究は3つの重要な貢献を提供しています。

概念的貢献として、厳選されたフィードバックではなく、有機的な人間の相互作用から学習するパラダイムを導入しました。これは従来の専門家注釈に基づく手法からの根本的な転換を表しています。

方法論的貢献では、長期的なペルソナとターンレベルの好みをペルソナ条件付き好み最適化によって結びつける2つの補完的なRLHI手法を開発しました。ユーザーガイド書き直しとユーザーベース報酬の両手法は、ユーザーの個人的な好みと文脈を考慮した学習を可能にします。

実証的貢献として、個人化、指示追従、推論において複数のベンチマークで改善を実証しました。特に、個人化において60%以上の勝率を達成し、推論ベンチマークでも平均5.3ポイントの改善を示しています。これらの結果は、有機的な人間の相互作用がスケーラブルで効果的な個人化されたアライメントの監視を提供することを示唆しています。

## 2. 提案手法

### 2.1 手法の概要
RLHI（Reinforcement Learning from Human Interaction）は、実世界のユーザー会話から直接学習する新しいパラダイムです。従来の専門家による事前注釈フィードバックではなく、自然な人間とモデルの相互作用から得られる信号を活用します。

このアプローチの核心は、ユーザーの長期的な会話履歴から導出される「ペルソナ」の概念です。ペルソナは自然言語で表現され、ユーザーの専門知識レベル、情報量の好み、トーン、構造などの好みを捉えます。

WildChatデータの分析により、以下のことが明らかになりました。59.8%のユーザーが専門レベルを好み、49.9%が詳細な情報を好みます。また、84.5%が真剣・フォーマルなトーンを好み、77.1%が構造化された応答を好みます。

RLHIは2つの補完的な手法で構成されています。第一に、ユーザーガイド書き直しでは、ユーザーが提供するフォローアップフィードバック（26.51%の相互作用で発生）に基づいて不満足なモデル出力を修正します。第二に、ユーザーベース報酬では、明示的なフィードバックのない初期リクエストに対して、ペルソナ条件付き報酬モデルを用いて候補応答をランク付けします。

両手法は、ペルソナ条件付きDPO（Direct Preference Optimization）を使用してトレーニングされ、ユーザーの個人的な好みと文脈を明示的に考慮します。これにより、汎用的な応答ではなく、各ユーザーの特定のニーズに合わせた個人化された学習が可能になります。

### 2.2 技術的詳細
**ユーザーガイド書き直し手法**は、ユーザーの自然言語フィードバックを活用してモデル出力を改善します。プロセスは以下の通りです。まず、ユーザーからの初期クエリに対してモデルが応答を生成します。ユーザーが不満足な場合、フォローアップで具体的な改善要求を提供します。システムはこのガイダンスに基づいて応答を書き直し、元の応答よりも改善された応答として好み対を作成します。

技術的実装では、ペルソナ条件付きDPOを使用します。損失関数は以下のように定義されます。
```
L_persona-DPO = E[log σ(β(log π_θ(y+|x,persona)/π_ref(y+|x,persona) - log π_θ(y-|x,persona)/π_ref(y-|x,persona)))]
```
ここで、personaはユーザーペルソナ、y+は好ましい応答（書き直し後）、y-は好ましくない応答（元の応答）を表します。

**ユーザーベース報酬手法**は、明示的なフィードバックのないシナリオで機能します。初期リクエストに対してN個の候補応答を生成し、ペルソナ条件付き報酬モデルを使用してランク付けします。最高スコアと最低スコアの候補から好み対を作成し、オフラインまたはオンラインDPOでトレーニングします。

報酬モデルは、ユーザーの長期的な相互作用履歴から導出されたペルソナに明示的に条件付けられます。これにより、各ユーザーの個人的な好みパターンを学習し、汎用的な報酬信号ではなく個人化された評価を提供します。

**ペルソナ導出プロセス**では、ユーザーの過去の会話履歴を分析します。専門知識レベル、好ましい情報量、トーンの好み、構造の好みなどの特徴を自然言語で記述します。これらのペルソナは、両手法において応答生成と評価の文脈として使用され、各ユーザーに特化した学習を可能にします。

### 2.3 新規性
この研究の主要な技術的新規性は複数の側面にわたります。まず、**実世界での相互作用からの学習**において、従来の専門家注釈や実験室設定とは対照的です。自然な人間とモデルの相互作用から直接学習する最初の包括的なアプローチを提案しています。これにより、より authentic で多様なフィードバック信号を活用できます。

**ペルソナ条件付き好み最適化**は新しい概念です。ユーザーの長期的な履歴から導出された自然言語ペルソナを使用して、好み学習を個人化します。これは従来の一律的なアライメント手法とは根本的に異なるアプローチです。

**デュアル手法統合**では、明示的フィードバック（ユーザーガイド書き直し）と暗黙的評価（ユーザーベース報酬）を統合した包括的なフレームワークを提供します。これにより、様々なタイプの相互作用シナリオをカバーできます。

**スケーラブルな個人化**を実現しており、大規模なユーザーベースでの個人化を可能にします。1268人の多様なユーザーからのデータを使用し、少数の高頻度ユーザーよりも多様性が重要であることを示しています。

**オーガニック監視信号**の活用により、人工的に作成された注釈タスクではなく、実際のユーザーニーズから生じる自然な監視信号を使用します。これにより、より authentic で実用的なアライメントが可能になります。

## 3. 実験結果

### 3.1 実験設定
実験は包括的なデータセットと評価フレームワークで実施されました。**ベースデータセット**として、WildChat-1Mを使用しました。これは100万以上のChatGPT会話を含む実世界の会話データセットです。処理過程では、WildLlamaChat を作成するため、ユーザーメッセージを保持し、アシスタント応答をLlama-3.1-8B-Instructで再生成しました。

品質管理として、厳格なフィルタリングとユーザー選択基準を適用しました。また、PRM800K データセットから10,000件の数学会話を合成し、推論データとして使用しました。最終的に1268人の多様なユーザーからの高品質な会話データを使用してトレーニングしました。

**評価手法**は複数の側面から構成されています。WildChat UserEvalでは、実世界のユーザークエリに対する個人化と全体的な品質を評価します。標準ベンチマークとして、AlpacaEval 2.0、Arena-Hard、および複数の推論ベンチマーク（Minerva、OlympiadBench、GPQA、MMLU-Pro）を使用しました。

人間による研究では、実際のユーザーにモデル出力を評価してもらい、自動評価の妥当性を確認しました。また、包括的なアブレーション研究を実施し、各コンポーネントの貢献を分析しました。

### 3.2 主要な結果
**個人化評価**において、両RLHI手法は顕著な改善を示しました。ユーザーガイド書き直し手法は62.5%の個人化勝率と54.9%の全体的なUserEval スコアを達成しました。ユーザーベース報酬手法は61.0%の個人化勝率と51.3%の全体的なUserEval スコアを達成しました。これらの結果は、従来のベースモデルを15-20%上回っています。

人間による研究の検証では、それぞれ72.6%と74.0%の勝率を達成し、自動評価結果の妥当性を確認しました。この高い勝率は、実際のユーザーが RLHIで訓練されたモデルの出力を明確に好むことを示しています。

**標準ベンチマーク**では、ユーザーベース報酬を用いたRLHIがAlpacaEval 2.0で77.9%の長さ制御勝率を達成しました。Arena-Hardでは64.3スコアという競争力のある性能を示し、ユーザー非依存の報酬手法を上回りました。これらの結果は、個人化された学習が汎用性能も向上させることを示しています。

**推論ベンチマーク**において、ユーザーガイド書き直し手法は平均精度を26.5%から31.8%に改善しました。具体的には、Minervaで20.2%から25.4%、OlympiadBenchで14.5%から18.4%への改善を示しました。またGPQAで26.3%から33.1%、MMLU-Proで44.9%から50.1%への改善も確認されました。これは個人化手法が推論能力の向上にも寄与することを示しています。

### 3.3 既存手法との比較
**アブレーション研究**により、各コンポーネントの重要性が明らかになりました。ユーザーガイダンス対再生成の比較では、ユーザーガイド書き直しが一から再生成するよりも60.4%の勝率を達成しました。これはユーザーの具体的なフィードバックの価値を示しています。

強化学習と教師あり学習の比較では、強化学習が一貫して教師あり微調整を上回りました。これは好み最適化の有効性を確認しています。品質フィルタリングの重要性も確認され、フィルタリングなしでは+2.5ポイントの改善に留まったのに対し、フィルタリングありでは+23.4ポイントの大幅な改善を達成しました。

**ユーザー多様性**の分析では、1268人の多様なユーザーでのトレーニングが、10人の高頻度ユーザーでのトレーニングを上回ることが示されました。これは、量よりも多様性が重要であることを示唆しています。また、WildChatデータセットが既存のデータセットよりも高い多様性を持つことが埋め込み空間の可視化により確認されました。

**比較分析**では、従来の専門家注釈ベースの手法と比較して、RLHIが個人化と汎用性能の両面で優位性を示しました。特に、実世界のユーザー好みを反映する能力において、従来手法では捉えられない微妙な好みの違いを学習できることが確認されました。

## 4. 実用性評価

### 4.1 実装の容易性
RLHIフレームワークは実装の容易性を考慮して設計されています。既存のDPO（Direct Preference Optimization）フレームワークを拡張し、ペルソナ条件付けを追加する形で実装できます。主要なコンポーネントは明確に分離されており、段階的な導入が可能です。

データ処理パイプラインは自動化されており、WildChatのような実世界会話データから自動的にペルソナを抽出し、好み対を生成します。品質フィルタリング基準は事前定義されており、新しいデータセットにも適用可能です。

ユーザーガイド書き直し手法は、既存の会話システムに比較的簡単に統合できます。ユーザーフィードバックの検出と応答の書き直し生成は、標準的な言語モデルAPIを使用して実装できます。ユーザーベース報酬手法も、報酬モデルの訓練と候補応答のランキングという標準的な手順で実装可能です。

### 4.2 計算効率
計算コストの観点から、RLHIは追加のオーバーヘッドを伴いますが、管理可能な範囲内です。ペルソナ抽出は一度行えば再利用可能で、長期的なコストは限定的です。DPOベースの訓練は既存の好み最適化手法と同等の計算コストです。

ユーザーベース報酬手法では、複数の候補応答生成が必要ですが、これは推論時のバッチ処理で効率化できます。報酬モデルの追加的な推論コストは、個人化による品質向上と比較して合理的です。

スケーラビリティの面では、ペルソナのキャッシュ機能により、大規模ユーザーベースでも効率的に動作します。オフラインでの好み対生成により、訓練時の計算コストを分散できます。実験結果では、1268人のユーザーデータでも実用的な訓練時間で完了することが確認されています。

### 4.3 応用可能性
RLHIフレームワークは会話AI分野で広範な応用可能性を持ちます。カスタマーサポート、教育アシスタント、クリエイティブライティング支援など、個人化が重要な様々な領域で活用できます。特に、長期的なユーザー関係を築く必要があるアプリケーションでは大きな価値を提供します。

多言語展開の可能性も高く、ペルソナ抽出と好み学習の原理は言語に依存しません。異なる文化的背景を持つユーザーの好みパターンを学習することで、グローバルなサービスの個人化に貢献できます。

企業環境では、社内チャットボットや知識管理システムでの活用が期待されます。従業員の作業スタイルや好みを学習し、より効果的な情報提供やサポートを実現できます。教育分野では、学習者の理解レベルや学習スタイルに適応した個人化された指導が可能になります。

リアルタイム学習への拡張により、ユーザーとの継続的な相互作用から動的に学習する適応型システムの構築も可能です。これにより、従来の静的なモデルでは不可能だった継続的な改善と個人化の深化を実現できます。

## 5. まとめと所感

### 5.1 論文の意義
この論文は、AI アライメント分野において重要な方向転換を提示しています。従来の専門家による事前注釈フィードバックから、実世界のユーザー相互作用に基づく学習への転換は、より authentic で実用的なアライメント手法への道を開いています。

RLHIフレームワークの最も重要な貢献は、個人化と汎用性能の向上を同時に実現したことです。従来の研究では、個人化は特化した設定でのみ有効とされることが多かったですが、この研究は推論ベンチマークを含む多様なタスクでの改善を示し、個人化された学習の広範な適用可能性を実証しています。

ペルソナ条件付き学習の概念は、今後の個人化AI研究において重要な方向性を示しています。ユーザーの長期的な履歴から導出される自然言語ペルソナは、解釈可能で制御可能な個人化を可能にします。ブラックボックス化を避けながら効果的な適応を実現しています。

方法論的な観点から、明示的フィードバック（ユーザーガイド書き直し）と暗黙的評価（ユーザーベース報酬）を統合したデュアルアプローチは重要です。これは実世界の多様な相互作用シナリオを包括的にカバーする洗練されたフレームワークを提供しています。

### 5.2 今後の展望
この研究は将来的な発展方向について多くの可能性を開いています。まず、**プライバシー保護個人化**の研究により、ユーザーの個人情報を保護しながら効果的な個人化を実現する手法の開発が重要な課題となります。連合学習や差分プライバシーと組み合わせることで、プライバシーと性能のバランスを取ることができるでしょう。

**オンライン継続学習**への拡張により、固定された訓練データではなく、継続的なユーザー相互作用から動的に学習するシステムの開発が期待されます。これにより、ユーザーの好みの変化や新しいタスクへの適応が可能になります。

**マルチモーダル拡張**では、テキストベースの会話だけでなく、音声、画像、動画を含む多様なモダリティでの個人化学習が可能になります。これにより、より豊かで自然なユーザー体験を提供できます。

**安全性と倫理**の観点では、悪意のある入力や有害なフィードバックへの対処、バイアスの軽減、公平性の確保などの重要な課題があります。ロバストな安全機構の開発と継続的な監視システムの構築が必要です。

**スケーラビリティの向上**により、より大規模なユーザーベースと多様なタスクカバレッジに対応できるシステムの開発が求められます。計算コストを削減する分散学習と段階的な個人化手法の研究が重要になるでしょう。

この研究が示した「実世界の人間相互作用から学習する」というパラダイムは、「時間とともに改善する有能で個人化されたアシスタント」の構築に向けた重要な一歩です。将来的には、このアプローチがAIシステムの標準的な学習方法となり、より人間中心で実用的なAI の実現に貢献することが期待されます。