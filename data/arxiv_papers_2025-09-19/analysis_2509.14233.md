# Apertus: Democratizing Open and Compliant LLMs for Global Language Environments

## 基本情報
- arXiv ID: 2509.14233v1 (https://arxiv.org/abs/2509.14233)
- 著者: Project Apertus（101名の著者による大規模共同プロジェクト、Core Team含む）
- 所属: EPFL、Mistral AI、Meta、Google DeepMind、多数の大学・研究機関
- 投稿日: 2025年09月22日  
- カテゴリ: cs.CL, cs.AI

## 簡単に説明すると

Apertusは、現在のLLM開発における重要な課題を解決することを目的とした大規模なオープンソースLLMプロジェクトです。従来のモデルが抱えるデータコンプライアンス、多言語対応、記憶化問題、透明性の欠如といった根本的な限界に取り組んでいます。

このプロジェクトでは、8Bパラメータと70Bパラメータの2つのスケールで、15兆トークンでトレーニングされたモデルを提供しています。特筆すべきは、約620の言語を含む多言語データセットでの学習と、robots.txtを遵守したデータコンプライアンス、記憶化を防ぐGoldfish目的関数の使用です。

技術的革新として、新しいxIELU活性化関数、AdEMAMix最適化器、QK-Norm、そして効率性を30-40%改善した独自のアーキテクチャを採用しています。完全なオープンソース化により、モデル重み、学習コード、データセット、評価スクリプトなど全ての成果物が公開されています。

多言語評価では従来モデルを上回る性能を示し、特にアフリカ言語やユーラシア言語での評価は過去最大規模となっています。Hugging Faceとコード一式がGitHubで利用可能です。

## 1. 研究概要

### 1.1 背景と動機

現在のLLM開発エコシステムには、グローバルなユーザーのニーズを軽視する系統的な問題が存在します。多くの強力なモデルが登場していますが、その設計決定は往々にしてデータコンプライアンスと多言語性の重要性を見落としています。

第一の問題はデータコンプライアンスです。今日のオープンモデルの多くは実際にはオープンソースでも再現可能でもなく、単にオープンウェイト（重みのみ公開）に過ぎません。これらのモデルは学習に使用されたデータを公開せず、トークン数以外はほとんど情報を明かしません。さらに、コンテンツ所有者のアクセス権を考慮しない大量の違法素材を含んでいると疑われています。

第二の問題は多言語表現の制限です。現在のモデルの多くは単一言語か、高リソース言語の小さなサブセットのみに焦点を当てており、低リソース言語環境での拡張を制限しています。BLOOMやAya、Qwen3などは例外的ですが、それでも本研究の約10分の1の言語数にとどまっています。

これらの問題は、AI技術へのアクセスの民主化を阻害し、グローバルな言語環境での公平性を損なっています。特に、EU AI法のようなデータ規制が強化される中で、コンプライアントなモデル開発は急務となっています。

### 1.2 主要な貢献

Apertusプロジェクトは、これらの課題に対する包括的なソリューションを提供します。その貢献は以下の5つの主要な軸に分かれています。

**スケールの貢献**として、Apertus 70Bモデルは、このスケール（70Bパラメータ、15兆トークン）で学習された初の完全オープンモデルです。最大4096GPU での学習を実現するため、xIELUやAdEMAMix、QRPOなどの建築・学習革新を実装し、大規模学習を安定化しました。

**データコンプライアンスの貢献**では、プレトレーニングコーパスを完全にウェブデータから構築し、クロール時点（2025年1月）だけでなく、過去のクロールに対しても遡及的に2025年1月のオプトアウト設定を適用してrobots.txtを尊重しています。ポストトレーニングで使用されたすべてのデータセットも同様に、非許可ライセンスの下でリリースされたデータなどの非準拠データについてフィルタリングされています。

**記憶化防止の貢献**として、ApertusモデルはGoldfish目的関数を使用してプレトレーニングされ、モデルのテキスト再生成能力を制約しています。この手法が、大規模モデルスケールでも、128回の学習中露光後でも、逐語的想起を効果的に抑制することを実証しています。

**多言語性の貢献**では、プレトレーニング中に620言語からの15兆トークンでモデルを学習し、FineWeb-2ウェブクロールデータセットを使用しています。これらの学習した一般能力を、ポストトレーニングで100言語のデータで運用化しています。文化的、知識的、指示追従ベンチマークでさらに100言語（これまでオープンLLM学習で考慮されたことのないアフリカ言語を多数含む）でモデルをテストしています。

**透明性の貢献**として、Apertusは完全にオープンなモデルです。Apertusモデルスイートの重みのリリースとともに、ソースコード、最終・中間モデルチェックポイント、学習データの再現スクリプト、評価スイート、そしてこの技術レポートを含む完全な再現アーティファクトセットを提供しています。

## 2. 提案手法

### 2.1 手法の概要

Apertusアーキテクチャは、Transformerベースの密な decoder-only アーキテクチャです。基本設計は深いTransformerブロックのスタックで構成され、各ブロックにはマルチヘッド自己注意メカニズムとフィードフォワードネットワーク（MLP）が含まれ、各サブレイヤーの周りに残差接続と正規化が適用されています。

8Bモデルは32層と32の並列注意ヘッドを持ち、70Bモデルは80層と64の並列注意ヘッドを持ちます。両モデルともグループドクエリアテンション（GQA）、RoPE、RMSNormなどの確立された修正に加えて、QK-NormとxIELU活性化関数を使用してアーキテクチャ効率を向上させています。

特に重要な設計選択として、Pre-NormとRMSNormの使用により学習安定性を向上させ、すべてのバイアス項を除去してメモリ効率を改善しています。また、入力埋め込み重みと出力埋め込み重みを分離し、性能向上を図っています。

### 2.2 技術的詳細

**xIELU活性化関数**は本プロジェクトの主要な技術革新の一つです。この関数は以下のように定義されます：

xIELU(x) = αₚx² + 0.5x (x > 0の場合)
         = αₙ(eˣ - 1) - αₙx + 0.5x (x ≤ 0の場合)

ここで、αₚとαₙは層ごとの学習可能なスカラーです。xIELUはSquared ReLUを負の入力を処理するように拡張したものです。

**AdEMAMix最適化器**を使用し、従来の最適化手法と比較して学習効率を大幅に改善しています。1Bおよび3Bスケールでの実験では、この最適化器により30-40%の効率改善が達成されています。

**QK-Norm**は注意層のクエリとキーを正規化し、過度に大きな注意ロジットを防いで学習安定性を向上させます。これは大規模学習において特に重要な要素となっています。

**Goldfish目的関数**により記憶化を防止します。この手法は、128回の学習中露光後でも逐語的想起を効果的に抑制し、モデルが学習データを単純に記憶するのではなく、一般化可能な表現を学習することを促進します。

**多言語トークナイザー**として、Mistral-Nemoのv3 tekkenトークナイザーを採用しています。このトークナイザーは多言語文書の大部分に対応するよう設計されており、語彙サイズは131,072サブワードです。複数の主要LLMトークナイザーとの比較評価により、fertility rate、compression ratio、vocabulary utilization、Gini係数の4つの指標でMistral-Nemoが最適な性能を示すことが確認されています。

### 2.3 新規性

Apertusの最大の新規性は、技術的革新とコンプライアンスアプローチの統合にあります。xIELU活性化関数は従来のSquared ReLUを拡張し、負の入力に対する処理を改善した完全に新しい活性化関数です。

AdEMAMix最適化器との組み合わせにより、従来手法と比較して30-40%の効率改善を実現しています。これは大規模学習において重要な進歩であり、計算資源の制約下での高性能モデル開発を可能にします。

データコンプライアンスの観点では、robots.txt の遡及的適用という革新的なアプローチを採用しています。これまでのモデルがクロール時点でのオプトアウト設定のみを尊重していたのに対し、Apertusは過去のクロールデータに対しても最新のオプトアウト設定を適用することで、より厳格なコンプライアンス基準を確立しています。

Goldfish目的関数の大規模適用も新規性の一つです。記憶化防止手法としては知られていましたが、70Bスケールでの実証的検証は初めてであり、15兆トークンの学習においても効果的であることが示されています。

## 3. 実験結果

### 3.1 実験設定

評価は、プレトレーニングからポストトレーニングアライメントまでの各段階で、その時点でモデルが発達することが期待される特定の能力に合わせたベンチマークを使用して実施されています。これらのベンチマークは幅広いタスクとドメインにわたり、包括的なスキルカバレッジを確保しています。

評価には英語と多言語の両方のベンチマークが含まれ、多言語LLMの最も広範で言語的に多様な評価の一つとなっています。特に、アフリカ言語とユーラシア言語での最も徹底的な評価であり、合計100以上の言語をカバーしています。

比較対象モデルは、オープンウェイトモデルと完全オープンモデルの2つのカテゴリに分類されます。オープンウェイトモデルはチェックポイントを提供しますが、学習データやコードなどのすべてのコンポーネントを完全にリリースしません。完全オープンモデルは、モデル重みだけでなく、学習レシピ、データセット、完全な再現性のためのコードもリリースします。

ベンチマーク評価には、EleutherAIのlm-evaluation-harnessフレームワークを確率的スコアリングで使用しています。この手法により、単純な生成精度よりも敏感なモデル進歩の測定が可能になり、初期段階で低いままか緩やかにしか変化しない可能性のある学習ダイナミクスをより細かく観察できます。

### 3.2 主要な結果

プレトレーニング評価では、一般言語理解と事実知識獲得の2つの主要領域に焦点を当てています。多言語性能への関心から、言語に依存しない事実知識と地域固有の事実知識の間のニュアンスを捉えることを目指しています。

使用されたベンチマークには、HellaSwag、ARC、WinoGrande、XNLI、PIQA、COPAとその多言語バリアントが含まれています。言語に依存しない事実想起と推論の評価には、MMLUとGlobal-MMLUを使用しています。地域固有の事実知識については、INCLUDE、BLEnD、CulturalBenchを使用し、さらにスイスの地域知識をターゲットとするカスタムベンチマークSwitzerlandQAを導入しています。

結果として、Apertusモデルは同スケールクラス内の既存モデルと比較して優れた性能を示しています。特に多言語ベンチマークにおいて、オープンな最先端性能を達成し、いくつかの設定では単純なオープンウェイト対応モデルさえも上回っています。

ポストトレーニング評価では、知識、数学、推論、文化、安全性の5つの主要カテゴリで評価を実施しています。各カテゴリにおいて、Apertusモデルは競合するオープンモデルと同等以上の性能を示し、特に多言語設定での優位性が顕著です。

### 3.3 既存手法との比較

Apertusは、OLMo2、EuroLLM、SmoLM2、SmollLM3、Poroなどの完全オープンモデル、およびLlama3、Llama 4、Qwen2.5、Qwen3、GPT-OSSなどのオープンウェイトモデルと比較されています。

特に注目すべきは、文化的知識と地域固有の情報に関するベンチマークでの性能です。SwitzerlandQAベンチマークでは、英語、イタリア語、フランス語、ドイツ語、ロマンシュ語での評価を実施し、地域特化型の知識獲得能力を実証しています。

アフリカ言語での評価では、これまでオープンLLM学習で考慮されたことのない多数の言語を含む包括的な評価を実施し、多言語LLMとしての汎用性を証明しています。これらの評価は、Apertusが真にグローバルな言語環境でのLLM民主化を実現していることを示しています。

## 4. 実用性評価

### 4.1 実装の容易性

Apertusプロジェクトは完全な透明性を重視しており、実装の容易性においても優れています。NVIDIA's Megatron-LMをベースとしたコードベースは、複数の機能拡張（データローダー形式、学習中のログ記録など）とアーキテクチャに必要な修正（活性化関数、損失、最適化器）を含んでいます。

すべてのプレトレーニングと長コンテキスト学習スクリプトが公開されており、研究者や開発者は容易に再現や拡張を行うことができます。Hugging Faceでのモデルリリースとともに、GitHub上でのコード公開により、実装のしやすさが確保されています。

AdEMAMix最適化器とxIELU活性化関数の実装も含まれており、30-40%の効率改善を独自に検証・適用することが可能です。また、Goldfish目的関数の実装により、記憶化防止機能も容易に利用できます。

### 4.2 計算効率

計算効率の面では、複数の革新により大幅な改善を実現しています。xIELU活性化関数とAdEMAMix最適化器の組み合わせにより、従来手法と比較して30-40%の効率改善が達成されています。

FP8精度の使用とoutlier protected blockにより、メモリ使用量を削減しながら数値安定性を維持しています。QK-Normによる学習安定性の向上は、大規模学習において特に重要で、学習の失敗率を大幅に削減しています。

70Bモデルの学習には最大4096GPUを使用していますが、効率改善により同規模の他モデルと比較して学習時間とエネルギー消費を削減しています。グループドクエリアテンション（GQA）の採用により、推論効率も向上しています。

### 4.3 応用可能性

Apertusの応用可能性は非常に広範囲にわたります。第一に、データコンプライアンスを重視する設計により、EU AI法やその他の規制に準拠したAIシステムの開発基盤として活用できます。これは商用利用においても重要な要素となります。

620言語での学習により、低リソース言語を含む広範囲な言語環境でのアプリケーション開発が可能です。特に、アフリカ言語やユーラシア言語での性能は、これらの地域でのAI技術普及に大きく貢献すると期待されます。

完全オープンソース化により、研究用途だけでなく、商用アプリケーション開発、教育利用、カスタマイズ開発など多様な用途に対応しています。記憶化防止機能は、プライバシーを重視するアプリケーションでの利用価値を高めています。

長コンテキスト対応（65,536トークン）により、文書解析、長文生成、複雑な推論タスクなどの高度なアプリケーションにも対応可能です。多言語tokenizer の公平性により、言語間での性能格差を最小化し、真のグローバル利用を実現しています。

## 5. まとめと所感

### 5.1 論文の意義

Apertusプロジェクトは、現在のLLM開発エコシステムが抱える根本的な問題に対する包括的なソリューションを提供しています。データコンプライアンス、多言語性、記憶化防止、完全な透明性という4つの軸での革新は、AI技術の民主化において画期的な意義を持ちます。

特に重要なのは、robots.txtの遡及的適用という新しいコンプライアンス基準の確立です。これは今後のLLM開発における新しいベストプラクティスとなる可能性があり、著作権やプライバシー保護の観点から極めて重要な進歩です。

101名の研究者による大規模共同プロジェクトとして、学術界の連携の新しいモデルも示しています。複数の機関が協力して大規模LLMを開発し、すべての成果物を完全にオープン化することで、AI研究の新しいパラダイムを確立しています。

技術的には、xIELU活性化関数やAdEMAMix最適化器による30-40%の効率改善は、大規模学習の実現可能性を大幅に向上させています。これらの革新は、リソース制約のある研究機関でも高性能モデル開発を可能にする重要な貢献です。

### 5.2 今後の展望

Apertusプロジェクトが確立した基準は、今後のLLM開発における新しいスタンダードとなることが期待されます。データコンプライアンスへの厳格なアプローチは、規制強化が進む現在の環境において特に重要な意味を持ちます。

多言語性の拡大においては、620言語という規模は現時点での限界に近いものですが、今後は質的向上（各言語での性能改善）と新しい言語の追加の両方向での発展が期待されます。特に、現在データが不足している言語でのコーパス収集とモデル改善は重要な課題です。

技術的革新の観点では、xIELU活性化関数やAdEMAMix最適化器のさらなる改良と、他のアーキテクチャとの組み合わせ研究が進むと考えられます。また、Goldfish目的関数による記憶化防止技術の他のタスクへの応用も有望な研究方向です。

完全オープン化のアプローチは、AI技術の透明性と説明可能性の向上に大きく貢献します。今後は、このモデルをベースとした様々なカスタマイズモデルの開発と、それらの成果の共有により、AI技術のさらなる民主化が進むことが期待されます。

長期的には、Apertusが示したアプローチが業界標準となり、よりコンプライアントで公平なAI技術の発展が促進されることで、グローバルなAI技術格差の解消に貢献すると期待されます。