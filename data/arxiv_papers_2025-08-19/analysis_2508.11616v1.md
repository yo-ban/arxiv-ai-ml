# Controlling Multimodal LLMs via Reward-guided Decoding

## 基本情報
- arXiv ID: 2508.11616 (https://arxiv.org/abs/2508.11616)
- 著者: Oscar Mañas、Pierluca D'Oro、Koustuv Sinha、Adriana Romero-Soriano、Michal Drozdzal、Aishwarya Agrawal
- 所属: Mila - Quebec AI Institute、Université de Montréal、McGill University、Meta FAIR、Canada CIFAR AI Chair
- 投稿日: 2024年08月22日
- カテゴリ: cs.CV, cs.AI, cs.LG

## 簡単に説明すると
本論文は、マルチモーダル大規模言語モデル（MLLM）の推論時制御を実現する新しい手法「MRGD（Multimodal Reward-Guided Decoding）」を提案しています。従来のMLLMではオブジェクトの精度と再現率の間にトレードオフがあります。ユーザーが柔軟に制御できませんでした。本手法は2つの報酬モデル（幻覚抑制用と再現率向上用）を構築し、それらを線形結合することで推論時にオブジェクトの精度と再現率のバランスを動的に制御可能にしています。

## 1. 研究概要
### 1.1 背景と動機
マルチモーダル大規模言語モデル（MLLM）の普及に伴い、ユーザーの多様なニーズに応じてモデルの動作を制御したいという需要が高まっています。特に重要な制御軸として、(a) 出力の精度と詳細さ（オブジェクト再現率など）の制御、(b) 出力生成に費やす計算量の制御があります。

例えば、視覚障害者がMLLMを使用する場合、幻覚は非常に望ましくないため高精度な出力を求めますが、スマートフォンなどの限られた計算環境では低レイテンシも重要です。一方、下流モデル訓練用の合成キャプション生成では、精度は多少犠牲にしても多様で詳細な出力を優先し、より多くの計算リソースを使用する柔軟性があります。

### 1.2 主要な貢献
- MLLMに対する初の報酬誘導復号手法MRGDの提案
- オブジェクト精度と再現率のトレードオフを推論時に制御可能な仕組みの実現
- 計算量と視覚的グラウンディング品質のトレードオフ制御
- 標準的な幻覚評価ベンチマークにおける既存手法を上回る性能の実証
- 複数のMLLM（LLaVA-1.5、Llama-3.2-Vision、SmolVLM-2）での有効性確認

## 2. 提案手法
### 2.1 手法の概要
MRGDは以下の3つの主要コンポーネントから構成されます。

1. **マルチモーダル報酬モデルの構築**: オブジェクト幻覚用報酬モデル $r_{\text{hal}}$ と再現率用報酬モデル $r_{\text{rec}}$ を構築
2. 報酬誘導復号: 2つの報酬関数の線形結合によるスコア計算と探索ベース復号
3. **推論時制御**: 重み係数 $w$ とサンプル数 $k$ による柔軟な制御

### 2.2 技術的詳細
**オブジェクト幻覚報酬モデル $r_{\text{hal}}$**:
PaliGemmaをバックボーンとし、Bradley-Terryモデルに基づく選好データから訓練します。損失関数は以下の通りです。

$$\mathcal{L}(\theta) = \mathbb{E}_{(x, y^+, y^-)\sim D}[\mathcal{L}_{RM}(x, y^+, y^-; \theta) + (r^\theta_{\text{hal}}(x, y^+) - 1)^2 + r^\theta_{\text{hal}}(x, y^-)^2]$$

**オブジェクト再現率報酬モデル $r_{\text{rec}}$**:
事前訓練済みのOWLv2オブジェクト検出器、Sentence-BERT、品詞タガーを組み合わせて構築します。

$$r_\text{rec} = \frac{\sum_{i=1}^m \mathbb{I}(\max_{j=1,...,n} \mathrm{sim}_{ij} > \tau)}{n}$$

**マルチモーダル報酬誘導復号化**:
2つの報酬モデルの線形結合でスコアを計算し、探索ベースの復号化を行います。

$$s(x_v, x_q, y) = w \cdot r_\text{hal}(x_v, x_q, y) + (1-w) \cdot r_\text{rec}(x_v, x_q, y)$$

各ステップで $k$ 個の候補を生成し、最高スコアの候補を選択して文脈に追加するプロセスを繰り返します。

### 2.3 新規性
- MLLMに対する初の報酬誘導復号化手法
- 推論時におけるオブジェクト精度と再現率の動的制御
- 計算量と視覚的グラウンディング品質のトレードオフ制御
- マルチモーダル報酬モデルの効果的な設計と組み合わせ
- 文レベルでの段階的評価による効率的な探索

## 3. 実験結果
### 3.1 実験設定
- **データセット**: CHAIR（5k）、AMBER（1k）でのオブジェクト幻覚評価
- **ベースモデル**: LLaVA-1.5（7B）、Llama-3.2-Vision（11B）、SmolVLM-2（2.2B）
- **評価指標**: CHAIR_i/C_i（インスタンスレベル幻覚率）、CHAIR_s/C_s（文レベル幻覚率）、Rec./Cov.（オブジェクト再現率/カバレッジ）
- **比較手法**: 既存のファインチューニング手法（LLaVA-RLHF、HA-DPO、POVID等）および誘導復号化手法（VCD、CGD）

### 3.2 主要な結果
**LLaVA-1.5での性能向上**:
- w=1.0でCHAIR_iが15.05%から4.53%に約70%削減（COCOベンチマーク）
- w=0.5で精度と再現率の良好なバランスを実現
- w=0.0で最高の再現率85.23%を達成（幻覚率は増加）

**他モデルでの一般化性能**:
- Llama-3.2-Vision: CHAIR_iが5.82%から4.38%に削減
- SmolVLM-2: CHAIR_iが6.06%から4.32%に削減
- 全てのモデルで既存手法を上回る性能を実証

**計算効率性**:
- MRGD（k=5）が拒否サンプリング（k=30）より低い幻覚率を達成
- 拒否サンプリングより6倍以上のサンプル効率を実現

### 3.3 既存手法との比較
MRGDは一貫して以下の性能を示しました。
- ファインチューニング手法を全て上回る性能
- 誘導復号化手法CGDに対してCOCOで約50%、AMBERで約30%低い幻覚率
- LLaVA-RLHFやVCDは意外にもキャプション生成タスクで高い幻覚率を示す（VQAタスクとの相関の弱さを示唆）
- 再訓練不要で新しいMLLMに適用可能

## 4. 実用性評価
### 4.1 実装の容易性
学習不要の報酬モデル構築（$r_{\text{rec}}$）と軽量な選好データ学習（$r_{\text{hal}}$）により実装が容易です。PaliGemmaの30.6k選好例での訓練は8×H100 GPUで約9分で完了します。既存のMLLMに再訓練なしで適用可能な点も実用的です。

### 4.2 計算効率
k=30でのMRGDは約30倍多くのテキストを生成しますが、バッチ生成により実際のレイテンシ増加は30倍未満に抑えられています。報酬モデルはベースMLLMより大幅に小さく、評価は単一の順伝播のみで済むため、計算オーバーヘッドは制限的です。

### 4.3 応用可能性
- 視覚障害支援システムでの高精度画像理解
- 合成データ生成での詳細なキャプション作成
- 教育分野での適切な詳細レベルでの画像説明
- 医療画像解析での精度重視の記述生成
- マルチモーダルチャットボットでのユーザー適応型応答

## 5. まとめと所感
### 5.1 論文の意義
本研究はMLLMの推論時制御という重要な問題に対する実用的で効果的な解決策を提供しています。特に、オブジェクト精度と再現率の固有のトレードオフを明確に示し、それを制御可能にした点は画期的です。報酬誘導復号化の概念をマルチモーダル領域に拡張し、再訓練なしで既存モデルに適用可能な手法を確立したことの技術的・実用的価値は非常に高いといえます。

### 5.2 今後の展望
更なる発展として、より多様な報酬関数の組み合わせ、動的な重み調整機構、計算効率の更なる改善が期待されます。また、他のマルチモーダルタスク（画像生成、動画理解等）への拡張や、より大規模なモデルでの評価も興味深い研究方向です。ユーザーの実際のニーズに基づく重み選択の自動化機能も実用化において重要な発展領域となるでしょう。