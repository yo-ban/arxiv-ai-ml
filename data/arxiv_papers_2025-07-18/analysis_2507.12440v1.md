# EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos

## 基本情報
- **arXiv ID**: 2507.12440v1 (https://arxiv.org/abs/2507.12440)
- **著者**: Ruihan Yang, Qinxi Yu, Yecheng Wu, Rui Yan, Borui Li, An-Chieh Cheng, Xueyan Zou, Yunhao Fang, Hongxu Yin, Sifei Liu, Song Han, Yao Lu, Xiaolong Wang
- **所属**: UC San Diego, UIUC, MIT, NVIDIA
- **投稿日**: 2025年07月17日
- **カテゴリ**: cs.CV, cs.AI, cs.LG

## 簡単に説明すると
この論文は、一人称視点の人間動画からロボットの操作スキルを学習する「EgoVLA」というビジョン・言語・アクションモデルを提案しています。従来のロボット学習は実際のロボットによるデータ収集に依存していましたが、これはコストが高く、スケールが限定的でした。EgoVLAは、世界中で80億の「特殊なロボット」（人間）が日常的に生成している豊富な動画データを活用します。

主なアイデアは、人間の手首と手の動作を予測するVLAモデルを人間動画で事前学習し、その後少数のロボットデモンストレーションでファインチューニングすることです。人間とロボットの動作空間の違いは、逆運動学やリターゲティングによって変換できます。論文では、NVIDIA IsaacSimを使用した新しいヒューマノイド双腕操作ベンチマーク「Isaac Humanoid Manipulation Benchmark」も提案し、12のタスクでEgoVLAの有効性を実証しています。

プロジェクトのウェブサイト（https://rchalyang.github.io/EgoVLA/）では、動画による結果の確認ができます。また、実験ではNVILA-2Bという視覚言語モデルをバックボーンとして使用し、MANOハンドモデルを統一的な動作空間として採用しています。

## 1. 研究概要
### 1.1 背景と動機
ロボット操作の分野は、大規模な実ロボットデータ収集により近年大きな進歩を遂げています。シミュレーションを活用するアプローチと比較して、実ロボットデータによる教師あり学習はSim2Realのドメインギャップを回避でき、タスクの複雑性を容易に増加させることができます。複雑なロボット操作データを効率的に収集するため、関節マッピング、外骨格、VRデバイスなどを使用した複数の遠隔操作ツールが提案されています。

しかし、ロボットと専門的なオペレーターの必要性は、収集できるデータのスケールを根本的に制約しています。一方で、人間を特殊な形式のロボットと考えると、世界中で80億の「ロボット」が、私たちがロボットに動作させたいすべての環境で継続的に動作しています。

最近のHand-Object Interaction予測の研究は、操作における長期的な人間の意図予測において有望な結果を示しています。もし人間データを使用してロボットポリシーを訓練できれば、訓練データの数だけでなく、より重要なことに、タスクとシーンの多様性を容易にスケールアップできます。これにより、現在のロボットが容易に適合できないシーンや、遠隔操作でさえ困難なタスクでの訓練が可能になります。

### 1.2 主要な貢献
この論文の主要な貢献は以下の3点にまとめられます。

第一に、人間の一人称視点動画から学習し、ヒューマノイドロボットに転移できるビジョン・言語・アクションモデル「EgoVLA」を提案しました。このモデルは、人間の手首と手の関節角度を予測し、逆運動学とリターゲティングを通じてロボット動作に変換します。

第二に、4つのソース（HOI4D、HOT3D、HoloAssist、TACO）から約50万の画像-アクションペアを含む大規模な一人称視点人間操作データセットを構築しました。このデータセットは、ピックアンドプレース、再配向、関節式オブジェクトとの相互作用などの多様なタスクを含んでいます。

第三に、NVIDIA IsaacSimを使用した新しいヒューマノイド双腕操作ベンチマーク「Isaac Humanoid Manipulation Benchmark」を提案しました。このベンチマークには、短期的な原子的アクションと長期的な複合スキルを含む12のタスクが含まれ、各タスクに100のデモンストレーションが提供されています。

- 人間動画からロボット操作を学習する新しいパラダイムの確立
- 大規模な一人称視点人間操作データセットの構築
- 再現可能なヒューマノイド操作ベンチマークの提供

## 2. 提案手法
### 2.1 手法の概要
EgoVLAは、視覚言語モデルをベースに構築され、強力な視覚的・意味的推論能力を活用します。具体的には、NVILA-2Bをバックボーンとして使用し、堅牢な視覚言語理解とコンパクトなサイズにより、意図推論と効率的なファインチューニングの両方を可能にします。

EgoVLAは、現在および過去の一人称視点視覚観測、言語指示、アクションクエリトークン、人間の固有受容覚を入力として受け取ります。これらの入力はVLMバックボーンによってエンコードされ、アクションヘッドによってさらに処理されて、将来の人間またはロボットのアクションを予測します。

視覚観測は、現在の観測と5つの過去のフレームの合計6つのRGBフレームで構成され、0.2秒間隔でサンプリングされ、1秒の履歴をカバーします。各フレームの解像度は384×384です。言語指示は、即座の望ましい動作を記述します。この設計により、モデルは高レベルの計画ではなくスキル実行に焦点を当て、言語入力と予測されるアクション間の明確なマッピングを確保します。

### 2.2 技術的詳細
EgoVLAの技術的詳細は、以下の主要コンポーネントから構成されます。

**アクション空間の設計**では、各予測アクションは手首のポーズ（カメラフレームでの3D変換とrot6D表現での回転）と、MANOハンドモデルの上位15個のPCAコンポーネントを使用して表現される手の関節角度を含みます。EgoVLAは、カメラフレームでの将来の手首ポーズと手の関節パラメータを回帰するように訓練されます。

**損失関数**は、手首変換、手首回転、関節角度の3つの要素から構成されます。手首変換と関節角度にはL2損失を使用し、手首回転にはrot6D用の回転損失を使用します。これらは重み係数によってバランスが取られます。

**アクションヘッド**は、300Mパラメータのトランスフォーマーで、6つのエンコーダレイヤーから構成され、各レイヤーの隠れサイズは1536です。人間（またはロボット）の固有受容覚状態とアクションクエリトークンに対応する潜在埋め込みを入力として受け取り、両手に対して1秒間のホライズン（30Hz で30の将来ステップ）にわたるシーケンスを予測します。

**訓練の詳細**として、EgoVLAは最初に一人称視点人間操作データセットで20エポック事前学習されます。その後、ロボットデモンストレーションデータで115エポックのポストトレーニングが行われ、100エポック後に学習率が減少されます。訓練中、視覚エンコーダを含む完全なモデルがファインチューニングされます。

**ヒューマノイドロボットへの転移**では、統一されたアクション空間を使用します。MANOハンドパラメータは、人間とロボットの共有アクション空間として使用されます。ロボットハンドの場合、最適化されたMANOパラメータが、訓練中にロボットハンドの指先と同じ位置を生成します。展開時には、小さなMLPが予測された指先位置を関節コマンドにマッピングします。

### 2.3 新規性
EgoVLAの新規性は、以下の点にあります。

第一に、人間の一人称視点動画からロボット操作を学習するという新しいパラダイムを提案している点です。従来の手法がロボットデータの収集に依存していたのに対し、EgoVLAは豊富に存在する人間動画データを活用します。

第二に、人間とロボット間の統一されたアクション空間の設計です。MANOハンドモデルを共通の表現として使用し、逆運動学とリターゲティングによって人間とロボットのアクション空間を橋渡しします。この設計により、人間動画で学習したスキルを少数のロボットデモンストレーションでロボットに転移できます。

第三に、視覚言語モデルをベースとしたアーキテクチャにより、言語指示に基づいた柔軟な操作が可能になっている点です。これにより、同じ視覚入力に対して異なる言語指示で異なる動作を生成できることが実証されています。

## 3. 実験結果
### 3.1 実験設定
実験は、提案したIsaac Humanoid Manipulation Benchmarkを使用して行われました。このベンチマークは、Unitree H1ヒューマノイドロボットに2つのInspire巧緻ハンドを装備し、12の操作タスクを含んでいます。

タスクは2つのカテゴリに分類されます。
- 短期的タスク：Push-Box、Flip-Mug、Pour-Balls、Close-Drawer、Open-Drawer、Open-Laptop、Stack-Canなどの原子的アクション
- 長期的タスク：Sort-Cans、Insert-Cans、Unload-Cans、Insert-And-Unload-Cans、Stack-Can-Into-Drawerなどの複数段階のスキル

評価は2つの設定で行われました。「Seen」では、訓練中に遭遇した視覚的背景と同一の環境で評価し、「Unseen」では、完全に新しい背景で評価しました。ロバスト性を促進するため、各ロールアウトの開始時にオブジェクトの位置がランダム化され、配置は訓練中に見られなかった領域に制約されました。

### 3.2 主要な結果
定量的評価の結果、EgoVLAは複数の重要な発見を示しました。

**人間操作モデリング**において、人間の手首変換の平均将来予測誤差は約8cmでした。2D画像平面に投影した場合、正規化誤差は約0.13で、HOI-forecastで報告された最先端の結果と同等でした。言語指示を変更すると、予測される手の軌跡が適切に調整されることが確認されました。

**ロボット操作評価**では、EgoVLAは以下の結果を達成しました。

短期的タスクにおいて、Seen環境でEgoVLAは平均85.32%の成功率を達成し、EgoVLA-NoPretrain（62.63%）を大幅に上回りました。Unseen環境でも、EgoVLAは強い汎化性を維持し、わずかな性能低下のみでした。一方、EgoVLA-NoPretrainは23%の大幅な低下を示しました。

長期的タスクでは、Seen環境でEgoVLAは45.93%の成功率を達成し、EgoVLA-NoPretrain（26.67%）より約20%高い性能を示しました。Unseen環境では、成功率は低下しましたが、進捗率は同様に維持され、失敗が主にタスク実行の最終段階で発生することを示唆しています。

専門家モデル（ACT）との比較では、汎用モデル（EgoVLAとEgoVLA-NoPretrain）が短期的および長期的タスクの両方で大幅に優れた性能を示しました。

### 3.3 既存手法との比較
アブレーション研究により、以下の知見が得られました。

**ロボットデータスケールの影響**：ロボットデモンストレーションの50%のみを使用したEgoVLA（50%）は、一部のタスクを完了できましたが、全体的な成功率は大幅に低下し、特に長期的タスクでは45.93%から7.41%に低下しました。

**事前学習データ混合の影響**：異なるデータ混合で事前学習したEgoVLAを評価した結果、人間の事前学習データのスケールと多様性を増やすことで、下流のパフォーマンスが一貫して向上することが示されました。HoloAssistのノイジーな手のアノテーション、HOT3Dの欠落した言語ラベル、TACOの限定的な視覚的多様性にもかかわらず、正の転移が観察されました。

**空間的汎化**：オブジェクトの生成位置に条件付けられたモデルのパフォーマンスを分析した結果、短期的タスクでは、オブジェクトがランダム化領域の中心に近く配置された場合に成功率が高くなりました。長期的タスクでは、2つの異なる高確率領域が出現し、これは多くの長期的タスクが双腕操作を含むためと推測されます。

## 4. 実用性評価
### 4.1 実装の容易性
EgoVLAの実装は、既存の視覚言語モデルフレームワークを基盤として構築されているため、比較的容易です。NVILA-2Bをバックボーンとして使用し、標準的なトランスフォーマーアーキテクチャでアクションヘッドを実装しています。

統一されたアクション空間の設計により、人間とロボット間の転移が簡素化されています。MANOハンドモデルは広く使用されているツールであり、逆運動学とリターゲティングのための既存のライブラリを活用できます。

ただし、人間データには手首と手のポーズアノテーションが必要であり、これがデータの可用性を制限する可能性があります。しかし、Quest 3、Vision Pro、Aria Glassesなどの高忠実度AR/VRデバイスのアクセシビリティの向上により、この制約は緩和されることが期待されます。

### 4.2 計算効率
EgoVLAは2Bパラメータのモデルをベースとしており、比較的コンパクトです。アクションヘッドは300Mパラメータで、全体として効率的なアーキテクチャとなっています。

訓練は標準的なGPUで実行可能で、人間データでの事前学習に20エポック、ロボットデータでのファインチューニングに115エポックを要します。推論時は30Hzで動作し、リアルタイムロボット制御に適しています。

データ処理では、視覚観測を3FPSでサンプリングすることで、計算効率と時間的連続性のバランスを取っています。

### 4.3 応用可能性
EgoVLAは、様々な応用分野での利用が期待されます。

第一に、ヒューマノイドロボットの操作タスクにおいて、人間の豊富な操作動画から学習できるため、従来の手法よりも多様なスキルを獲得できる可能性があります。

第二に、視覚言語モデルをベースとしているため、自然言語による柔軟なタスク指定が可能です。これにより、同じロボットが異なる言語指示に基づいて異なるタスクを実行できます。

第三に、シミュレーション環境での評価により、安全で再現可能な開発サイクルが可能になります。Isaac Humanoid Manipulation Benchmarkは、研究コミュニティに標準的な評価プラットフォームを提供します。

ただし、現在のフレームワークでは、統一されたアクション空間で事前学習されているにもかかわらず、ロボットデータでのファインチューニングなしでは直接展開できないという制限があります。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、ロボット学習における重要なパラダイムシフトを提示しています。従来のアプローチがロボットハードウェアによるデータ収集に依存していたのに対し、EgoVLAは人間の日常活動から生成される豊富な動画データを活用します。

特に重要なのは、人間とロボット間の統一されたアクション空間の設計です。これにより、人間の操作スキルをロボットに効率的に転移できるようになり、大規模なロボットデータ収集の必要性を大幅に削減します。

また、視覚言語モデルをベースとしたアーキテクチャにより、言語指示に基づいた柔軟な操作が可能になっています。実験結果は、人間動画での事前学習が、限られたロボットデータでも強力な汎化性能を達成できることを示しています。

### 5.2 今後の展望
論文で指摘されている制限事項として、現在のフレームワークは手首と手のポーズアノテーションを持つ人間データを必要とすることが挙げられています。しかし、AR/VRデバイスの普及により、この制約は緩和されることが期待されます。

また、統一されたアクション空間で事前学習されているにもかかわらず、適度な量のロボットデータでのファインチューニングなしでは操作に直接展開できない点も改善の余地があります。将来の研究では、より実施形態に依存しない事前学習を通じて、ゼロショット転移可能性の向上が探求される可能性があります。

この研究は、人間の動画データという豊富なリソースをロボット学習に活用する新しい方向性を示しており、今後のロボティクス研究に大きな影響を与える可能性があります。