# CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models

## 基本情報
- arXiv ID: 2509.09675v1 (https://arxiv.org/abs/2509.09675)
- 著者: Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen Liang, Dian Yu, Haitao Mi, Zhaopeng Tu, Rui Liu, Tong Zheng, Hongtu Zhu, Dong Yu
- 所属: Tencent AI Lab, Tencent Multimodal Department, University of North Carolina at Chapel Hill, University of Virginia, University of Maryland College Park
- 投稿日: 2025年09月14日
- カテゴリ: cs.AI, cs.LG

## 簡単に説明すると

この論文は、大規模言語モデル（LLM）の推論能力を向上させるための新しい探索手法「Curiosity-Driven Exploration（CDE）」を提案しています。
従来のReinforcement Learning with Verifiable Rewards（RLVR）手法では、早期収束やエントロピー崩壊といった問題により、十分な探索ができずに性能が制限されていました。
CDEでは、モデル自身の「好奇心」を活用した探索を実行します。
具体的には、アクター（生成器）の好奇心として生成応答のperplexity（困惑度）を使用し、クリティック（評価器）の好奇心としてマルチヘッドアーキテクチャによる価値推定の分散を使用します。
これらの好奇心シグナルを探索ボーナスとしてRLVRフレームワークに統合することで、モデルをより効果的に導きます。
理論的解析では、アクター側のボーナスが過信エラーを内在的に抑制し、正答の多様性を促進することを示しています。さらにクリティック側のボーナスが確立されたcount-basedな探索ボーナスと同等であることを証明しています。
実験では、GRPO/PPOを用いたAIMEベンチマークで標準的なRLVRに対して約+3ポイントの改善を達成しています。さらにRLVR内の「較正崩壊（calibration collapse）」メカニズムを明らかにしました。

関連リンクについては、論文内に具体的なリンクは記載されていませんが、実装にはVerlフレームワークとQwen3-4B-Baseモデルが使用されています。

## 1. 研究概要
### 1.1 背景と動機

大規模言語モデルの推論能力は数学やコーディングなど多様な分野で顕著な性能を達成しています。しかし高品質なChain-of-Thought（CoT）推論を効率的に引き出すことが中核的課題です。
Reinforcement Learning with Verifiable Rewards（RLVR）は、最終回答の正確性シグナルを直接使用してモデルを最適化する画期的な訓練パラダイムですが、早期収束やエントロピー崩壊といった根本的な問題が観察されています。
これらの課題は強化学習における古典的な探索・活用ジレンマに由来しています。活用に偏重した訓練によってモデルが十分な探索をせず早期収束してしまいます。

従来の探索戦略（エントロピーボーナス、ε-greedy政策、count-based手法など）は、LLMに適用する際に重要な制限があります。
単純なヒューリスティクス手法は理論的に次善であったり、複雑な環境での効果が疑問視されています。
より原理的なcount-based手法は、行列逆変換などの集約的操作を要求します。また長いCoT推論パスの高表現力のある状態-行動対表現に依存するため、推論重視のLLMには実用的ではありません。

### 1.2 主要な貢献

本研究は、LLMのための効率的で拡張性のある探索手法の開発という重要な課題に対し、以下の4つの主要貢献を提供しています。

- 好奇心による探索（CDE）フレームワークの提案。モデル自身の内在的な好奇心感覚を活用した新しい探索アプローチを導入し、アクターとクリティック両方からの好奇心シグナルを統合しています。
- 理論的基盤の確立。アクター側のperplexityベースボーナスが過信エラーを抑制し、正答の多様性を促進することを証明しています。またクリティック側ボーナスが古典的count-based探索ボーナスと理論的に同等であることを示しています。
- 実証的性能改善の実現。4つの数学的推論ベンチマーク（AIME25、AIME24、AMC23、MATH）で一貫した性能向上を達成しています。特にAIMEベンチマークで約+3ポイントの改善を実現しています。
- **較正崩壊メカニズムの発見**: RLVR内の「較正崩壊」現象を明らかにし、モデルの信頼度と正確性が徐々に乖離していく問題とその解決策を提示

## 2. 提案手法
### 2.1 手法の概要

Curiosity-Driven Exploration（CDE）は、モデルの内在的な好奇心感覚を探索の指針として活用する革新的なフレームワークです。
CDEは、幼児期の発達において学習が外部的な経験の要約や集計ではなく、新奇な状況を探索する内在的好奇心によって推進されることに着想を得ています。

CDEはアクターとクリティック両方からの好奇心シグナルを考慮します。
アクター側では、生成した応答に対するperplexity（PPL）を好奇心の尺度として使用します。
クリティック側では、マルチヘッド・ブートストラップ構造に拡張されたPPOフレームワークで近似した価値関数の事後分布の分散を通じて好奇心を測定します。
これらの好奇心シグナルは探索ボーナスとして機能し、報酬と利得関数を形成して効果的な探索を導きます。

### 2.2 技術的詳細

**アクター好奇心による探索**

アクター好奇心は、アクターが自身の行動に対して持つ不確実性としてモデル化されます。
文レベルの好奇心ボーナスは、生成された文o={o₁, ..., oₜ}の負の平均対数確率として定義されます：

B_actor(q,o) = -1/T ∑log π(oₜ| o<ₜ, q)

実用的には、適応的クリッピングメカニズムを使用して統合されます：

r̃(q, o) = r(q,o) + ωₜ min(|r(q,o)|/κ, α B_actor(q,o))

ここで、ωₜ（ボーナス重み）、κ（クリッピング比）、α（ボーナススケーリング因子）が動作を制御します。

**クリティック好奇心による探索**

マルチヘッドクリティック構造を用いて価値推定の事後分布を近似し、K個のクリティック{V̂₁, ..., V̂ₖ}が共通のLLMバックボーンを共有します。
各ヘッドは収集された軌跡のリサンプルされたサブセットで訓練され、事後分布の経験的近似を生成します。

クリティック好奇心ボーナスは、K個の価値ヘッドの標準偏差として定義されます：

B_critic(q, o_{≤t+1}) = std({V̂_j(q, o_{≤t+1}) | 1 ≤ j ≤ K})

### 2.3 新規性

本手法の新規性は、従来の明示的な状態-行動カウントのパラダイムを超えて、モデル自身の新奇性の尺度を利用する点にあります。

理論的新規性として、たとえば次のような点がある。
- Perplexityベースボーナスが過信エラーを内在的に抑制し、正答の多様性を促進することの理論的証明
- マルチヘッドクリティックの標準偏差が線形MDPにおいて古典的count-based探索ボーナスと一致することの証明

技術的新規性として、たとえば次のような点がある。
- 計算集約的な行列逆変換操作の回避
- 高次元埋め込みの表現力不足問題の解決
- 既存訓練アーキテクチャへの最小限の変更での実装

実用的新規性として、たとえば次のような点がある。
- エントロピーボーナスとの明確な差別化（サンプル依存vs.サンプル非依存）
- 較正崩壊メカニズムの発見と解決
- 数学的推論タスクでの一貫した性能向上の実現

## 3. 実験結果
### 3.1 実験設定

実験はQwen3-4B-Baseモデルを基盤として、GRPOとPPO両方のアルゴリズムで実施された。
評価には4つの数学的推論ベンチマーク（MATH、AMC23、AIME24、AIME25）を使用している。
トレーニングはverlフレームワークを使用し、300ステップの訓練を行っている。

主要な実験設定として、GRPOではactor学習率1e-6、バッチサイズ256、ロールアウト数8を使用。
PPOではactor学習率1e-6、critic学習率1e-5、バッチサイズ512、ロールアウト数4を使用している。
CDE固有のハイパーパラメータとして、クリッピング比κ=3、スケーリング因子α=1（PPL用）または0.5（マルチヘッド用）を設定している。

### 3.2 主要な結果

実験結果は、CDEフレームワークが一貫した性能向上を達成することを示している。
特にAIMEベンチマークにおいて約+3ポイントの改善を実現している。

具体的な結果として、AIME24では標準GRPO（21.0）に対してPPLボーナス付きGRPO（23.5）、AIME25では20.8から23.3への改善を達成している。
MATHベンチマークでは87.3から87.7への改善、AMC23では63.6から67.8への改善を示している。

ボーナス重み減衰スケジュールの比較では、階段状減衰（Staircase decay）が最も良い性能を示し、総合平均50.6を達成している。
線形減衰（48.7）、コサイン減衰（49.7）、減衰なし（48.2）と比較して最高の結果となっている。

エントロピー動態の分析により、PPLボーナスがエントロピー崩壊を軽減し、探索能力を維持することが確認された。
特に階段状減衰スケジュールが安定したエントロピー軌道を示している。

### 3.3 既存手法との比較

標準的なRLVR手法との比較において、CDEは複数の重要な利点を示している。

まず較正崩壊（calibration collapse）現象の発見と解決が重要な成果である。
標準GRPO訓練では、正答の困惑度と誤答の困惑度の差が訓練の進行とともに縮小し、最終的に消失する現象が観察された。
しかしPPLボーナスを適用することで、この分離が訓練全体を通じて維持されることが確認された。

マルチヘッドクリティックの分散分析では、訓練データセット、ドメイン内検証セット（AMC23）、ドメイン外検証セット（GPQA）の順で標準偏差が大きくなることが示された。
これは、訓練中に頻繁に遭遇するデータほどクリティック間の合意が高いことを示している。

サブサンプル割合ζの感度分析では、16ヘッドと4ヘッドの両設定でζ=0.5と1.0の間で安定した性能を示し、手法の頑健性が確認された。

## 4. 実用性評価
### 4.1 実装の容易性

CDEフレームワークの実装容易性は高く評価できる。
既存のGRPOやPPOフレームワークへの統合に必要な変更は最小限である。

アクター好奇心（PPLボーナス）の実装は、生成された応答の対数確率を計算するだけで済むため、計算コストが低い。
クリティック好奇心（マルチヘッドクリティック）も、既存の価値関数の拡張として実装でき、大幅なアーキテクチャ変更は不要である。

ハイパーパラメータの設定も比較的単純で、クリッピング比κ、スケーリング因子α、ボーナス重みωtの3つの主要パラメータのみ調整すれば済む。
論文で提供された設定値（κ=3、α=1など）が良い出発点となる。

verlフレームワークでの実装が確認されており、実用的な訓練環境での適用可能性が示されている。

### 4.2 計算効率

CDEの計算効率は既存のcount-based手法と比較して大幅に改善されている。

従来のcount-based探索手法では、共分散行列の逆行列計算Λt^(-1)が必要で、高次元埋め込みでは計算量が禁止的になる。
CDEではこの問題を回避し、モデル自身の内在的な新奇性尺度を利用することで効率化を実現している。

PPLボーナスの計算は、既に生成プロセスで計算される対数確率の平均を取るだけなので、追加的な計算コストは negligible である。
マルチヘッドクリティックも、複数の価値ヘッドでの標準偏差計算のみで済む。

ハッシュベース手法との比較でも、埋め込みの表現力不足問題を解決しつつ、計算効率を維持している。
SimHash技法では多くのCoT軌跡が同じハッシュグリッドに集約される問題があったが、CDEはこの問題を回避している。

訓練時間への影響も軽微で、300ステップの標準的な訓練設定で安定した性能改善を達成している。

### 4.3 応用可能性

CDEフレームワークの応用可能性は広範囲にわたる。

数学的推論を超えて、コーディング、論理的推論、マルチモーダル推論などの他のRLVR適用領域への拡張が期待される。
好奇心の概念は領域に依存しない汎用的な原理であるため、様々な推論タスクに適用可能である。

較正崩壊の発見と解決は、LLMの幻覚問題の根本原因理解に寄与する可能性がある。
信頼度と正確性の関係を維持することで、解釈可能性と推論時選択戦略の改善が期待される。

アクター・クリティック両方からの好奇心シグナルという設計原理は、他の強化学習設定にも適用可能である。
特に長期的な行動系列を扱う環境では、内在的動機による探索が有効である可能性が高い。

多言語モデルや文化的コンテキストの異なる推論タスクへの適用も可能である。
好奇心の概念は文化や言語に依存しない認知的メカニズムであるため、グローバルな展開が期待できる。

教育分野での応用では、学習者の好奇心をモデル化した適応的学習システムの構築に活用できる可能性がある。

## 5. まとめと所感
### 5.1 論文の意義

この研究は強化学習ベースのLLM推論において重要な理論的・実用的貢献を提供している。

最大の意義は、従来の明示的状態-行動カウントのパラダイムから、モデル自身の内在的新奇性尺度を活用するアプローチへの転換である。
これは計算効率と表現力の両面で根本的な改善をもたらしている。

理論的貢献として、アクター側のperplexityベースボーナスが過信エラーを内在的に抑制し、正答の多様性を促進することの証明は重要である。
また、クリティック側ボーナスが線形MDPにおいて古典的count-based探索ボーナスと理論的に同等であることの証明も、手法の理論的基盤を確立している。

較正崩壊メカニズムの発見は、LLM訓練における根本的な問題を明らかにした点で特に価値が高い。
従来のRLVR訓練では信頼度と正確性が乖離する現象が見過ごされていたが、この研究により問題が可視化され、解決策も提示された。

実用的な意義として、既存フレームワークへの最小限の変更で一貫した性能改善を実現した点は、研究コミュニティでの採用を促進する重要な要因である。
AIMEベンチマークでの+3ポイント改善は、実際の数学的推論能力向上を示している。

### 5.2 今後の展望

この研究により開かれた研究方向は多岐にわたる。

短期的には、他の推論領域（コーディング、論理的推論、科学的推論）への適用が期待される。
好奇心の概念は領域汎用的であるため、数学以外の分野でも同様の効果が期待できる。

より高度な好奇心モデルの開発も重要な研究方向である。
現在のperplexityベースの手法を超えて、より洗練された不確実性推定や新奇性検出手法の統合が可能である。

較正崩壊現象の更なる理解と対策は、LLMの信頼性向上において重要である。
この現象が他の訓練パラダイムでも発生するか、どのような条件下で発生するかの調査が必要である。

マルチヘッドクリティックアーキテクチャの拡張も有望である。
現在の価値関数の分散ベース手法を超えて、より sophisticated なアンサンブル手法や不確実性推定手法の適用が考えられる。

長期的には、人間の好奇心や学習プロセスとの更なる整合性向上が期待される。
認知科学や発達心理学の知見を取り入れた、より生物学的にplausibleな探索メカニズムの開発が可能である。

計算効率の更なる改善も重要である。
現在でも既存手法より効率的だが、より大規模なモデルや複雑なタスクへの適用を考慮すると、更なる最適化の余地がある。

最終的に、この研究は強化学習とLLMの交差点における新しいパラダイムの基礎を築いており、今後の推論能力向上研究に重要な影響を与えると予想される。