# FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark

## 基本情報
- arXiv ID: 2509.09680v1 (https://arxiv.org/abs/2509.09680)
- 著者: Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li
- 所属: CUHK, HKU, BUAA, Alibaba
- 投稿日: 2025年09月14日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると

この論文は、Text-to-Image（T2I）生成モデルの推論能力を向上させるための大規模データセットと包括的な評価ベンチマークを提供しています。
具体的には、600万枚の高品質画像と2000万の多言語キャプション（英語・中国語）から成るFLUX-Reason-6Mデータセットを提案しています。
さらに、7つのタスクトラック（想像力、実体、テキストレンダリング、スタイル、感情、構図、長文）で構成されるPRISM-Benchも提供しています。
特に革新的なのは、画像生成の思考プロセスを段階的に説明するGeneration Chain-of-Thought（GCoT）アプローチの導入です。
これにより従来の記述的キャプションを超えて、なぜその画像がそのように構成されるのかという理由までモデルが学習できます。
データ作成には15,000 A100 GPU日という膨大な計算リソースが投入され、おそらく最も高価なオープンソースデータセットとなっています。

関連リンクについては、論文中でGitHubやHugging Faceなどのロゴが含まれていることから、データセットとベンチマークの公開が予定されていると推測されます。
しかし具体的なリンクは論文内では明示されていません。

## 1. 研究概要
### 1.1 背景と動機

Text-to-Image（T2I）生成モデルは、人工知能の重要な研究分野として急速に発展してきているが、現在、最先端のクローズドソースモデル（Gemini2.5-Flash-Image、GPT-Image-1など）と、オープンソースモデル（SDXL、PixArt、SD3シリーズなど）の間には顕著な性能差が存在している。
この格差は、複雑で詳細なプロンプトを処理する際に特に顕著に現れ、オープンソースモデルの実用性を制限する要因となっている。

この問題の根本的な原因は二つの重要な課題にある。
まず第一に、研究コミュニティには大規模で高品質、かつ包括的なオープンソースデータセットが不足している。
既存のデータセットの多くは、ウェブからクロールされた画像-テキストペア（LAION、Conceptual Captionsなど）で構成されており、これらのデータはT2Iモデルに推論能力を付与するには不適切である。
推論指向のデータセットも存在するが、それらは範囲が限定的で、例えばGoTデータセットは主にバウンディングボックスを通じたレイアウト計画に焦点を当てており、推論の他の重要な側面をカバーしていない。

第二に、人間の判断と整合性のある包括的な評価ベンチマークが存在しない。
既存のベンチマーク（GenEval、T2I-CompBench、TIFA、HRS等）は、限られた次元での評価しか行わず、想像力や感情表現などの重要な側面を軽視している。
また、これらのベンチマークは物体検出器やCLIPスコアといった粗い評価指標に依存しており、その結果として評価指標が飽和しやすく、モデルの実際の性能を効果的に区別できない問題がある。

### 1.2 主要な貢献

この研究は、推論ベースのT2I研究におけるパラダイムシフトを表現する四つの重要な貢献を提供している。

まず最も重要な貢献は、FLUX-Reason-6Mという画期的なデータセットの開発である。
これは推論のために特別に設計された初の600万規模のT2Iデータセットで、2000万の多言語キャプション（英語・中国語）と革新的なGeneration Chain-of-Thought（GCoT）プロンプトを特徴としている。
このデータセットは128基のA100 GPUを4ヶ月間使用して作成され、次世代T2Iモデルの基盤データセットとしての役割を果たすことを目指している。

第二に、PRISM-Benchという新たな評価標準の確立がある。
これは7つのトラックから成る包括的ベンチマークで、GPT-4.1とQwen2.5-VL-72Bを使用した微妙で堅牢な評価を提供し、研究コミュニティにモデルの真の能力を測定する信頼できるツールを提供している。

第三に、先進的なモデルの広範囲な評価から得られた実用的な知見の提供がある。
この厳密で包括的な評価により、異なるモデル間のギャップと改善の可能性がある分野が明らかになり、将来の研究のための明確なロードマップを提供している。

最後に、T2I分野における民主化革命への貢献がある。
データセット、ベンチマーク、評価スイートの完全な公開により、参入の財政的・計算的障壁を下げ、世界中の研究者がより能力の高い生成モデルを構築・テストできるようになることを目指している。

## 2. 提案手法
### 2.1 手法の概要

FLUX-Reason-6Mの核心は、T2I生成に不可欠な6つの特性を中心とした多次元アーキテクチャ設計にある。
これらの特性は相互排他的ではなく、意図的に重複するように設計されており、複雑なシーン合成の多面的な側面を反映することで、より豊富で堅牢な訓練信号を提供している。

6つの特性は以下の通りである：想像力（Imagination）は超現実的、幻想的、抽象的概念を扱い、現実世界の物理法則に逆らうシナリオや異なるアイデアを新しい方法で組み合わせたシーンを含む。
実体（Entity）は知識に基づいた描写に焦点を当て、特定の現実世界の物体、存在、固有名詞の正確で詳細な生成を扱う。
テキストレンダリング（Text rendering）は生成モデルの既知の弱点に対処し、英語テキストを読みやすく組み込んだ画像から成る。
スタイル（Style）は芸術的・写真的スタイルの膨大で多様なライブラリを厳選し、特定の芸術運動、視覚技法、有名アーティストの美的特徴を明示的に参照する。
感情（Affection）は抽象的な感情概念を具体的な視覚表現に接続し、ムード、感情、雰囲気を表現する言語を使用する。
構図（Composition）はシーン内での物体の正確な配置と相互作用に焦点を当てている。

データセット構築の中核となるのがGeneration Chain-of-Thought（GCoT）の統合である。
標準的なキャプションが画像内容を記述するのに対し、GCoTキャプションは画像がどのように、なぜそのように構築されるのかを明確にしている。
この詳細な段階的推論チェーンは、最終画像の意味的・構図的論理を分解し、訓練のための強力な中間監督信号を提供する。

### 2.2 技術的詳細

データ構築パイプラインは複数の段階から成る精密なプロセスである。
まず、高品質な視覚的基盤の合成段階では、FLUX.1-devを合成エンジンとして選択し、その高度な能力を活用して細部まで美しく、一貫した美的価値を持つ画像を生成している。
Vision-Language Model（VLM）を併用してLAION-Aestheticsデータセットからのキャプションを書き換え、生成の広範囲で多様な出発点を提供する高品質な記述を作成している。

しかし、この戦略では想像力とテキストレンダリングの特性で深刻な量的不足が生じるため、補強戦略を実装している。
想像力カテゴリーでは、Gemini-2.5-Proを活用して200の高概念、想像力豊かなシードプロンプトを生成し、その後Qwen3-32Bを使用した創造的拡張技法で、温度パラメータを上げて新しい連想を促進し、大量の創造的キャプションを生成している。

テキストレンダリングデータの不足に対しては、3段階のマイニング-生成-合成パイプラインを開発している。
まず、Qwen2.5-VL-32Bを使用してLAION-2Bデータセットから明確で読みやすいテキストを含む画像を体系的に抽出する。
次に、確認された各テキストリッチ画像に対して、Qwen-VLの記述能力を使用してテキスト内容、視覚的提示、コンテキスト関係を正確に記述する高忠実度の新しいキャプションを生成する。
最後に、これらのテキスト中心のキャプションをFLUX.1-devに入力し、レンダリングされたテキストが精練されたキャプションと直接対応する高品質な画像を生成している。

品質フィルタリングと多次元スコアリングの段階では、Qwen-VLを自動品質保証検査官として使用し、各画像の基本的な明瞭性と構造的一貫性を分析している。
次に、6つの事前定義された特性それぞれについて、1から10までの関連性スコアを割り当てる定量的スコアリングシステムを採用している。
テキストレンダリングカテゴリーについては、特別な活字品質フィルタリング段階を実装し、低コントラスト、歪曲、無意味なテキストを含むインスタンスを除外している。

### 2.3 新規性

この研究の最大の新規性は、従来の記述的キャプションから推論認識注釈フレームワークへの根本的なパラダイム転換にある。
Generation Chain-of-Thought（GCoT）アプローチは、単純に何が画像にあるかを説明するのではなく、複雑な視覚シーンを分解し理解する方法についてモデルに明示的に指導する構造化された推論テンプレートを提供している。

従来の研究とは異なり、この手法はカテゴリ特化の密集キャプションを採用している。
一般的な記述を生成する従来のアプローチとは対照的に、各画像が例示する特定の特性を強調する高度にターゲット化されたキャプションを生成している。
例えば、実体画像を処理する際、Qwen-VLは存在する特定の物体、ランドマーク、人物の正確な識別と詳細な記述を優先するキャプションの生成を指示される。

多次元設計も重要な革新である。
画像は複数のカテゴリに属することができ、例えば「ヴァン・ゴッホの星月夜のスタイルで描かれたエッフェル塔」は実体（ランドマークの正確な描写）とスタイル（アーティストのスタイル模倣）の両方にカテゴライズされる。
この意図的な重複により、モデルは人間のアーティストと同様に、異なるタイプの推論を融合することを学習する。

また、大規模で体系的な多言語対応も新規性の一つである。
単純な翻訳ではなく、テキストレンダリングカテゴリーでは意味の整合性を維持するために重要な内容保存戦略を実装し、画像内でレンダリングされることを意図した特定の英語テキストは、翻訳されたキャプション内でも元の形式で保持されている。

## 3. 実験結果
### 3.1 実験設定

実験は19の先進的な画像生成モデルでPRISM-Benchを使用して実施された。
評価対象には、最先端のクローズドソースモデル（Gemini2.5-Flash-Image、GPT-Image-1）、競争力のあるオープンソースモデル（Qwen-Image、SEEDream 3.0）、FLUX シリーズ、HiDreamシリーズ、Stable Diffusionシリーズ（SDXL、SD2.1、SD3、SD3.5）、Playground、Bagel、JanusProが含まれている。

評価プロトコルは、VLMの高度な認知能力を人間の判断の代理として活用する包括的評価手順を採用している。
GPT-4.1とQwen2.5-VL-72Bを代表的なクローズドソース・オープンソースVLMとして使用し、プロンプト-画像アライメントと画像美学という2つの軸に沿った詳細分析を行っている。

各トラック特化の評価プロンプトにより、一般的な「画像がプロンプトに合致するか？」という問い合わせでは不十分な各カテゴリの特定の課題を捉えている。
想像力では新奇で超現実的概念の合成成功を評価し、実体では特定の現実世界実体の正確なレンダリングを基準とし、テキストレンダリングでは可読性、スペル精度、指定テキストの正確な位置決めに厳格な基準を適用している。
スタイルでは明示的に要求された芸術的・写真的スタイルへの忠実度を評価し、感情では色彩、照明、被写体表現を通じた指定されたムードの効果的な伝達を中心とし、構図では空間配置と相対位置の検証を重視し、長文では複雑な多文GCoTプロンプトからの高密度詳細の組み込み能力を測定している。

### 3.2 主要な結果

全体的な結果は、現在の最先端クローズドソースモデルの優位性を明確に示している。
GPT-Image-1が86.3という最高総合スコアを達成し、Gemini2.5-Flash-Imageが85.3で僅差で続いている。
これらのモデルはほぼ全ての評価トラックで他を上回る性能を示している。

残りのモデルの中では、Qwen-Imageが率いる競争層が形成されている。
トップモデルとの間には依然として顕著な性能差があるものの、これらのモデルはオープンソースコミュニティからの大幅な飛躍を表している。
HiDream-I1-FullとFLUX.1-Krea-devも優秀な結果を達成しており、この分野の急速な進歩を示している。

トラック別の詳細分析では興味深いパターンが明らかになっている。
想像力トラックでは、Gemini2.5-Flash-Imageが88.6という高いスコアで大きなマージンでリードし、GPT-Image-1が86.4で続いている。
これは先進的なクローズドソースモデルがより高度な創造的解釈能力を持つことを示している。

実体トラックでは、GPT-Image-1が88.2の最高スコアで卓越した性能を示し、堅牢な内部知識ベースと高忠実度レンダリング能力を実証している。
Gemini2.5-Flash-ImageとSEEDream 3.0も良好な性能を示している。

テキストレンダリングは、ほぼ全てのT2Iモデルにとって重要な課題として残っている。
ベンチマークでは、このカテゴリが全トラック中最低の総合スコアを記録することが確認された。
特に注目すべきは、BagelやJanusProなどの自己回帰モデルがこのトラックで劣悪な性能を示していることで、テキストレンダリングタスクにおける自己回帰アーキテクチャの本質的限界を浮き彫りにしている。

### 3.3 既存手法との比較

スタイルトラックでは、GPT-Image-1が93.1のスコアで優秀な性能を実証している。
現代的なモデルの多くがこのトラックで比較的良好な性能を示しており、要求されたスタイルへの高い忠実度を示している。
これらのモデルの高いスコアは、テキストレンダリングなどの他のタスクと比較して、文体的本質を捉える能力がより成熟していることを示している。

感情トラックでは、トップモデルが情感や雰囲気の捉え方で並外れた能力を示している。
Gemini2.5-Flash-Imageが92.1という印象的なスコアでリードし、GPT-Image-1とQwen-Imageが僅差で続いている。
興味深いことに、FLUX.1-devがこのカテゴリで最高の美学スコアを達成しており、プロンプトアライメントがやや低いにも関わらず、生成された画像が感情の視覚的伝達で特に効果的であることを示している。

構図トラックでは、GPT-Image-1が92.8の高スコアで大きなマージンでリードし、複雑な空間指示を解析・実行する能力を完全に実証している。
Gemini2.5-Flash-Imageが90.5のスコアで僅差で続いている。
注目すべきは、トップオープンソースモデルがこの領域で高い競争力を示していることで、Qwen-ImageがGemini2.5-Flash-Imageとほぼ同じスコアを記録し、複雑な構図理解のギャップが縮まっていることを示している。

最も挑戦的な長文トラックでは、評価結果がトップモデルを明確に区別している。
Gemini2.5-Flash-Imageが81.1の最高スコアを達成し、GPT-Image-1とSEEDream 3.0も比較的良好な性能を示している。
しかし、他のトラックと比較して、全モデルの総合スコアが著しく低く、プロンプト内の複雑で多層的な指示に従って高品質な画像を生成する能力に大幅な改善の余地があることを示している。

## 4. 実用性評価
### 4.1 実装の容易性

FLUX-Reason-6MデータセットとPRISM-Benchの実装容易性は、研究コミュニティにとって重要な考慮事項である。
データセットの作成には15,000 A100 GPU日という膨大な計算リソースが必要であったが、完成したデータセットの利用は比較的容易である。
既存のT2I訓練パイプラインに統合可能な標準的な画像-キャプションペア形式で提供されているため、研究者は最小限の変更で自分のモデル訓練に組み込むことができる。

PRISM-Benchの評価プロトコルは、GPT-4.1やQwen2.5-VL-72BといったVLMへのアクセスを必要とするが、これらは一般的に利用可能なAPIサービスである。
評価スクリプトの公開により、研究者は一貫した方法で自分のモデルを評価できる環境が整備されている。

多言語対応（英語・中国語）により、グローバルな研究コミュニティでの採用が促進されている。
特にテキストレンダリングタスクでの文化的適応（例：「WHISTLEPIG」を「茅台」に変更）は、地域固有の評価の重要性を示している。

### 4.2 計算効率

FLUX-Reason-6Mデータセットの作成プロセスは計算集約的だが、効率的なパイプライン設計により最適化されている。
128基のA100 GPUを4ヶ月間使用したプロジェクトは、並列処理とバッチ処理の効果的な活用を示している。

初期の800万画像から最終的な600万画像への厳格なフィルタリングプロセスは、品質と計算効率のバランスを取る実用的なアプローチを示している。
VLMを使用した多段階フィルタリング（基本品質チェック、多次元分類、特殊フィルタリング）により、人的労働を大幅に削減しながら高品質を確保している。

PRISM-Benchの評価は、1つのモデルあたり700のプロンプト（各トラック100プロンプト×7トラック）で、現代のGPUインフラストラクチャでは管理可能な計算負荷となっている。
評価の自動化により、大規模な比較研究が実現可能になっている。

### 4.3 応用可能性

FLUX-Reason-6Mデータセットは、単なるT2Iモデルの訓練を超えて幅広い応用可能性を持っている。
6つの特性フレームワーク（想像力、実体、テキストレンダリング、スタイル、感情、構図）は、マルチモーダル理解、画像キャプション生成、視覚的質問応答システムなどの関連タスクにも適用可能である。

Generation Chain-of-Thought（GCoT）アプローチは、他の生成タスクへの転用可能性が高い。
このアプローチは、なぜその出力が生成されたかの説明を求める解釈可能AI研究、教育向けAIシステム、創造的プロセスを理解するための認知科学研究などに応用できる。

PRISM-Benchは、新しい評価パラダイムとして他の生成モデル評価に適応可能である。
VLMベースの評価アプローチは、ビデオ生成、3D生成、音楽生成などの他のモダリティにも拡張可能である。

多言語対応の設計は、グローバルな研究コラボレーションを促進し、地域固有のニーズに対応したモデル開発を可能にしている。
これは特に、文化的コンテンツや地域特有の概念を扱うアプリケーションで重要である。

## 5. まとめと所感
### 5.1 論文の意義

この研究は、Text-to-Image生成分野における重要なマイルストーンを確立している。
最も注目すべき意義は、オープンソースコミュニティに対する膨大なリソースの民主化である。
15,000 A100 GPU日という産業レベルの計算リソースを投入したデータセットを公開することで、従来は大企業の研究室でしか不可能だった大規模研究を、世界中の研究者が実行可能にしている。

Generation Chain-of-Thought（GCoT）の導入は、単なるデータセット作成を超えて、AI研究における新しいパラダイムを提示している。
従来の記述的アプローチから推論認識アプローチへの転換は、より人間らしい創造プロセスの理解と実装への道を開いている。
これは、説明可能AIや教育用AIシステムの発展にも寄与する可能性がある。

6つの特性フレームワークによる体系的なアプローチは、T2I研究に科学的厳密性をもたらしている。
想像力、実体、テキストレンダリング、スタイル、感情、構図という包括的な分類により、モデルの能力と限界を詳細に分析できる基盤が確立された。

PRISM-BenchによるVLMベースの評価手法の確立は、従来の粗い評価指標（CLIPスコア、物体検出器ベースの指標）の限界を克服している。
人間の判断により近い細粒度評価により、モデルの真の性能を測定し、意味のある改善方向を特定できるようになった。

### 5.2 今後の展望

この研究により明らかになった課題と機会は、将来の研究方向を示している。
最も明確な課題はテキストレンダリング能力の改善である。
全モデルでこのタスクのスコアが最低であることから、アーキテクチャレベルでの根本的改善が必要である。
特に自己回帰モデルの限界が明確になったことで、新しいアーキテクチャパラダイムの探索が重要になる。

長文プロンプトに対する理解と実行能力の向上も重要な研究領域である。
GCoTプロンプトによる評価で全モデルのスコアが低いことは、複雑な指示を統合して高品質画像を生成する能力に大きな改善余地があることを示している。
これは、より高度な注意機構、階層的プロンプト理解、段階的生成プロセスなどの技術革新を促進する可能性がある。

オープンソースとクローズドソースモデル間のギャップの解消は、継続的な挑戦である。
この研究により、どの特定領域でギャップが最も顕著かが明確になったため、ターゲット化された改善努力が可能になる。
特にQwen-Imageのような競争力のあるオープンソースモデルの出現は、この格差が技術的に克服可能であることを示している。

多言語・多文化対応の拡張は、グローバルなAI民主化に重要である。
現在の英語・中国語対応から、より多くの言語と文化的コンテンツへの拡張により、AIシステムの包括性と公平性を向上させることができる。

最終的に、この研究は単なる技術的貢献を超えて、AI研究における協力的で開かれたアプローチの重要性を示している。
大規模なリソースの公開により、世界中の研究者が参加できる研究環境を創出し、イノベーションの加速と AI技術の民主化に貢献している。