# DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router

## 基本情報
- **arXiv ID**: 2507.22050v1 (https://arxiv.org/abs/2507.22050)
- **著者**: Minghao Guo, Qingcheng Zeng, Xujiang Zhao, Yanchi Liu, Wenchao Yu, Mengnan Du, Haifeng Chen, Wei Cheng
- **所属**: Rutgers University, Northwestern University, NEC Laboratories America, NJIT
- **投稿日**: 2025年07月30日
- **カテゴリ**: cs.AI, cs.LG

## 簡単に説明すると
DeepSieveは、大規模言語モデル（LLM）を「知識ルーター」として活用する新しいRAG（Retrieval-Augmented Generation）手法です。従来のRAGシステムが単一の統合インデックスから情報を取得するのに対し、DeepSieveは複雑なクエリを構造化されたサブクエリに分解し、それぞれを最適な知識ソース（API、SQL、異なるRAGコーパスなど）に動的にルーティングします。

GitHubリポジトリは https://github.com/MinghoKwok/DeepSieve で公開されています。この手法は「情報のふるい分け（information sieving）」という概念を導入し、クエリ側とソース側の両方で段階的に情報をフィルタリングすることで、異種の知識ソースからより正確で関連性の高い情報を取得できます。特に、プライバシー制約や構造的な非互換性により単一のインデックスに統合できない実世界の知識ソースに対して有効です。

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデル（LLM）は多くの推論タスクで優れた性能を示していますが、最新のドメイン固有の情報へのアクセスが必要な知識集約的なクエリでは困難に直面しています。これは、LLMのパラメータが固定されており、外部知識への動的アクセスができないことに起因します。RAGはこの問題に対する有望な解決策として登場しましたが、既存のRAGシステムには根本的な制限があります。

主な課題は2つあります：
1. **クエリ側のふるい分けの欠如**：ほとんどのシステムはユーザークエリを原子単位として扱い、基礎となる意味構造を分解・分析せずに直接情報を取得する
2. **ソース側のふるい分けの欠如**：知識は異なるソース間（非構造化コーパス、構造化API、プライベートデータベース）で異種性があり、単一ソース内でも複数のドメインやトピックを含む可能性があるが、既存システムはフラットで統一されたインデックスから取得する

### 1.2 主要な貢献
- 実世界の知識ソースの構造的・意味的異種性をRAGの中核的課題として特定し、「情報ふるい分け」を提案
- 初めて「LLM-as-a-knowledge-router」を使用して、クエリを動的に分解し、サブクエリを異種ソースにディスパッチ
- マルチソース設定での有効性だけでなく、標準的な単一ソース環境でも性能向上を実証
- プラグアンドプレイで多様なツール、検索バックエンド、RAGモデルとの統合をサポートするモジュラーで拡張可能な設計

## 2. 提案手法
### 2.1 手法の概要
DeepSieveは、クエリとソース空間の両方で情報ふるい分けを実行する新しいRAG手法です。フレームワークは4つの主要コンポーネントで構成されています：

1. **質問分解（Question Decomposition）**：複雑な入力クエリを構造化されたサブ質問に分解
2. **思考生成（Thought Generation）**：各サブ質問に対して「思考」（検索計画を導く潜在的推論ステップ）を生成
3. **データソースルーティング（Data-source Routing）**：LLMベースのルーターがサブ質問ごとに最適なツール・コーパスペアを選択
4. **再帰的反省（Recursive Reflexion）**：取得した情報が不十分な場合、思考を再評価し、行動計画を修正するか、ソースを再選択

### 2.2 技術的詳細
**アルゴリズムの流れ**：
```
入力: 自然言語クエリQ、ソースセットS={(Tk, Ck)}
出力: 最終回答Â

1. Decompose(Q) → {q1, ..., qn}  // クエリを構造化サブクエリに分解
2. 各qi に対して：
   - Route(qi, S) → si  // 知識ソースsi = (Ti, Ci)を選択
   - Retrieve(qi, si) → ai  // ソースsiからサブクエリqiの回答aiを取得
   - Reflect(qi, ai) → IF_REPLAN  // aiが不十分な場合、再計画を決定
   - IF_REPLANなら：
     - 別のソースを選択して再取得
   - メモリMに(qi, si, ai)を保存
3. Fuse({ai}) → Â  // 全ての有効なサブ回答を最終応答に統合
```

**モジュラーアーキテクチャ**：
- 各コンポーネント（分解、ルーティング、取得、反省、融合）は独立して置換・拡張可能
- 知識ソースは(Tool, Corpus)ペアとして抽象化され、自然言語プロファイルで注釈付け
- 新しいレトリーバー（BM25、FAISS、ColBERTv2など）や新しいソース（SQL、APIなど）の追加は、ラッパーとプロファイルの登録のみで可能

### 2.3 新規性
- クエリとソースを分離したエンティティとして扱い、サブゴールごとのソース選択を可能にする
- LLMを知識ルーターとして使用し、異種ソースへの動的ディスパッチを実現
- 多段階の情報ふるい分けプロセスにより、ノイズの多い取得を削減し、推論の深さを向上
- インデックスのマージやスキーマの統一を必要とせず、プライバシー制約下でも動作可能

## 3. 実験結果
### 3.1 実験設定
- **ベンチマーク**：MuSiQue、2WikiMultiHopQA、HotpotQAの3つのマルチホップQAデータセット（各1,000件の開発用質問）
- **LLMバックボーン**：DeepSeek-V3とGPT-4o
- **ソースの異種性シミュレーション**：各データセットを「ローカル」と「グローバル」セグメントに分割
- **ベースライン**：RAG手法（IRCoT、ColBERTv2、HippoRAG、RAPTOR）および推論・エージェント手法（ReAct、ReWOO、Reflexion、CoT）
- **評価指標**：Exact Match（EM）、F1スコア、生成トークン数

### 3.2 主要な結果
**DeepSeek-V3での性能**：
- 平均F1スコア：58.9（最高性能）
- 平均EMスコア：49.3
- MuSiQue：F1 46.8（+13.4改善）、EM 36.0（+14.6改善）
- 2WikiMultiHopQA：F1 68.4（+5.3改善）、EM 62.8（+16.3改善）
- HotpotQA：F1 61.6、EM 49.0

**効率性**：
- 平均トークン使用量：3,926.6（HotpotQAでの比較）
- Reflexion（37,893トークン）やReAct（9,795トークン）と比較して大幅に効率的
- 高精度を維持しながら計算コストを削減

**アブレーション研究**：
- 反省機能なし：性能が大幅に低下（HotpotQA F1: 61.6→21.6）
- ルーティングなし：HotpotQAではわずかに向上するが、他のデータセットでは低下
- 分解のみでも強力な性能を示し、3つのコンポーネントの組み合わせが最高性能を達成

### 3.3 既存手法との比較
- 純粋なRAGベースライン（HippoRAG、RAPTOR等）を全てのデータセットで上回る
- エージェント型手法（ReAct、Reflexion）と比較して、より少ないトークンで同等以上の性能
- GraphRAGなどの構造認識型検索と比較しても、静的グラフ前処理なしで優れた性能
- 単一ソース環境でも、従来のRAGより優れた検索精度と回答精度を実現

## 4. 実用性評価
### 4.1 実装の容易性
DeepSieveはモジュラー設計により、実装が比較的容易です。各コンポーネントは独立しており、既存のRAGパイプラインに段階的に統合できます。新しい知識ソースの追加は、適切なラッパーとプロファイルを定義するだけで可能です。また、GitHubでソースコードが公開されており、SQL、JSON、異なるRAGコーパスなどの複数のソースタイプのサポートが既に実装されています。

### 4.2 計算効率
DeepSieveは他のLLMベースの推論システムと比較して非常に効率的です。分解とルーティングにより、必要な情報のみを選択的に取得するため、全体的なトークン使用量が削減されます。HotpotQAでの実験では、Reflexionの約10分の1、ReActの約40%のトークンで、より高い精度を達成しています。

### 4.3 応用可能性
DeepSieveは以下の幅広い応用が可能です：
- **企業内ナレッジマネジメント**：異なる部門のデータベース、API、文書を統合的に検索
- **医療・法務分野**：プライバシー制約のある異種データソースからの情報取得
- **研究支援システム**：論文データベース、実験データ、APIを横断的に検索
- **マルチモーダルQA**：テキスト、構造化データ、画像など異種メディアの統合
- **パーソナライズドQA**：ユーザー固有の知識グラフやアクセスパターンに基づく検索

## 5. まとめと所感
### 5.1 論文の意義
DeepSieveは、RAGシステムの根本的な課題である「情報のふるい分け」問題に対する画期的な解決策を提供しています。LLMを知識ルーターとして活用するという発想は斬新で、実世界の異種知識ソースに対応できる実用的なアプローチです。特に、プライバシーや構造的制約により統合できない知識ソースを扱える点は、企業や組織での実装において大きな価値があります。

また、モジュラー設計により、既存のRAGシステムに段階的に導入できる点も評価できます。実験結果が示すように、単一ソース環境でも性能向上が見られることから、DeepSieveの分解・ルーティング・反省のパラダイムは、RAGの基本的な改善として広く採用される可能性があります。

### 5.2 今後の展望
- より表現力豊かな分解戦略の探索（現在のDAG構造を超えた複雑な依存関係の表現）
- パラメータ化されたアクションによる細粒度のツール選択（検索深度、温度、APIモードなど）
- ユーザーやタスク固有の好みに基づくパーソナライズされたルーティング
- 長期的な適応とユーザー中心のQA動作を可能にする個人化メモリモジュール
- クロスフォーマットコーパスでの完全な評価と、より多様なデータタイプへの拡張