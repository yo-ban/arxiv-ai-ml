# MetaCLIP 2: A Worldwide Scaling Recipe

## 基本情報
- **arXiv ID**: 2507.22062v1 (https://arxiv.org/abs/2507.22062)
- **著者**: Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen-tau Yih, Shang-Wen Li, Hu Xu
- **所属**: FAIR (Meta), MIT, Princeton University, New York University
- **投稿日**: 2025年07月30日
- **カテゴリ**: cs.CV, cs.LG

## 簡単に説明すると
MetaCLIP 2は、世界規模のWebデータから収集した多言語の画像-テキストペアを使って、CLIPモデルを最初から訓練する世界初のレシピです。従来のCLIPは英語データのみに依存していましたが、MetaCLIP 2は300以上の言語に対応し、英語と非英語データが互いに性能向上に寄与する「相互利益」を実現しました。

GitHubリポジトリは https://github.com/facebookresearch/MetaCLIP で公開されています。この論文では、これまで英語中心だったCLIPの訓練を世界規模に拡張する際の「多言語の呪い」（multilingual curse）を打破する方法を示しています。具体的には、メタデータ、キュレーションアルゴリズム、訓練フレームワークの3つの要素を慎重に設計・スケーリングすることで、ViT-H/14モデルでImageNetの精度を80.5%から81.3%に向上させ、多言語ベンチマークで新たな最高性能を達成しています。

## 1. 研究概要
### 1.1 背景と動機
CLIPは、ゼロショット画像分類、検索、マルチモーダル大規模言語モデル（MLLM）のビジョンエンコーダとして、現代のビジョンおよびマルチモーダルモデルの重要な構成要素となっています。しかし、既存のCLIPとその大部分の変種は英語のみの設定を採用しており、MetaCLIPも10億規模の英語データセットを抽出するスケーラブルなデータキュレーションアルゴリズムを導入していました。

現在のCLIPの限界として、Webデータの約50.9%を占める非英語データを破棄していることが挙げられます。世界規模のWebデータにCLIPの訓練を拡張するためには、これらの非英語画像-テキストペアを扱う必要がありますが、以下の2つの主要な課題があります：

1. **基本的なデータキュレーション手法の欠如**：非英語データを大規模に扱うための根本的なキュレーション手法が存在しない
2. **多言語の呪い**：多言語CLIPは英語のみのCLIPよりも英語性能が悪化する（例：mSigLIPはImageNetで英語のみのSigLIPより1.5%劣る）

### 1.2 主要な貢献
- 世界規模の画像-テキストペアでCLIPを最初から訓練する初のレシピ「MetaCLIP 2」を提案
- 多言語の呪いは適切なレシピと十分なスケーリングによって克服可能であることを実証
- 英語と非英語データが相互に利益をもたらすことを示す
- 300以上の言語に対応したメタデータとキュレーションアルゴリズムを開発
- 多言語ベンチマークで新たな最高性能を達成（XM3600 64.3%、Babel-ImageNet 50.2%、CVQA 57.4%）
- 機械翻訳や蒸留に頼らない、ネイティブ言語による監督を実現
- 文化的多様性と地理的認識の向上を実現

## 2. 提案手法
### 2.1 手法の概要
MetaCLIP 2のレシピは3つのステップから構成されています：

1. **世界規模メタデータの構築**：英語のみだった50万エントリのメタデータを、300以上の言語に対応した2700万エントリに拡張
2. **世界規模キュレーションアルゴリズムの実装**：言語ごとの部分文字列マッチングとバランシングを実装
3. **世界規模モデルの訓練フレームワークの構築**：訓練中の画像-テキストペア数の増加と、世界規模データから学習するための最小限のモデル容量の研究

### 2.2 技術的詳細
**メタデータの構築**：
- 多言語WordNet（31言語）
- Wikipedia（329言語）のユニグラムとバイグラム
- Wikipediaページタイトル（40のランダムな日付のスナップショット）
- 言語ごとに独立したメタデータを維持（同じ単語でも言語によって意味が異なるため）

**キュレーションアルゴリズム**：
- 言語識別（LID）を使用してalt-textの言語を分類
- 言語固有のメタデータを使用して概念をマッチング
- 言語ごとに異なる閾値 t_lang を設定（英語の閾値 t_en から各言語のtail概念の割合pを計算し、同じ割合を維持するように各言語の閾値を決定）
- Algorithm 1として詳細な擬似コードが提供されている

**訓練フレームワーク**：
- 多言語テキストトークナイザーの採用（XLM-V語彙が最高性能）
- 見たペア数のスケーリング（グローバルバッチサイズを2.3倍に増加）
- 最小限のモデル容量の研究（ViT-H/14が呪いを破る転換点）

### 2.3 新規性
- 外部リソース（プライベートデータ、機械翻訳、蒸留）に依存しない世界規模のCLIP訓練
- 言語ごとに適応的な閾値設定による、head/tail概念のバランスの維持
- 英語と非英語データが相互に性能向上に寄与することの実証
- OpenAI CLIPおよびMetaCLIPとの最大限の重複を維持することで、一般化可能な知見を提供

## 3. 実験結果
### 3.1 実験設定
- インターネットから収集した公開画像-テキストペアを使用
- 言語識別後、約44%のalt-textが英語（MetaCLIPの英語のみデータと同規模）
- ViT-L/14とViT-H/14モデルで実験
- OpenAI CLIPとMetaCLIPの訓練設定をベースに、世界規模対応に必要な変更のみを追加

### 3.2 主要な結果
**英語ベンチマーク**：
- ImageNet精度：80.5% → 81.3%（ViT-H/14、世界規模データ29B pairs）
- SLIP 26タスク平均：72.4% → 74.5%
- DataComp 37タスク平均：66.5% → 69.6%

**多言語ベンチマーク**：
- Babel-ImageNet：50.2%（新記録、+3.8%改善）
- XM3600（画像→テキスト検索）：64.3%（新記録、+1.5%改善）
- CVQA（ローカル）：57.4%（新記録、+7.6%改善）
- Flickr30k-200：53.2%（画像→テキスト）
- XTD-200：51.0%（画像→テキスト）

### 3.3 既存手法との比較
- mSigLIPと比較して、より少ない見たペア数（72%）、より低い解像度（224px vs 256px）で優れた性能を達成
- SigLIP 2は英語性能を優先（訓練データの90%が英語）しているが、MetaCLIP 2は英語と多言語の両方で優れた性能を実現
- 英語のみのデータ（13B pairs）から世界規模データ（13B pairs、同数だが多言語）に変更するだけで、文化的多様性ベンチマークで大幅な改善

## 4. 実用性評価
### 4.1 実装の容易性
MetaCLIP 2は、既存のOpenAI CLIPアーキテクチャとの最大限の重複を維持しているため、実装が容易です。必要な変更は以下の3点のみです：
- 多言語テキストトークナイザーの採用
- グローバルバッチサイズの増加（2.3倍）
- モデルサイズの拡大（ViT-H/14）

完全なメタデータ、キュレーションコード、訓練コードがオープンソースで公開されており、再現性が高いです。

### 4.2 計算効率
訓練には追加の計算リソースが必要ですが、推論時の計算コストは標準的なCLIPモデルと同等です。ViT-H/14モデルは大きいものの、多言語対応による追加の計算オーバーヘッドはありません。また、英語のみのタスクでも性能が向上しているため、単一モデルで多様なユースケースに対応できます。

### 4.3 応用可能性
MetaCLIP 2は以下の幅広い応用が可能です：
- **多言語画像検索**：300以上の言語で画像とテキストの検索が可能
- **文化的に多様な画像認識**：地理的に多様なベンチマークで優れた性能
- **マルチモーダル大規模言語モデル（MLLM）**：より良いビジョンエンコーダとして利用可能
- **画像生成**：DALL-Eや拡散モデルの基盤データとして活用
- **自己教師あり学習**：Web-DINOなどの手法への応用

## 5. まとめと所感
### 5.1 論文の意義
MetaCLIP 2は、CLIPの訓練を英語中心から真にグローバルなスケールに拡張した画期的な研究です。「多言語の呪い」という長年の課題を、適切なスケーリングと慎重な設計によって克服できることを実証しました。特に、英語と非英語データが相互に性能向上に寄与するという発見は、今後のマルチモーダルAI研究に大きな影響を与えるでしょう。

また、機械翻訳や既存モデルからの蒸留に頼らず、ネイティブスピーカーが書いたalt-textから直接学習することで、文化的な多様性と真正性を保持している点も重要です。完全にオープンソースで公開されている点も、研究コミュニティへの貢献として高く評価できます。

### 5.2 今後の展望
- さらなる言語の追加と、低リソース言語への対応の改善
- より効率的なモデルアーキテクチャの探索（ViT-H/14より小さいモデルでの呪いの克服）
- 地域固有の文化的コンテキストをより深く理解するモデルの開発
- 世界規模データセットの継続的な更新と改善メカニズムの確立
- 他のマルチモーダルタスク（ビデオ理解、音声-視覚統合など）への拡張