# X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again

## 基本情報
- **arXiv ID**: 2507.22058v1 (https://arxiv.org/abs/2507.22058)
- **著者**: Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang, Jie Jiang
- **所属**: Tencent Hunyuan X
- **投稿日**: 2025年07月30日
- **カテゴリ**: cs.CV, cs.LG

## 簡単に説明すると
X-Omniは、強化学習を使って離散的な自己回帰モデルによる画像生成の品質を劇的に向上させた統一型マルチモーダルAIモデルです。従来、自己回帰モデルによる画像生成は、累積誤差や情報損失により品質が低く、複雑な指示に従えないという問題がありました。しかし、X-Omniは強化学習（GRPO）を導入することで、これらの問題を解決し、高品質な画像生成と高度な画像理解を単一のモデルで実現しています。

プロジェクトのウェブサイトは https://x-omni-team.github.io で公開されており、特に長いテキストのレンダリング（英語と中国語の両方）において優れた性能を示しています。7Bパラメータの言語モデルをベースに、FLUX.1-devを拡散デコーダとして使用し、SigLIP-VQトークナイザーを採用することで、画像とテキストを統一的に扱うことが可能となっています。

## 1. 研究概要
### 1.1 背景と動機
「次のトークン予測」パラダイムは、言語モデリングにおいてAI革命を引き起こしました。このパラダイムを画像領域に拡張する試みとして、iGPTやDALL-Eなどが画像を離散的なトークンに変換し、逐次的なトークン予測により画像を生成する手法を開発しました。しかし、初期のDALL-Eなどによる画像品質は限定的でした。

この制限は、画像トークンの逐次生成から生じる累積誤差に起因すると考えられており、その結果、多くの研究は拡散モデルへと移行しました。しかし、拡散モデルのアーキテクチャとモデリングの異質性は、画像生成に強力な意味理解能力を統合する上での課題となっています。最近の研究では、画像生成を拡散目的で、言語生成を自己回帰目的で共同訓練する方向に向かっており、これがモデリングのミスマッチをさらに悪化させています。

### 1.2 主要な貢献
- 強化学習が自己回帰手法の限界を大幅に軽減できることを実証
- 慎重に設計された報酬モデルにより、累積誤差を効果的に削減し、拡散デコーダとより整合性の高いトークンを生成
- 統一された自己回帰フレームワークで画像生成と画像理解を統合
- 長いテキストレンダリング能力において最高水準の性能を達成（英語・中国語両方）
- 新しいベンチマーク「LongText-Bench」を提案し、長いテキストレンダリング能力を評価
- 意味的エンコーダ（SigLIP 2）を採用し、画像理解タスクにも自然に対応

## 2. 提案手法
### 2.1 手法の概要
X-Omniは、画像とテキストトークンを統一された自己回帰アーキテクチャ内で統合します。フレームワークは以下の3つの主要コンポーネントで構成されています：

1. **SigLIP-VQトークナイザー**：画像を意味的に豊富な離散トークンに変換
2. **統一自己回帰モデル**：Qwen2.5-7Bをベースに、画像とテキストの両方を処理
3. **拡散デコーダ**：FLUX.1-devを使用して、離散トークンから高品質な画像を生成

### 2.2 技術的詳細
**画像トークン化**：
- 事前学習済みSigLIP2-g ViTを視覚的意味抽出器として使用
- ベクトル量子化器を追加（コードブックサイズ：16,384、次元：2,048）
- Qwen2.5-1.5B LLMと整合させて視覚理解タスクで訓練

**自己回帰モデリング**：
- Qwen2.5-7Bを基本モデルとして採用
- オリジナルのTransformerレイヤーの前後に4つのビジョン専用ブロックを挿入
- 画像トークン用の埋め込み層と分類ヘッドを追加
- 任意の画像解像度に対応（解像度情報をプレフィックスとして追加）

**強化学習（GRPO）**：
- Group Relative Policy Optimization（GRPO）アルゴリズムを採用
- 各プロンプトに対して16個のロールアウトを生成
- 包括的な報酬システム：
  - HPSv2：美的品質と人間の好みの整合性
  - Unified Reward：高解像度画像の品質評価
  - テキスト-画像整合性スコア：Qwen2.5-VL-32Bを使用
  - OCR精度スコア：GOT-OCR2.0とPaddleOCRを使用

### 2.3 新規性
- 強化学習を離散自己回帰画像生成に適用した初の成功例
- classifier-free guidanceへの依存を排除
- 画像生成と理解を真に統一されたアーキテクチャで実現
- 意味的トークン化により、生成された画像から再度特徴抽出する必要がない
- 長いテキストレンダリング能力の実現（既存の統一モデルでは困難だった）

## 3. 実験結果
### 3.1 実験設定
**事前訓練データ**：
- 画像生成：COYO-700M、DataComp-1B、LAION-2Bから200M枚の高品質画像を選定
- 画像理解：LLaVA-OneVision、BLIP3-KALE、Infinity-MMから59Mデータ
- 合計700Bマルチモーダルトークンで訓練

**訓練段階**：
1. 事前訓練：3段階戦略（42B→629B→42Bトークン）
2. 教師ありファインチューニング：1.5Bトークン
3. 強化学習：180Kプロンプト、200ステップ

### 3.2 主要な結果
**テキストレンダリング性能**：
- OneIG-Bench（英語）：0.901（GPT-4oの0.857を上回る）
- OneIG-Bench（中国語）：0.895（最高性能）
- LongText-Bench（英語）：0.900
- LongText-Bench（中国語）：0.814（最高性能）

**テキスト-画像生成性能**：
- DPG-Bench総合：87.65（統一モデルで最高）
- GenEval総合：0.83（競争力のある性能）

**画像理解性能**：
- POPE：89.3
- GQA：62.8
- MMBench：74.8
- DocVQA：88.6（LLaVA-OneVisionの87.5を上回る）

### 3.3 既存手法との比較
- 統一モデル（Janus-Pro、Show-o2、BAGEL等）と比較して、テキストレンダリングで圧倒的な優位性
- 画像生成品質でもFLUX.1-devなどの専用生成モデルに匹敵
- 画像理解タスクでも、理解専用モデル（LLaVA系）と競争力のある性能
- GPT-4oと比較しても、多くのベンチマークで同等以上の性能

## 4. 実用性評価
### 4.1 実装の容易性
X-Omniは既存の確立されたコンポーネント（Qwen2.5、SigLIP、FLUX.1-dev）を活用しているため、実装は比較的容易です。アーキテクチャの変更は最小限で、主に以下の点のみです：
- ビジョン専用ブロックの追加
- 画像トークン用の埋め込み層と分類ヘッド
- 強化学習フレームワークの実装

分散訓練戦略（テンソル、パイプライン、コンテキスト並列化）との完全な互換性も維持されています。

### 4.2 計算効率
推論時は7Bパラメータの自己回帰モデルとして動作するため、計算効率は良好です。強化学習による追加の計算コストは訓練時のみで、推論時にはclassifier-free guidanceが不要なため、むしろ効率的です。また、統一アーキテクチャにより、マルチターンの画像理解・生成タスクにおいて、再度の特徴抽出が不要となり、大幅な効率化を実現しています。

### 4.3 応用可能性
X-Omniは以下の幅広い応用が可能です：
- **複雑な指示に基づく画像生成**：詳細な要求に応じた高品質画像の生成
- **長いテキストを含む画像生成**：ポスター、看板、Webページなどのデザイン
- **多言語対応**：英語と中国語の両方で高品質なテキストレンダリング
- **マルチモーダル対話**：画像理解と生成を統合した対話システム
- **文書理解と生成**：DocVQAで高性能を示し、文書関連タスクへの応用

## 5. まとめと所感
### 5.1 論文の意義
X-Omniは、離散自己回帰モデルによる画像生成の可能性を再び示した重要な研究です。長年の課題であった累積誤差の問題を強化学習によって解決し、拡散モデルに匹敵する品質を達成しました。特に、統一されたアーキテクチャで画像生成と理解の両方を高いレベルで実現したことは、今後のマルチモーダルAI研究に大きな影響を与えるでしょう。

長いテキストレンダリング能力の実現は、実用的な観点からも非常に価値があり、デザインや文書生成などの実世界アプリケーションへの応用が期待されます。また、強化学習の効果的な活用は、他の生成タスクへの応用可能性も示唆しています。

### 5.2 今後の展望
- より多くの言語への対応（現在は英語と中国語のみ）
- 動画生成への拡張（時系列的な一貫性の維持）
- より大規模なモデルサイズでの実験（13B、70Bなど）
- リアルタイムインタラクティブな画像編集への応用
- 強化学習の報酬設計のさらなる改善と自動化
- 3D生成やその他のモダリティへの拡張