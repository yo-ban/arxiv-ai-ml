# GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning

## 基本情報
- **arXiv ID**: 2507.19457v1 (https://arxiv.org/abs/2507.19457)
- **著者**: Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab
- **所属**: UC Berkeley, Stanford University, Notre Dame, Databricks, BespokeLabs.ai, MIT
- **投稿日**: 2025年07月28日
- **カテゴリ**: cs.LG, cs.AI

## 簡単に説明すると
この論文は、複雑なAIシステム（複数のLLM呼び出し、検索、ツール使用などを組み合わせた「複合AIシステム」）を最適化するための新しい手法「GEPA（Genetic-Pareto）」を提案しています。

GEPAの核心は、従来の強化学習（RL）アプローチとは異なり、自然言語によるフィードバックと反省的な学習を活用することです。従来のRLベースの手法（GRPOなど）では、タスクの最後に得られるスカラー報酬のみに依存し、数万回のロールアウトを必要としていました。一方、GEPAは自然言語のトレースから得られる豊富な診断情報を活用し、わずか数百回のロールアウトで大幅な性能向上を実現します。

具体的には、GEPAは進化的アルゴリズムの構造を採用し、各プロンプトを自然言語フィードバックに基づいて反省的に変異させます。さらに、パレート最適性に基づく候補選択戦略により、多様な戦略を維持しながら効率的に探索を行います。実験では、HotpotQA、IFBench、PUPA、HoVerなどのベンチマークで、GEPAがGRPO（24,000ロールアウト）を最大19%上回り、MIPROv2を14%以上上回る性能を示しました。

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデル（LLM）の発展により、複数のLLM呼び出し、検索モジュール、ツール使用などを組み合わせた「複合AIシステム」が広く開発されています。これらのシステムを最適化する主要なアプローチとして、強化学習（RL）手法、特にGroup Relative Policy Optimization（GRPO）が使用されてきました。

しかし、現在のRLベースのアプローチには重大な制限があります。第一に、スカラー報酬のみに依存するため、長いトラジェクトリの最後に得られる限定的な信号から学習する必要があります。第二に、効果的な最適化には数万から数十万のロールアウトが必要で、各ロールアウトに高コストがかかる実世界のアプリケーションでは実用的ではありません。

一方で、LLMベースのAIシステムは、すべてのコンポーネント（検索器、ツールAPI、LLM自体、報酬メカニズム）が自然言語トレースを生成・処理するという特徴があります。これらのトレースには、エラーメッセージ、中間推論、ツール呼び出しの詳細など、豊富な診断情報が含まれています。しかし、従来の手法はこの情報をスカラー報酬に圧縮してしまい、貴重な学習信号を失っていました。

この研究の動機は、自然言語が持つ以下の特性を活用することです：
- 高レベルの抽象化と流動的な抽象度の調整が可能
- 小さな変更で大きな行動変化を引き起こせる表現力
- 複数の「教訓」を容易に組み合わせられる構成性
- システムの全トラジェクトリを可視化し、各ステップをデバッグできる透明性

### 1.2 主要な貢献
この研究の主要な貢献は以下の通りです：

- 自然言語フィードバックを活用した反省的プロンプト進化による、サンプル効率の高い複合AIシステム最適化手法GEPAの提案
- パレート最適性に基づく多目的最適化と遺伝的アルゴリズムの統合により、局所最適解を回避しつつ多様な戦略を探索
- 4つの挑戦的なベンチマーク（マルチホップ推論、指示追従、プライバシー配慮型委譲、検索拡張検証）での評価により、GRPOを最大35倍少ないロールアウトで19%上回る性能を実証
- 命令最適化のみで、従来の命令＋少数ショット共同最適化手法（MIPROv2）を上回ることを示し、最近のLLMの能力向上を活用
- コード最適化タスクにおける推論時探索戦略としての有望な予備結果の提示

## 2. 提案手法
### 2.1 手法の概要
GEPAは、複合AIシステムのプロンプトを反省的に進化させる最適化アルゴリズムです。核心的なアイデアは、自然言語フィードバックから得られる豊富な診断情報を活用し、各ロールアウトから高信号の「教訓」を抽出してプロンプトに組み込むことです。

アルゴリズムは以下の要素から構成されます：

1. **反省的変異（Reflective Mutation）**：システムトレースとフィードバックを分析し、失敗の原因を診断して改善案を生成
2. **遺伝的構造（Genetic Structure）**：プロンプトを進化木として組織化し、各世代が親から学習信号を継承しながら新しい洞察を統合
3. **パレート最適化（Pareto Optimization）**：各訓練例に対する最良候補を維持し、多様な戦略の探索を促進

### 2.2 技術的詳細
#### 問題定義
複合AIシステム P を、相互接続されたモジュール M = {m₁, ..., mₙ} の集合として定義します。各モジュール mᵢ は、重みパラメータ θᵢ とプロンプトパラメータ πᵢ でパラメータ化されます。最適化の目標は、訓練データ D_train 上でスカラー報酬関数 r を最大化することです。

#### GEPAアルゴリズム
GEPAの主要なアルゴリズムは以下のステップで構成されます：

1. **初期化**：ベースプロンプトから開始し、候補プールを初期化
2. **候補選択**：パレートフロントから確率的に候補を選択
3. **変異生成**：選択された候補とランダムな訓練例に基づいて、反省的にプロンプトを変異
4. **評価**：新しい候補を検証セットで評価
5. **更新**：パレート最適候補を更新

#### 反省的変異の詳細
反省的変異は、以下のプロンプトテンプレートを使用してLLMに実行させます：

```
Given the current prompt and the feedback from a rollout:
1. Identify what went wrong in the execution
2. Propose specific improvements to the prompt
3. Generate an updated prompt incorporating these improvements
```

この過程で、LLMは自然言語トレースから具体的な失敗パターンを特定し、それに対応する明示的な指示やヒューリスティクスをプロンプトに追加します。

#### パレートベース候補選択
各訓練例 x に対して最高性能を示す候補 c を追跡し、候補選択時は以下の確率分布に従います：

P(c) ∝ exp(β × score(c))

ここで β は温度パラメータで、探索と活用のバランスを制御します。

### 2.3 新規性
GEPAの主要な新規性は以下の点にあります：

1. **自然言語フィードバックの活用**：スカラー報酬だけでなく、診断的な自然言語フィードバックを学習信号として使用
2. **反省的プロンプト進化**：LLMの自己反省能力を活用し、わずか1回のロールアウトから大きな行動変更を実現
3. **パレート最適化の導入**：複合AIシステム最適化において、各訓練例の最良候補を追跡することで多様性を保持
4. **遺伝的アプローチとの統合**：進化木構造により、複数の例にわたる一般化を促進

## 3. 実験結果
### 3.1 実験設定
評価には以下のベンチマークとモデルを使用しました：

**ベンチマーク**：
- HotpotQA：ウィキペディアベースのマルチホップ質問応答（150訓練/300検証/300テスト）
- IFBench：正確な指示追従能力の評価（150訓練/300検証/294テスト）
- HoVer：オープンドメインのマルチホップ事実検証（150訓練/300検証/300テスト）
- PUPA：プライバシー配慮型委譲タスク（111訓練/111検証/221テスト）

**モデル**：
- Qwen3 8B：オープンソースモデル（温度0.6、top-p 0.95）
- GPT-4.1 mini：商用モデル（温度1.0）

**比較手法**：
- Baseline：最適化なしの基本プログラム
- MIPROv2：最先端のプロンプト最適化手法
- GRPO：24,000ロールアウトでのRL手法（Qwen3のみ）

### 3.2 主要な結果
**観察1：反省的プロンプト進化は高いサンプル効率を示し、重み空間強化学習を上回る**

GEPAは全4ベンチマークで、GRPO（24,000ロールアウト、LoRA使用）を最大19%上回り、最大35倍少ないロールアウトで達成しました。具体的には：

- HotpotQA：6,438ロールアウトで19%向上
- IFBench：678ロールアウト（35倍少ない）で2.73%向上
- HoVer：6,858ロールアウトで13.66%向上
- PUPA：2,157ロールアウト（11倍少ない）で5.19%向上

さらに、GEPAはGRPOの最良検証スコアにわずか402、330、1,179、306ロールアウトで到達し、最大78倍のサンプル効率を示しました。

**観察2：反省的プロンプト進化により、命令最適化のみで命令＋少数ショット共同最適化を上回る**

GEPAはMIPROv2を全設定で一貫して上回り、GPT-4.1 miniで最大11.1%、Qwen3 8Bで最大10.3%の差をつけました。GEPAとGEPA+Mergeは、MIPROv2の2倍以上の改善率を達成しました（+16.02%と+14.29% vs MIPROv2の+7.04%）。

**観察3：候補選択戦略が最適化軌道と最終性能に大きく影響**

パレートベース選択戦略は、常に最良候補を選択する単純な戦略を最大8.17%上回り、全ベンチマークで平均+6.4%の差を維持しました。

### 3.3 既存手法との比較
GEPAと既存手法の主要な違いは以下の通りです：

1. **GRPO との比較**：
   - GRPO：スカラー報酬のみ、24,000ロールアウト必要
   - GEPA：自然言語フィードバック活用、数百〜数千ロールアウトで優れた性能

2. **MIPROv2 との比較**：
   - MIPROv2：命令と少数ショット例の共同最適化
   - GEPA：命令のみの最適化で上回る性能、より解釈可能な改善

3. **一般化ギャップ**：
   - GEPAで生成されたプロンプトは、検証セットとテストセット間の性能差が小さく、より良い一般化を示す

## 4. 実用性評価
### 4.1 実装の容易性
GEPAの実装は比較的シンプルで、以下の要素のみが必要です：

1. 基本的なLLM APIアクセス（変異生成用）
2. システムトレースとフィードバックを収集する仕組み
3. 候補プロンプトの管理とスコア追跡

特別なハードウェアや複雑な分散システムは不要で、単一のGPUで実行可能です。コードは公開予定とのことです。

### 4.2 計算効率
GEPAの主要な利点は、その卓越したサンプル効率です：

- 訓練ロールアウト数：最適性能に到達するのに737（HotpotQA）から79（IFBench）のみ
- 総ロールアウト数：GRPOの1/35から1/2程度
- 検証セット評価が大部分を占めるため、動的サブセット選択でさらに効率化可能

メモリ使用量も、重み更新が不要なため、GRPOのようなRL手法より大幅に少なくなります。

### 4.3 応用可能性
GEPAの応用可能性は非常に広範です：

1. **複合AIシステムの最適化**：検索、ツール使用、複数LLM呼び出しを含む任意のシステム
2. **コストの高いドメイン**：医療、金融など、各ロールアウトが高価な分野
3. **迅速なプロトタイピング**：少数の例から素早く効果的なプロンプトを開発
4. **推論時最適化**：コード最適化など、実行時に動的にプロンプトを改善
5. **解釈可能な改善**：生成されたプロンプトは人間が読んで理解でき、さらに手動で改良可能

## 5. まとめと所感
### 5.1 論文の意義
この研究は、複合AIシステム最適化の分野に重要なパラダイムシフトをもたらしています。主要な意義は以下の通りです：

第一に、自然言語の豊かな表現力と診断能力を活用することで、従来のRLアプローチの限界を突破しました。スカラー報酬という「低帯域幅」の信号から、自然言語という「高帯域幅」の学習媒体への移行は、人間の学習プロセスにより近く、直感的です。

第二に、サンプル効率の劇的な改善により、実世界のアプリケーションでの実用性が大幅に向上しました。数万回のロールアウトから数百回への削減は、計算コストだけでなく、開発サイクルの短縮にも貢献します。

第三に、生成されるプロンプトの解釈可能性と具体性は、AIシステムの透明性向上に寄与します。図示されたプロンプト例では、抽象的な指示ではなく、具体的な戦略とヒューリスティクスが明示的に記述されています。

第四に、最近のLLMの能力向上（自己反省、指示追従、構成的推論）を効果的に活用する手法として、今後のAIシステム開発の方向性を示唆しています。

### 5.2 今後の展望
この研究は多くの興味深い将来の方向性を示唆しています：

1. **スケーラビリティの向上**：より大規模で複雑な複合AIシステムへの適用
2. **他の最適化手法との統合**：GEPAとRLや勾配ベース手法の組み合わせ
3. **自動フィードバック生成**：人間の介入なしに高品質なフィードバックを生成する仕組み
4. **マルチエージェントシステムへの拡張**：複数のエージェントが協調するシステムの最適化
5. **継続的学習**：デプロイ後も実行時のフィードバックから学習し続けるシステム

技術的には、パレート最適化の理論的分析、収束保証の研究、より効率的な候補選択戦略の開発などが重要な研究課題となるでしょう。

実用面では、GEPAのようなアプローチが、AIシステム開発のベストプラクティスとして定着し、開発ツールやフレームワークに統合されることが期待されます。これにより、より多くの開発者が高品質なAIシステムを効率的に構築できるようになるでしょう。