# Delta Activations: A Representation for Finetuned Large Language Models

## 基本情報
- arXiv IDは2509.04442v1です (https://arxiv.org/abs/2509.04442)
- 著者はZhiqiu Xu、Amish Sethi、Mayur Naik、Ser-Nam Limです
- 所属はUniversity of Pennsylvania、University of Central Floridaです
- 投稿日は2025年9月6日です
- カテゴリはcs.AI、cs.LGです

## 簡単に説明すると

この論文では、大規模言語モデル（LLM）のファインチューニング済みモデルをベクトル表現として表現する新しい手法「Delta Activations」を提案しています。現在、多くの研究者や組織がLLaMA、Gemma、QwenなどのベースモデルからファインチューニングしたモデルをHugging Faceなどで公開していますが、これらの膨大なモデル群を効率的に発見・比較・活用することが困難でした。

Delta Activationsは、ファインチューニング済みモデルとベースモデルにシンプルな汎用プロンプトを入力し、両者の内部状態（アクティベーション）の差分を計算することで、そのモデルの「特徴」をベクトル表現として抽出します。この手法により、ドメインや用途が似たモデル同士を自動的にクラスタリングでき、モデル選択やモデルマージングなどの実用的なタスクに活用できることを実証しています。

コードは https://github.com/OscarXZQ/delta_activations で公開されています。

## 1. 研究概要
### 1.1 背景と動機

現在のAI研究コミュニティでは、LLaMA、Gemma、Qwen、DeepSeekなどの強力な事前学習済みLLMを出発点として、特定のタスクやドメインに特化した膨大な数のファインチューニング済みモデルが作成されています。これらのモデルは教師ありファインチューニング（SFT）や選好アライメント技術（RLHF、DPO等）により最適化されており、それぞれ異なる能力や知識を持っています。

しかし、この豊富なモデルエコシステムには重要な課題があります。これらのモデルは同一のベースモデルから派生しているものの、異なる調整目標、ドメイン、データセットを反映して多様な振る舞いを示します。モデル間の類似点や相違点を特定し、特定の能力や知識に基づいてグループ化することは、このエコシステムでモデルを発見・再利用するために必要不可欠ですが、現状では非常に困難です。

この困難さは、モデルリポジトリにおける標準化されたメタデータの不足によってさらに悪化しています。モデルは曖昧な名前付けがされ、ドキュメントが不足しており、ファインチューニング中に使用されたデータセットや目的との関連付けが稀であり、これらは従来手法がモデル特性化に依存する属性です。

### 1.2 主要な貢献

この研究では、ファインチューニング済みLLMの内部アクティベーションの変化を測定することで、モデル自体から独立して導出される表現を提供する新しいモデル埋め込み手法Delta Activationsを導入しました。

主要な貢献は以下の通りです：

- ファインチューニング済みモデルを効果的にクラスタリングし、望ましい性質を示すベクトル埋め込み手法Delta Activationsの提案
- 異なる訓練設定やファインチューニング手法にわたる手法の安定性実証
- Delta Activationsがファインチューニングデータセットが混合された際に加法性を示すという興味深い性質の発見
- 少数ショット学習を通じたタスク埋め込みへの拡張と、モデル選択・マージングへの応用実証
- Delta-Xファミリーとしてアクティベーション以外の表現（Delta LogitsやDelta Meaning）への一般化

## 2. 提案手法
### 2.1 手法の概要

Delta Activationsは、ベースモデルと比較したファインチューニング済み言語モデルの隠れ状態の違いを測定することで、ファインチューニング済み言語モデルをベクトル埋め込みとして表現する手法です。

具体的には、ベースモデル f_base とファインチューニング済みモデル f について、共有入力シーケンスにおける隠れ状態を比較します。モデル f の入力 x に対する最終層の最後のトークンのアクティベーション h^f(x) を用いて、Delta Activationsを以下のように定義します：

Δ_f(x) = h^f(x) - h^base(x)

この Δ_f(x) は、ファインチューニングの結果として、モデルの入力 x に対する内部表現がベースモデルからどれだけ発散しているかを定量化します。

### 2.2 技術的詳細

モデルの埋め込みを構築するため、固定されたプローブデータセット D_probe = {x_1, x_2, ..., x_N} にわたってDelta Activationsを集約します：

v_f = (1/N) Σ Δ_f(x_i)

**プローブデータセット**の設計が重要なポイントです。手法の普遍的適用性の必要性に動機付けられ、プローブデータセットは意図的に完全に汎用的に設計されており、特定のタスクやドメインにバイアスをかけることなく、モデルの核となる計算経路を活性化することを目指しています。

プローブデータセットの第一の要素として、ダミーの指示と入力を含むAlpacaテンプレートを使用します：

「Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
### Instruction: Please provide a response. ### Input: Input.
### Response:」

残りのプローブデータセットはGPT-4oによるパラフレーズで生成され、テンプレートのシンプルさと汎用性を保ちながら言語的多様性を導入しています。実験では N=5 を使用しています。

**Delta-Xファミリー**：Delta Activationsは、より広範なDelta-Xファミリーの一部です。プローブデータセットからベースモデルとファインチューニング済みモデルの両方から一貫して抽出できる任意の特徴ベクトルが、Delta埋め込みの基礎として機能できます。これにより、Delta Logits、Delta Meaning、Delta Weighted Activationsなどの変形が可能になります。

### 2.3 新規性

既存手法と比較したDelta Activationsの主要な新規性は以下の通りです：

**モデル依存表現**：訓練データやアダプター構成に依存する従来手法とは異なり、Delta Activationsはモデル自体から直接導出されます。これにより、同一データでの異なる訓練設定の区別や、プロプライエタリデータセットでの訓練モデルの表現が可能になります。

**計算効率性**：新規モデルに対しては一度の順伝播のみで埋め込みを計算でき、評価ベース手法よりもはるかに少ない計算量で済みます。また、PCAや行列因子分解を使用する手法とは異なり、既存モデルの埋め込みを変更しません。

**汎用性**：標準化された入力プロンプトにより、様々なモデルアーキテクチャやファインチューニング手法に適用可能です。メタデータや外部情報を必要とせず、モデルの内部動作を直接捉えます。

**少数ショットタスク埋め込み**：特定タスクの数例でファインチューニングすることで、タスクのプロキシとしてモデルを機能させ、モデルとタスクの埋め込みを統一できます。

## 3. 実験結果
### 3.1 実験設定

実験では3つの異なる事前学習済みベースモデルから構築された3つのモデルプールを使用しました：LLaMA-3.1-8B、Gemma-2-9B、Qwen-2.5-7B。各プールには15個のファインチューニング済みモデルが含まれ、5つのドメイン（Legal、Math、Medical、Commonsense、Coding）にそれぞれ3つのモデルが配置されています。

各ドメインに1つのデータセットを割り当て、3000例ずつの3つの分離したスプリットを作成して教師ありファインチューニングを実施しました。データセットは、Legal（LegalBench）、Math（GSM-8K）、Medical（PubMedQA）、Commonsense（HellaSwag）、Coding（OPC-SFT）です。

全モデルは学習率1e-4、バッチサイズ4で3エポック、LoRAを使用してファインチューニングしました（rank=8、α=16）。

評価指標として、シルエットスコアを使用しました。これは各モデル i について s(i) = (b(i) - a(i))/max(a(i), b(i)) として定義され、a(i) は同一クラスター内のモデルへの平均距離、b(i) は最近傍クラスターへの平均距離です。

### 3.2 主要な結果

Delta Activationsは全てのベースラインを大幅に上回る性能を示しました。平均シルエットスコア0.614を達成し、これは次点の手法（0.190）を大きく上回りました。

**ベースライン比較**：
- 平坦化されたアダプター重み：-0.043（負のスコア、クラスタリング失敗）
- 顕著マスク（Localize-and-Stitch）：0.190
- 出力文埋め込み：0.087（モデルによって大きく変動）
- Delta Activations：0.614

**個別モデルでの結果**：
- LLaMA-3.1-8B：0.645
- Gemma-2-9B：0.545  
- Qwen-2.5-7B：0.653

t-SNE可視化により、Delta Activationsが他の手法では達成できない明確で分離されたドメインクラスターを形成することが確認されました。

### 3.3 既存手法との比較

**加法性の実証**：Delta Activationsは望ましい加法的性質を示します。2つのドメインのデータで訓練されたモデルのDelta Activationsは、個別に訓練されたモデルのDelta Activationsの和に近似します。全10のドメインペアでこの性質が一貫して確認され、混合データセットでの訓練効果を予測可能にします。

**プローブデータセットの影響**：5つのプロンプトが最適で、プロンプト数を1から20に増やしてもさらなる改善は見られませんでした。適度な長さの汎用指示テンプレートが最も効果的で、ドメイン固有プロンプトやWikitextでは性能が低下しました。

**訓練設定への頑健性**：異なる学習率、データ量、エポック数での訓練にわたってドメイン固有クラスタリングが維持され、Delta Activationsが訓練手順の一般的な変動に対して効果的にファインチューニングドメインを特定できることが確認されました。

**ファインチューニング手法の汎用性**：教師ありファインチューニングだけでなく、選好最適化（DPO）でも高いシルエットスコア（0.93）を達成し、異なる監督信号に対する適応性を実証しました。

## 4. 実用性評価
### 4.1 実装の容易性

Delta Activationsの実装は比較的簡単です。既存のモデルインフラストラクチャ（HuggingFace Transformers等）を使用して、ベースモデルとファインチューニング済みモデルに同一の入力を与え、最終層の隠れ状態を抽出するだけです。

**技術的要件**：
- PyTorchやTransformersライブラリなどの標準的なツール
- ベースモデルとファインチューニング済みモデルへのアクセス（重みレベル）
- 単一の順伝播計算（推論時のみ）

**統合の容易性**：既存のモデル選択やマージングパイプラインに容易に統合できます。プローブデータセットは事前定義されているため、新しいモデルに対してすぐに適用可能です。

ただし、プロプライエタリモデル（GPT-4等）には適用できないという制限があります。内部の隠れ状態へのアクセスが必要なためです。

### 4.2 計算効率

**訓練時コスト**：Delta Activations自体は訓練を必要としません。既存のファインチューニング済みモデルに対して事後的に適用可能です。

**推論時コスト**：新しいモデルあたり5回の順伝播（プローブデータセット分）のみが必要で、評価ベースの手法と比較して大幅に高速です。計算時間はモデルサイズに線形比例し、LLaMA-8Bクラスでは数秒程度で完了します。

**メモリ効率性**：埋め込みの次元数はモデルの隠れ状態サイズ（4096次元等）で固定されており、重みベースの手法（〜10^7次元）やマスクベースの手法（〜10^9次元）と比較してはるかにコンパクトです。

**スケーラビリティ**：新しいモデルの追加時に既存の埋め込みを再計算する必要がないため、大規模なモデルハブに適しています。

### 4.3 応用可能性

**モデル選択**：Big-Bench Hard（BBH）でのLoRAHubでの実証実験では、Delta Activationsを使用した最類似モデル選択により、ランダム選択と比較して2.0%の性能向上（34.3%→36.3%）を達成しました。

**モデルマージング**：類似モデルの識別により、干渉を回避したマージング戦略が可能になります。完全に類似なモデルのマージは性能低下を示しましたが、これはモデル間の関係についてDelta Activationsが情報を提供している証拠です。

**タスク埋め込み**：20例の少数ショット学習でタスク表現を作成し、98%以上の精度で対応するフルモデルクラスターを発見できました。これにより、タスクベースのモデル検索が可能になります。

**クロスアーキテクチャ対応**：Delta Meaningを使用することで、異なるベースモデル（LLaMA-3.1-8B vs LLaMA-3.2-1B等）から派生したモデル間の比較も可能です。

**研究ツール**：ファインチューニングの効果の可視化、モデル動作の理解、大規模モデルエコシステムの分析などの研究用途にも活用できます。

## 5. まとめと所感
### 5.1 論文の意義

この研究は、急速に拡大するファインチューニング済みLLMエコシステムにおいて、モデル発見・比較・再利用を可能にする重要な技術的貢献を提供しています。Delta Activationsは、メタデータや外部情報に依存しない、モデル内在的な表現として革新的です。

**技術的意義**：従来のアプローチの制限（訓練データへの依存、評価の脆弱性、アーキテクチャ固有の制約）を克服し、シンプルでありながら効果的なソリューションを提示しています。加法性や安定性といった理論的に魅力的な性質も実証されています。

**実用的価値**：現実のAI開発において、適切なベースモデルやファインチューニング済みモデルの選択は重要な課題です。Delta Activationsは、この選択プロセスを自動化・最適化する実用的なツールを提供します。

**コミュニティへの影響**：Hugging FaceやGitHubなどのモデル共有プラットフォームが普及する中、Delta Activationsのような組織化技術は、オープンサイエンスとモデル再利用を促進する重要な基盤技術となる可能性があります。

### 5.2 今後の展望

**技術的拡張**：現在の手法は単一のベースアーキテクチャに限定されているため、Delta Meaningなどのアーキテクチャ非依存表現の改善が重要です。また、マルチモーダルモデルや拡散モデルへの拡張も興味深い研究方向です。

**スケールの検証**：現在の実験は比較的小規模なモデルプール（15-20モデル）で行われているため、数百から数千のモデルを含む大規模エコシステムでの性能検証が必要です。

**応用の深化**：モデルマージングにおいては、単純な類似性ベースの選択を超えて、Delta Activations情報を活用したより洗練されたマージング戦略の開発が期待されます。また、継続学習や転移学習における応用も探求価値があります。

**理論的理解**：なぜDelta Activationsがこれほど効果的に機能するのか、どのような情報が埋め込まれているのかについて、より深い理論的分析が求められます。

**エコシステム統合**：実際のモデルハブやMLOpsパイプラインへの統合を通じて、実世界でのユーザビリティと効果を検証することが重要です。

この研究は、AI研究のパラダイムが事前学習からファインチューニング中心に移行する中で、モデルエコシステム管理の基礎技術として重要な位置を占める可能性を持っています。