# VisualActBench: Can VLMs See and Act like a Human?

## 基本情報
- arXiv ID は 2512.09907v1 である（https://arxiv.org/abs/2512.09907 ）
- 著者は Daoan Zhang、Pai Liu、Xiaofei Zhou、Yuan Ge、Guangchen Lan らである
- 所属機関はロチェスター大学、パデュー大学、ノースイースタン大学である
- 投稿日は2024年12月13日である
- カテゴリは cs.CV、cs.AI である

## 簡単に説明すると
この論文は、Vision-Language Model（VLM）が人間のように視覚情報だけを基に状況を理解し、適切な行動を推論できるかを評価する新しいベンチマークを提案しています。
従来のVLM評価は主に視覚理解や言語との対話に焦点を当てていましたが、現実世界で必要とされる能動的な行動推論能力については十分に評価されていませんでした。

本研究では「Visual Action Reasoning」という新しいタスクを定義し、「VisualActBench」という包括的ベンチマークを構築しました。
このベンチマークには1,074本の動画と3,733の人間による行動アノテーションが含まれ、4つの現実世界シナリオ（動的ナビゲーション、家庭サービス、安全監視、人間-機械相互作用）をカバーしています。
各行動にはAction Prioritization Level（APL）と能動的・反応的タイプのラベルが付けられ、人間の価値観との整合性を評価できます。

29のVLMを評価した結果、GPT-4oなどの最先端モデルでも人間レベルの推論には大きなギャップがあることが判明しました。
特に能動的で高優先度の行動生成において顕著な限界が明らかになっています。

## 1. 研究概要
### 1.1 背景と動機
Vision-Language Model（VLM）の出現により、オープンワールドの知覚と理解能力が大幅に向上しています。
しかし、AGIの実現に向けては、VLMは反応的な言語媒介エージェントから視覚中心の能動的なエンティティへのパラダイムシフトが必要です。

従来のVLMは中間的な言語表現に依存して視覚入力を解釈し応答してきましたが、これからは言語的仲介なしに視覚ストリームを通じて環境の動的変化を自律的に感知し、文脈認識行動を開始する能力が求められています。
この視覚中心の処理は、人間が主に情報を処理する方法により密接に一致しています。

例えば、人間は言語的推論を必要とせず、純粋に視覚知覚と事前知識に依存して歩行中の障害物を回避したり、飛んでくるボールをキャッチしたりできます。
この視覚中心処理により迅速な意思決定と瞬間的応答が可能になり、ロボティクス、自動運転車、監視システムなどのアプリケーションにとって極めて重要です。

もう一つの重要な側面は、人間の意思決定における行動価値システムの役割です。
純粋に反応的な応答とは異なり、人間の行動は内在化された価値観、倫理的考慮、文脈固有の目的に導かれ、即座の感覚入力を超えて選択に影響を与え結果を優先順位付けします。

### 1.2 主要な貢献
この研究では、VLMの視覚中心能力と人間の価値観との整合性を評価するため、以下の主要な貢献を提供しています。

- 新しい視覚中心タスクの提案：テキストプロンプトなしで視覚入力から直接文脈認識的で能動的な行動を生成する「Visual Action Reasoning」タスクを提案し、動的環境における人間の意思決定を模倣することに挑戦
- 人間に整合したベンチマークの構築：4つの現実世界シナリオをカバーし3,700以上の注釈付き行動を含む「VisualActBench」を導入し、各行動にAction Prioritization LevelとProactive/Reactiveタイプのラベルを付けて価値整合性と行動品質を評価
- 包括的モデル評価の実施：30のVLMをベンチマークし、能動性、価値整合性、抽象的推論における重大なギャップを明らかにし、現在のモデルの限界に関する重要な洞察を提供して実世界展開に向けた将来の開発を指導

## 2. 提案手法
### 2.1 手法の概要
VisualActBenchは、VLMが視覚情報のみに基づいて人間のような行動推論を行う能力を評価する新しいベンチマークです。
このベンチマークは、明示的なテキスト指示やプロンプトに依存せず、視覚入力にのみ埋め込まれた情報からモデルが適切な応答を生成できるかを評価します。

4つの代表的な現実世界シナリオを定義しています：
1. Dynamic Navigation（動的ナビゲーション）：交通環境や移動場面での行動判断
2. Home Service（家庭サービス）：家庭環境でのサービス提供や家事支援
3. Safety and Monitoring（安全監視）：安全管理や監視状況での対応
4. Human-Machine Interaction（人間-機械相互作用）：人間とシステムの相互作用場面

各シナリオから1,074本の動画をサンプリングし、合計3,733の行動を手動で注釈付けしました。
各行動にはAction Prioritization Level（APL）が付けられ、文脈的に適切な応答に基づく人間整合的な優先度レベルを反映します。

### 2.2 技術的詳細
ベンチマークの構築において、各行動は「能動的」または「反応的」タイプに分類されています。
これにより、モデルの主体的行動傾向を評価できます。
能動的行動とは、現在の状況を分析して将来の結果を予測し、先制的に行動を起こすものです。
反応的行動は、既に発生した事象や明白な刺激に対する直接的な応答です。

Action Prioritization Level（APL）は、人間の価値判断システムに基づいて行動の重要度を評価する指標です。
これにより、モデルが生成する行動が人間の優先順位付けとどの程度一致するかを定量的に測定できます。

評価は、行動生成の正確性だけでなく、モデルが示す「人間整合的な能動性」の程度も測定します。
これは従来の動画理解タスクとは根本的に異なり、モデルの知覚能力と認知能力の両方を包括的に評価する設計となっています。

### 2.3 新規性
VisualActBenchの新規性は、純粋に視覚中心のアプローチにあります。
従来のベンチマークがテキスト質問やコマンドでモデルをプロンプトするのに対し、この設定では自律的にシーンを分析し、文脈的に望ましい結果を予測し、それに応じて行動することが求められます。

人間の行動価値システムとの整合性評価も重要な新規性です。
従来のVLM評価は主に視覚理解と記述に焦点を当てていましたが、本研究では倫理的考慮や社会的含意を含む価値ベース意思決定の評価を導入しています。

マルチモーダル評価アプローチにより、視覚知覚とLLM駆動推論能力の両方を包括的に評価できる点も特徴的です。
これにより、VLMの総合能力をより実用的な観点から測定可能になっています。

## 3. 実験結果
### 3.1 実験設定
29のVLMを評価対象とし、24のオープンソースモデルと5の商用モデルを含みます。
評価はVisualActBenchの4つのシナリオ全体にわたって実施されました。

各モデルは、動画を入力として受け取り、明示的なテキストプロンプトなしに適切な行動を生成することが求められます。
生成された行動は、人間のアノテーションと比較してAPLスコア、能動性・反応性分類、価値整合性の観点から評価されます。

評価指標には以下が含まれます：
- 行動精度：生成された行動の適切性
- APL整合性：人間の優先順位付けとの一致度
- 能動性比率：能動的行動の生成頻度
- 価値整合性：人間の価値判断との整合度

### 3.2 主要な結果
評価結果では、最高性能のVLMでもこのタスクにおいて人間のパフォーマンスとの間に実質的なギャップがあることが明らかになりました。
GPT-4oなどのフロンティアモデルが相対的に強い性能を示しましたが、人間レベルの推論、特に能動的で高優先度の行動生成において大きな差が残っています。

シナリオ別分析では、モデルはDynamic NavigationとSafety and Monitoringで最も良い性能を示しました。
これらの領域では視覚的手がかりが明示的で構造化されている場合が多いためです（例：障害物、道路レイアウト、火災）。

対照的に、Home ServiceとHuman-Machine Interactionドメインでは全モデルにわたって低いスコアを記録しました。
これらのシナリオは通常、抽象的な意図の理解、ユーザーニーズの予測、長期的結果の考慮を要求し、現在のVLMには大きく欠けている能力です。

### 3.3 既存手法との比較
従来のベンチマークとの比較において、VisualActBenchは複数の次元で差別化されています。
既存の動画ベンチマークの多くはキャプション生成や質問応答に焦点を当てているのに対し、本ベンチマークは行動推論生成と優先度整合性を重視しています。

Winoground、MME、MMBenchなどの画像データセットや、MSRVTT、MSVD-QA、TGIF-QAなどの動画データセットと比較して、VisualActBenchは以下の特徴を持ちます：
- 視覚中心（Visual-Centric）アプローチの採用
- 行動推論生成（Action Reasoning Generation）の評価
- 人間整合性（Human Alignment）の測定

これらの特徴により、より実世界に即した包括的な評価が可能になっています。

## 4. 実用性評価
### 4.1 実装の容易性
VisualActBenchは標準的な動画入力と自然言語出力を使用するため、既存のVLMに容易に適用可能です。
評価プロトコルは明確に定義されており、研究者が一貫した方法でモデルを評価できます。

ベンチマークデータは構造化されて提供され、APLスコアと能動性・反応性ラベルが事前に付与されているため、自動評価パイプラインの構築が容易です。
オープンソース化により、研究コミュニティでの広範囲な採用と検証が期待されます。

4つのシナリオへの分類により、特定のアプリケーション領域におけるモデルの強みと弱みを詳細に分析できます。

### 4.2 計算効率
ベンチマークの実行には標準的なVLM推論以外の追加的な計算要求はありません。
1,074本の動画という適度なサイズにより、包括的評価を可能にしながら計算コストを管理しています。

バッチ処理による効率的な評価が可能で、大規模なモデル比較研究にも適用できます。
評価指標の計算は軽量であり、リアルタイムフィードバックが可能です。

### 4.3 応用可能性
VisualActBenchの応用範囲は広く、ロボティクス、自動運転、監視システム、スマートホーム技術など多様な分野での活用が期待されます。
特に、人間との安全な相互作用が要求される環境でのAIシステム評価に重要な役割を果たします。

教育分野では、VLMの実世界適用能力を評価する標準的ツールとして活用可能です。
産業界では、商用VLMの実用性評価や品質保証プロセスに組み込むことができます。

規制当局にとっては、AI安全性とアライメントの評価基準として重要な参考資料となります。
研究開発においては、新しいVLMアーキテクチャの設計指針を提供します。

## 5. まとめと所感
### 5.1 論文の意義
この論文は、VLM研究における重要なパラダイムシフトを提示しています。
従来の言語媒介評価から視覚中心評価への移行は、AI分野の発展において極めて重要な意義を持ちます。

人間の価値観との整合性評価という観点の導入は、AI安全性とアライメント研究の発展に大きく貢献しています。
これは単なる技術的評価を超えて、社会実装における倫理的・安全性の考慮を評価フレームワークに組み込んでいます。

包括的ベンチマークの構築により、VLM研究コミュニティに標準的評価ツールを提供し、今後の研究の方向性を明確に示しています。
29モデルの詳細評価により、現在のVLM技術の限界と改善すべき領域が明確になりました。

### 5.2 今後の展望
今後の研究方向として、より大規模で多様なシナリオへの拡張が期待されます。
文化的・地域的差異を考慮した価値観の多様性への対応も重要な課題です。

マルチモーダル入力（音声、テキスト、センサーデータ等）を統合した包括的評価への発展も興味深い方向性です。
リアルタイム評価システムの開発により、動的環境での継続的評価が可能になるでしょう。

長期的には、このベンチマークがAGI開発における重要な評価基準の一つとなり、人間レベルの視覚理解と行動推論を持つAIシステムの実現に貢献することが期待されます。
産業界と学術界の協働により、より実用的で安全なAIシステムの開発が加速されるでしょう。