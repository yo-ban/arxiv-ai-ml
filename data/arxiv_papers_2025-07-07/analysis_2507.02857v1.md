# AnyI2V: Animating Any Conditional Image with Motion Control

## 基本情報
- arXiv ID: 2507.02857v1 (https://arxiv.org/abs/2507.02857)
- 著者: Ziye Li, Hao Luo, Xincheng Shuai, Henghui Ding
- 所属: Fudan University, DAMO Academy (Alibaba group), Hupan Lab
- 投稿日: 2025年07月06日
- カテゴリ: cs.CV

## 簡単に説明すると
この論文は、任意の条件付き画像を入力として受け取り、動画を生成する「AnyI2V」を提案しています。
入力可能な画像にはメッシュ、点群、エッジ、深度、骨格などが含まれます。
ユーザーが定義した軌跡に従って動画を生成する学習不要なフレームワークです。

従来のText-to-Video（T2V）やImage-to-Video（I2V）手法が抱えていた空間的制約と動的モーション信号の統合という課題を解決し、より柔軟で多様な動画生成を可能にしています。

プロジェクトページ：https://henghuiding.com/AnyI2V/

## 1. 研究概要
### 1.1 背景と動機
動画生成技術、特に拡散モデルにおいて、Text-to-Video（T2V）およびImage-to-Video（I2V）合成は大きな進歩を遂げてきました。しかし、既存の手法には重要な制限がありました。

T2V手法はテキストプロンプトに依存しており、生成されるコンテンツの空間的レイアウトを正確に制御することが本質的に困難でした。一方、I2V手法は実際のRGB画像に依存しているため、合成されるコンテンツの編集可能性が制限されていました。

ControlNetを組み込んで画像ベースの条件付けを導入する手法も存在しますが、これらは明示的なモーション制御を欠いており、計算コストの高い学習が必要でした。さらに、ベースモデルが変更されるたびに再学習が必要となり、柔軟性に欠けていました。

これらの制限を克服するため、新しいフレームワークの開発が求められていました。
学習不要でありながら、任意のモダリティの画像を初期フレームとして受け入れます。
後続フレームでユーザー定義の軌跡に従った動きを実現します。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです。

- 初期フレームの空間的条件とユーザー定義軌跡を統合する「AnyI2V」フレームワークの提案
- 学習不要なアプローチにより、計算コストを削減し、異なるバックボーンへの適応を簡素化
- メッシュ、点群、エッジ、深度、骨格など、幅広いモダリティの条件付き画像を入力として受け入れる柔軟性の実現
- 混合条件入力のサポートと、LoRAやテキストプロンプトによる編集機能の提供
- 特徴注入、フレーム間アラインメント、セマンティックマスク生成という3つの主要技術の開発

## 2. 提案手法
### 2.1 手法の概要
AnyI2Vは、動画拡散モデルに基づいた学習不要のフレームワークです。システムは3つの主要コンポーネントから構成されています。

第一に、特徴注入により、初期フレームのガイダンスを学習不要な方法で実現します。
第二に、フレーム間アラインメントにより、時間的に一貫性のある動画合成を保証します。
第三に、セマンティックマスク生成により、不規則な形状のオブジェクトを正確に制御します。

手法の全体的なパイプラインは、条件付き画像に対してDDIM逆変換することから始まります。3D U-Netから時間モジュールを除去し、特定のタイムステップで空間ブロックから特徴を抽出します。その後、自動生成されたセマンティックマスクによって特定の領域に制約された最適化を行います。

### 2.2 技術的詳細
特徴注入の再考では、拡散モデルが構造情報を捉える能力を活用しています。
研究チームは、ResBlockと自己注意層からの様々な特徴が生成結果にどのように寄与するかを詳細に分析しました。

特に重要な発見として、残差隠れ状態、クエリ、自己注意マップのそれぞれが独立して満足のいく構造制御を提供することが明らかになりました。しかし、残差隠れ状態はソース画像からの外観情報も多く含んでいるため、Adaptive Instance Normalization（AdaIN）を用いて外観情報を除去する手法を提案しています。

ゼロショット軌跡制御では、PCA次元削減を用いて特徴の時間的特性を分析しました。その結果、空間自己注意のクエリが強い時間的一貫性とエンティティ認識セマンティック表現を維持していることが判明しました。この洞察に基づいて、異なるフレーム間でクエリを整合させることで、時間的に一貫性のある動画合成を実現しています。

セマンティックマスク生成では、特徴に埋め込まれたセマンティック情報を活用して動的マスクをクラスタリングします。これにより、オブジェクトの変形に適応的に対応しながら、正確な空間制御を維持することが可能になります。

### 2.3 新規性
本研究の新規性は、学習不要でありながら高品質な動画生成を実現する点にあります。

従来の手法は、ControlNetやモーション制御モジュールの学習に計算コストがかかりました。
ベースモデルが変更されるたびに再学習が必要でした。
AnyI2Vは、これらの制約を排除し、任意のバックボーンへ即座に適応できる柔軟性を提供します。

さらに、ControlNetがサポートしていないメッシュや点群などのデータタイプを含む、より広範なモダリティを条件画像として受け入れることができます。混合条件入力のサポートにより、深度マップで背景構造を効果的に表現しながら、スケッチで前景の詳細を正確に定義するなど、異なるモダリティの補完的な強みを活用できます。

PCA分析による時間的特徴の理解と、それに基づくクエリアラインメント手法も独創的です。また、動的セマンティックマスク生成により、静的マスクの制限を克服し、より自然で柔軟なオブジェクト制御を実現しています。

## 3. 実験結果
### 3.1 実験設定
実験はAnimateDiffをベースに実装され、単一のNvidia A800 GPUで実行されました。DDIM逆変換は1000ステップで構成され、特徴はt_α = 201で抽出されます。

評価には、ウェブから収集したデータとVIPSegデータセットを使用し、Co-Trackerを用いて動画軌跡をアノテーションしました。評価指標として、Fréchet Inception Distance（FID）、Fréchet Video Distance（FVD）、およびObjMCを使用しています。ObjMCは、グラウンドトゥルース軌跡と生成結果間の誤差を定量化し、モーション精度の詳細な評価を提供します。

公平な比較のため、最初の入力フレームを様々な構造表現へランダムに変換しました。
変換先にはcanny、HED、深度、法線、セグメンテーションマップなどが含まれます。
AnyI2Vはこれらの表現を直接利用しますが、他の手法は最初にControlNetで処理します。

### 3.2 主要な結果
定量的評価では、AnyI2Vがベースラインモデルを上回り、既存手法と競争力のある結果を達成しました。

具体的には、FID: 104.53、FVD: 569.89、ObjMC: 16.39という結果を達成しました。
学習不要な手法の中で優れた性能を示しました。これらの数値は学習ベースの手法と比較しても競争力があります。
DragAnythingはFID: 95.83、FVD: 556.09、ObjMC: 13.60です。
MOFA-VideoはFID: 95.63、FVD: 599.48、ObjMC: 17.72です。

定性的評価では、AnyI2Vが多様な条件付き画像を処理できることが示されました。RGB画像に限定される従来の手法と異なり、メッシュ、点群、エッジマップなど、様々なモダリティを入力として受け入れることができます。さらに、混合モダリティ入力も効果的に処理でき、これは以前の手法では追加の学習が必要でした。

### 3.3 既存手法との比較
アブレーションスタディでは、提案した各コンポーネントの重要性が確認されました。

Key & Value一貫性を除去した場合、FIDが108.18に悪化しました。
PCA削減しない場合、FIDは105.95、ObjMCは17.14となりました。静的マスクを使用した場合、FIDは105.44でしたが、動的セマンティックマスクの方がより良い結果（FID: 104.53）を示しました。

最適化ターゲットの分析では、残差隠れ状態を最適化した場合、性能が低下しました。
FID: 129.40、FVD: 647.52、ObjMC: 36.23という結果になりました。
クエリを最適化ターゲットとする選択の妥当性が証明されました。

PCA削減次元の影響についても詳細に分析されました。
M=64が最適な設定として示されました。
これらの結果は、提案手法の各要素が性能向上において重要な役割を果たしていることを示しています。

## 4. 実用性評価
### 4.1 実装の容易性
AnyI2Vの実装は比較的シンプルです。学習不要なアプローチにより、既存の動画拡散モデルのバックボーンに容易に適応できます。

システムは単一のGPUで動作し、逆変換段階に約8秒、生成段階に約35秒を要します。
これは、大規模な学習を必要とする従来の手法と比較して、効率的です。

パラメータ設定も直感的で、パッチウィンドウサイズ（p=4）、PCA削減次元（M=64）、学習率（0.01）など、主要なハイパーパラメータは固定値で良好な結果を示します。25ステップのDDIMサンプリングを使用し、5ステップごとに潜在変数を最適化するという設定も、多くのケースで適切に機能します。

### 4.2 計算効率
AnyI2Vの最大の利点の1つは、学習不要であることによる計算効率の高さです。

従来の手法では、ControlNetやモーション制御モジュールの学習に数日から数週間かかることがありましたが、AnyI2Vは即座に使用可能です。推論時の計算コストも、半精度モードで逆変換に8秒、生成に35秒と実用的な範囲内です。

メモリ使用量も単一のA800 GPU（80GB）で十分です。
より小さなGPUでも、バッチサイズや解像度を調整することで動作可能です。
また、異なるバックボーンへの移植も容易で、再学習なしで新しいモデルへ適応できます。
長期的な計算コストの削減にも貢献します。

### 4.3 応用可能性
AnyI2Vの応用範囲は非常に広いです。

コンテンツ制作分野では、アーティストがスケッチやコンセプトアートから直接アニメーションを生成できます。建築ビジュアライゼーションでは、3Dメッシュや深度マップから動的なウォークスルー動画を作成できます。

科学的可視化では、点群データや分子構造から教育的なアニメーションを生成できます。ゲーム開発では、キャラクターの骨格データから自然な動きを生成したり、環境アセットをアニメーション化したりできます。

混合条件入力のサポートにより、より高度な制作ワークフローも可能になります。例えば、背景を深度マップで、キャラクターをスケッチで指定し、それぞれに異なる動きを与えることができます。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、動画生成分野において重要な進歩を示しています。学習不要でありながら高品質な結果を達成し、従来の手法の計算コストとflexibilityの制限を克服しました。

特に重要なのは、任意のモダリティを入力として受け入れる柔軟性です。これにより、様々な分野のクリエイターが既存のワークフローに動画生成を統合できるようになります。
RGB画像に限定されていた従来の手法と比較して、応用の幅が広がりました。

また、PCA分析による時間的特徴の理解は、動画生成における重要な洞察を提供しています。クエリが時間的一貫性とエンティティ認識を維持することの発見は、今後の研究にも影響を与える可能性があります。

### 5.2 今後の展望
論文で述べられている制限事項として、非常に大きな動きの範囲での正確な制御と、曖昧なオクルージョンの処理に課題があります。また、特徴注入が早期のデノイジングステップでのみ行われるため、最初のフレームの制御精度がControlNetほど高くない場合があります。

今後の研究方向として、モーション一貫性の改善、複雑なオクルージョンの処理、より良い適応性のための軽量ファインチューニングの統合などが考えられます。

また、より長い動画の生成や、複数のオブジェクトの独立した制御、インタラクティブな編集機能の追加なども興味深い拡張となるでしょう。学習不要アプローチの利点を活かしながら、これらの機能を実現することが次の課題となります。