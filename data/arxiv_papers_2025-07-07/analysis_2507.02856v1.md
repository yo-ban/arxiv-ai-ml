# Answer Matching Outperforms Multiple Choice for Language Model Evaluation

## 基本情報
- arXiv ID: 2507.02856v1 (https://arxiv.org/abs/2507.02856)
- 著者: Nikhil Chandak, Shashwat Goel, Ameya Prabhu他2名
- 所属: Max Planck Institute for Intelligent Systems他3機関
- 投稿日: 2025年07月06日
- カテゴリ: cs.CL, cs.LG

## 簡単に説明すると
この論文は、大規模言語モデル（LLM）の評価方法について革新的な提案をしています。従来の多肢選択式（MCQ）評価には「差別的ショートカット」という根本的な問題があることを実証し、その代替として「答え合わせ（Answer Matching）」という手法の優位性を示しています。

具体的には、モデルに質問だけを与えて自由形式で回答を生成させ、その回答が正解と一致するかを別のLLMで判定する方法です。この手法は、人間の評価と高い精度で一致し、MCQ評価よりもはるかに優れた性能を示します。

関連リソースは以下の通りです。
- GitHub: https://github.com/nikhilchandak/answer-matching
- HuggingFace: https://huggingface.co/collections/nikhilchandak/answer-matching-6866a99934c2a9e625cde219

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデルの能力を正確に評価することは、AIシステムの開発と改善において極めて重要です。しかし、現在主流となっている多肢選択式の評価には深刻な問題があります。

本研究の動機は、MCQ評価が言語モデルの本来の「生成」能力ではなく、選択肢を「識別」する能力を測定してしまうという根本的な欠陥にあります。研究者たちは、多くの多肢選択問題が「質問を見なくても」選択肢だけから正解を推測できることを発見しました。これは評価の妥当性を著しく損なう重大な問題です。

さらに、LLMの評価エコシステムが急速に拡大する中で、人間による評価は時間とコストの面で現実的ではありません。そのため、自動化可能でありながら、モデルの真の生成能力を正確に測定できる評価方法の確立が急務となっています。

### 1.2 主要な貢献
本研究の主要な貢献は以下の通りです。

- MCQベンチマークにおける「差別的ショートカット」の存在を実証的に証明
- 「答え合わせ（Answer Matching）」手法の有効性を大規模な実験により検証
- MMLU-ProとGPQA-Diamondデータセットに対する人間による詳細なアノテーションを実施し、公開
- 答え合わせ手法が人間の評価と高い精度で一致することを実証（Scott's π ≈ 0.97）
- 評価手法の違いがモデルランキングに与える影響を分析
- 答え合わせのコストがMCQ評価と同等以下であることを示す

## 2. 提案手法
### 2.1 手法の概要
答え合わせ（Answer Matching）は、以下の3段階で構成される評価手法です。

第1段階では、評価対象のモデルに質問のみを提示し、選択肢を与えずに自由形式で回答を生成させます。
これにより、モデルは真の生成能力を発揮します。
第2段階では、生成された回答と正解を別の判定用LLMに入力します。
第3段階で、判定用LLMが回答の意味的・機能的等価性を評価し、正誤を判定します。

この手法の核心は、従来のLLM-as-a-Judge手法とは異なり、判定モデルに「正解」を提供することです。これにより、判定タスクが「正しさの検証」から「等価性の判定」に変わり、より信頼性の高い評価が可能になります。

### 2.2 技術的詳細
本研究では、差別的ショートカットの問題を形式的に定義しています。質問Qに対する正解集合を$\mathcal{A}_Q$、モデルの回答をRとすると、生成的評価は$R \in \mathcal{A}_Q$を判定する問題として定式化されます。

MCQ評価では、入力が$(Q, a, \mathcal{D})$（Qは質問、aは正解、$\mathcal{D}$は誤答選択肢）に変更されます。
タスクが「正解の生成」から「正解と誤答の識別」に変質します。
実験では、Qwen3-4Bモデルを使用して質問を見せずに選択肢のみから正解を予測する分類器を訓練しました。
多くのベンチマークで高い精度を達成しました。

答え合わせの実装では、様々なサイズのLLMを判定モデルとして使用し、Scott's πという評価指標を用いて人間の判定との一致度を測定しました。

### 2.3 新規性
本研究の新規性は、答え合わせという手法自体ではなく、最近のLLMの能力向上により、この手法が実用的なレベルに達したことを実証した点にあります。

従来、参照回答を用いた評価は提案されていましたが、判定モデルの能力不足により実用的ではありませんでした。本研究は、Qwen3-4B（40億パラメータ）のような比較的小規模なモデルでも、人間レベルの判定精度を達成できることを初めて示しました。

さらに、MCQ評価の問題を「差別的ショートカット」として体系的に分析しました。
質問を提示せずに達成できる精度を検証しました。
その結果、TruthfulQA v2：83%、GoldenSwag：93%、MMMU-Pro：51%という高い精度に達しました。
これは評価手法の根本的な見直しの必要性を強く示唆しています。

## 3. 実験結果
### 3.1 実験設定
実験は3つの主要なデータセットで実施されました。MATHデータセットでは、数式の自動検証が可能なため、グラウンドトゥルースとして使用されました。MMLU-ProとGPQA-Diamondについては、800件のモデル応答に対して人間によるアノテーションを実施しました。

評価対象として、4つの異なる開発者によるフロンティアモデルを使用しました。
様々なサイズの判定モデル（Qwen3シリーズ、Llama、DeepSeek等）を用いて答え合わせを実行しました。人間のアノテーターは、ウェブ検索や計算機などのツールを使用し、平均1分以上をかけて各回答を慎重に評価しました。

### 3.2 主要な結果
MATHデータセットにおいて、17億パラメータのQwen3モデルを使用した答え合わせは、Scott's π = 0.97という高い人間との一致度を達成しました。
一方、MCQ評価はπ = 0.26という低い一致度にとどまりました。

MMLU-ProとGPQA-Diamondでの人間アノテーションとの比較では、答え合わせ手法が一貫して高い一致度を示しました。特に注目すべきは、小規模なQwen3-4Bモデルでも人間レベルの精度を達成し、より大規模なDeepSeekやLlamaモデルは人間同士の一致度の範囲内に収まったことです。

LLM-as-a-Judge手法（参照回答なし）は、フロンティアモデル（DeepSeek V3、OpenAI o4-mini）を使用しても、答え合わせより劣る結果となりました。
エラー分析により、これらのモデルは80%以上の確率で誤った回答を正解と判定する傾向（偽陽性）を示すことが判明しました。

### 3.3 既存手法との比較
MCQ評価の各種バリエーションとの比較も実施されました。
Multiple Choice Verification（各選択肢を個別に検証）は、標準的なMCQより改善が見られました（π = 0.43）。
しかし、答え合わせ（π = 0.97）には遠く及びませんでした。

Multiple Choice Cloze（完了尤度による評価）は最も低い性能を示し、π = 0.07という結果は、この手法がグラウンドトゥルースとほぼ独立であることを示しています。これは、現代のモデルが思考の連鎖を生成してから回答する特性と相性が悪いためと考えられます。

コスト面でも、答え合わせはMCQ評価と同等以下であることが実証されました。
これは、モデルが選択肢なしで質問に答える際、より短い回答を生成する傾向があるためです。

## 4. 実用性評価
### 4.1 実装の容易性
答え合わせ手法の実装は非常に簡単です。
既存のMCQベンチマークから質問部分を抽出し、正解選択肢を参照回答として使用するだけで基本的な実装が可能です。

判定モデルとしては、オープンソースの小規模モデル（Qwen3-4B等）でも十分な性能を発揮します。
そのため、計算リソースが限られた環境においても実装可能です。
また、判定処理は温度0で実行することにより再現性を確保できます。

ただし、既存のMCQ問題の多くは、選択肢に依存して問題の具体性を定義しているため、フィルタリングが必要です。
本研究では、人間のアノテーターが「単一の明確な答えがある」と判断した問題のみを使用しました。
データセットサイズは半分以下に削減されました。

### 4.2 計算効率
計算コストの分析により、答え合わせはMCQ評価より安価になりうることが判明しました。
17モデルの評価における総コストを比較した結果、DeepSeek v3を判定モデルとして使用しても、MCQ評価と同等のコストでした。

さらに興味深いことに、Llama-4-Scoutのような高性能かつ低コストなモデルを使用した場合、答え合わせのコストはMCQ評価より低くなりました。
これは、モデルが選択肢なしで回答する際、より簡潔な応答を生成するためです。
MCQでは、モデルは通常、まず自由形式で問題を解きます。
その後選択肢と照合するため、より長い応答となります。

将来的に推論時計算がスケールするにつれて、参照回答との照合は問題を最初から解くよりも少ない計算で済むため、答え合わせの追加コストはさらに小さくなると予想されます。

### 4.3 応用可能性
答え合わせ手法は、単一の明確な答えがある問題に最適です。これには、事実的知識、数学、科学、プログラミングなど、多くの重要な領域が含まれます。

ただし、翻訳、要約、創造的な文章作成など、複数の正解が存在する生成タスクには適していません。これらのタスクには、ルーブリックベースのLLM判定や、実行による検証などの他の手法がより適しています。

今後のベンチマーク設計では、答え合わせを前提とした問題作成が推奨されます。SimpleQAやBrowserCompなどの最新ベンチマークは、すでにこの方向性を採用しており、「単一で議論の余地のない短い答え」を持つ問題の作成を明示的に指示しています。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、言語モデル評価の分野において画期的な転換点を示しています。
MCQ評価が100年以上にわたって教育評価で使用されてきた歴史を持つ中で、その根本的な限界を実証的に示しました。
実用的な代替手法を提示したことは大きな意義があります。

特に重要なのは、答え合わせという手法自体は新しくないものの、最近のLLMの能力向上により「質的な変化」が起きたことを実証した点です。これは、AI評価手法も、評価対象のAIシステムの進化に合わせて進化する必要があることを示唆しています。

また、モデルランキングが評価手法によって大きく変わることを示したことも重要です。これは、現在のベンチマークに基づくモデル選択が、実際の使用場面での性能を正確に反映していない可能性を示唆しています。

### 5.2 今後の展望
本研究の成果は、今後のベンチマーク設計に大きな影響を与えるでしょう。すでにいくつかの最新ベンチマーク（Humanity's Last Exam、BrowserComp）では答え合わせが採用されており、この傾向は加速すると予想されます。

ただし、いくつかの課題も残されています。
第一に、答え合わせ手法の敵対的攻撃への耐性については、さらなる研究が必要です。
第二に、複数の正解が存在する問題への対応方法の確立も重要です。
第三に、判定モデル自体の継続的な改善と、その信頼性の監視も必要です。

長期的には、評価手法の進化がモデル開発にも影響を与えることが期待されます。真の生成能力が正確に評価されるようになることで、より実用的で信頼性の高いAIシステムの開発が促進されるでしょう。