# Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization

## 基本情報
- **arXiv ID**: 2512.10955v1 (https://arxiv.org/abs/2512.10955)
- **著者**: Tsai-Shien Chen, Aliaksandr Siarohin, Guocheng Gordon Qian, Kuan-Chieh Jackson Wang, Egor Nemchinov, Moayed Haji-Ali, Riza Alp Guler, Willi Menapace, Ivan Skorokhodov, Anil Kag, Jun-Yan Zhu, Sergey Tulyakov 
- **所属**: Snap Inc., UC Merced, CMU
- **投稿日**: 2025年12月14日
- **カテゴリ**: cs.CV, cs.LG

## 簡単に説明すると
この論文は、画像の特定の属性（アイデンティティ、表情、照明、スタイルなど）を抽出して別の文脈に転送する視覚的概念パーソナライゼーションのための初のオープンボキャブラリ属性エンコーダを提案している。従来手法では画像全体の情報が混在した表現を使用するため、不要な属性が混入する「コピー・アンド・ペースト」問題が発生していた。この問題を解決するため、著者らは属性特化型の表現学習を行い、高品質な属性の抽出と合成を可能にした。プロジェクトページ（https://snap-research.github.io/omni-attribute）では実装やデモが公開されている。

## 1. 研究概要
### 1.1 背景と動機
視覚的概念パーソナライゼーションは、参照画像から特定の属性のみを新しい文脈に転送することを目的とするが、既存手法には根本的な限界が存在する。現在の手法では、CLIP、DINOv2、VAEなどの汎用画像エンコーダを用いて画像全体の包括的表現を抽出し、それを用いて画像生成を誘導している。

しかし、これらのエンコーダは全ての視覚情報を単一の表現に圧縮するため、複数の視覚属性が複雑に絡み合った状態になってしまう。その結果、特定の属性（例：アイデンティティ）を転送しようとしても、関係のない属性（例：照明や衣服）まで一緒に転送されてしまう情報漏洩問題が発生する。これにより、望ましくない「コピー・アンド・ペースト」アーティファクトが生成され、一貫性のない合成結果となってしまうのが現状の課題である。

画像は本質的に複数の視覚属性が画素を共有して混在した「視覚的な単語の袋」であり、属性の分離と操作は特に困難な問題である。ライプニッツの言葉「現実は独立した、しかし相互に接続された事物のモザイクである」に着想を得て、著者らは属性レベルでの表現学習という新たな視点からこの問題にアプローチする。

### 1.2 主要な貢献
この研究では、エンコーダ側で属性レベルの表現学習を直接行うという新しいアプローチを提案している。具体的には、画像とテキストによる属性記述を同時に処理し、属性特化型の表現を学習するOmni-Attributeという初のオープンボキャブラリ属性エンコーダを開発した。

- 画像とテキスト属性記述を同時処理する初のオープンボキャブラリ属性エンコーダの提案
- 属性特化型表現学習のためのデータとモデルの共同設計：正負属性アノテーション戦略と双目的最適化の導入
- 属性指向画像検索、パーソナライゼーション、合成における汎用性の実証と埋め込み空間の可視化による解釈性の向上

提案手法の核心は、(1) 属性関連情報のみを忠実に抽出すること、(2) 他の無関係な視覚情報を抑制することの2つの目標を同時に達成することにある。これにより、従来の包括的な画像表現とは異なり、指定された属性に特化した高品質な表現の学習が可能になる。

## 2. 提案手法
### 2.1 手法の概要
Omni-Attributeは、データとモデルの両面から属性レベル表現学習にアプローチする。データ側では、意味的に関連する画像ペアを用いた新しいアノテーション戦略を導入している。各画像ペアには、両画像で共有される意味を記述する「正属性」と、異なる特徴を強調する「負属性」という2種類の属性アノテーションが付与される。

この正負属性の構造により、エンコーダに対してどの視覚概念を保持し、どれを抑制すべきかを明示的に教示することができる。モデル側では、属性レベル表現学習を双目的最適化問題として定式化している。一方では属性埋め込みが対象属性の高忠実度再構成のための十分な情報を捉える必要があり、他方では無関係な属性からの情報を排除する必要がある。

### 2.2 技術的詳細
アーキテクチャは属性エンコーダEとイメージデコーダDから構成される。エンコーダには、テキストと画像の同時処理能力と強力な視覚言語事前知識を提供するMLLM（Qwen-VL）をバックボーンとして採用している。学習済み表現の保持と破滅的忘却の軽減のため、全パラメータ微調整ではなくLoRA調整を使用する。

双目的最適化では、生成損失Lgenと対比損失Lconを最小化する：
- 生成損失：参照画像Irから抽出された属性埋め込みとテキストプロンプトcgを条件として、対の画像Igを再構成
- 対比損失：画像ペア(Ix, Iy)から抽出された正属性埋め込み間の類似性を最大化し、負属性との類似性を最小化

最終的な訓練目標は両損失の重み付け和として定義される：L = λgen · Lgen + λcon · Lcon

### 2.3 新規性
従来手法との最大の違いは、属性特化型の表現学習にある。既存のBreak-A-Scene、ConceptExpress、Token-Verse、Mod-Adapterなどは、空間マスクやDiTsのモジュレーション空間操作により概念分離を試みるが、空間的に分離可能な要素に限定されるか、単純なアフィン変換のみに制約されていた。

OADisやDeCLIPといったテキスト誘導対比目標を用いる手法も存在するが、固定された閉集合の属性に制限されている。対照的に、Omni-Attributeはオープンボキャブラリの属性埋め込みを忠実に抽出可能であり、より精密で柔軟な画像生成を実現する。

また、正負属性による明示的な教示と、72Bパラメータの大規模MLLMを用いた高品質アノテーション戦略、さらに32Bパラメータのエキスパートアノテータモデルによるコスト効率化も技術的な新規性として挙げられる。

## 3. 実験結果
### 3.1 実験設定
評価には複数のダウンストリームタスクを使用している。オープンボキャブラリ属性パーソナライゼーションでは、具体的オブジェクトと抽象概念の2カテゴリで15の参照属性を含むベンチマークを構築した。各属性に対して5画像を選定し、LLMにより参照属性と意味的競合を避ける5プロンプトを生成し、クロスペアリングにより属性当たり25サンプル、計350サンプルの評価セットを作成した。

ベースライン比較では、CLIP、DINOv2、Qwen-VLなどの異なる画像エンコーダを同一の凍結画像生成バックボーンとIP-Adapterモジュールで接続し、公平な比較を実現した。また、OmniGen2、FLUX-Kontext、Qwen-Image-Editなどの最新画像編集モデルも評価対象に含めた。

評価指標には、属性忠実度スコア（パーソナライズされた属性の忠実性）、テキスト忠実度スコア（生成画像と入力プロンプト間の意味一貫性）、画像自然さスコア（全体的な視覚一貫性）の3つを用いた。DreamBench++に従い、GPT-4oによる0-10スケールでの評価を実施し、結果を[0,1]に正規化した。

### 3.2 主要な結果
定性的結果では、CLIPとDINOv2は参照属性入力への対応不足により抽象概念のパーソナライゼーションで苦戦を示した。Qwen-VLは追加属性入力を受け付けるが、アーキテクチャの洗練不足と属性レベル対比学習の欠如により、属性パーソナライゼーションへの適応が不十分であった。

画像編集モデル群は参照画像により近い出力を生成するが、対象属性の分離に失敗し、視認可能な「コピー・アンド・ペースト」アーティファクトとテキストアライメントの弱さを示した。対照的に、Omni-Attributeは画像自然さとテキスト・属性条件両方のアライメント間で最適なバランスを達成した。

定量的評価では、MLLMと人間評価の両方でこれらの観察が一致することが確認された。10名の参加者による10.5K個の個別評価を含むユーザスタディでも、Omni-Attributeの優位性が実証された。また、t-SNE可視化により、同一画像群から抽出された埋め込みが属性に応じて異なる意味のあるクラスタリングを示すことが確認された。

### 3.3 既存手法との比較
属性指向画像検索タスクでは、GPT-4oとCLIPを組み合わせたテキスト誘導検索ベースラインと比較を行った。CelebAから17.7k画像をサンプリングし、「衣服」、「表情」、「髪型」の3属性で各画像の埋め込みを計算した。単一クエリ画像に対して各属性下で最も類似する画像をコサイン類似度により検索した。

結果として、Omni-Attributeはベースラインと比較して、きめ細かい属性特化型の視覚詳細をより正確に捉え、対象意味属性との強いアライメントを示す画像を検索することが実証された。これは、学習された表現が実際に属性特化型の情報を効果的にエンコードしていることを示している。

## 4. 実用性評価
### 4.1 実装の容易性
実装はQwen-VLのようなMLLMバックボーンとIP-Adapterモジュールの組み合わせに基づいているため、既存のインフラストラクチャを活用できる。LoRA調整の使用により、計算資源要件も比較的管理しやすい。ただし、高品質な正負属性アノテーションの作成には72B/32BパラメータのMLLMが必要であり、初期データ準備段階では相当な計算コストが発生する。

実用的な観点から、二段階のアノテーションパイプライン（高品質サブデータセットの作成とエキスパートアノテータの微調整）により、アノテーションコストを入力トークン長で3.1倍、サンプルあたりレイテンシで6.3倍削減することに成功している。これにより大規模データセットでの実用性が向上している。

### 4.2 計算効率
学習時はMLLMバックボーンのLoRA調整と軽量コネクタの追加学習のみを行うため、全パラメータ微調整と比較して計算効率が良い。推論時は単一の属性エンコーダと標準的なIP-Adapterベースの画像生成のみが必要であり、既存の画像生成パイプラインと同等の効率性を持つ。

ただし、複数属性の合成時には条件付きフローフィールドの線形結合計算が必要になるため、単一属性の場合と比較してわずかな計算オーバーヘッドが発生する。とはいえ、これは従来の試行錯誤的なアプローチと比較すると大幅に効率的である。

### 4.3 応用可能性
提案手法の応用範囲は広く、属性指向パーソナライゼーション、合成的画像生成、属性ベース検索など多様なタスクに対応可能である。オープンボキャブラリの特性により、事前定義された属性セットに制限されず、新しい属性や概念にも柔軟に対応できる。

創造的コンテンツ制作、教育的コンテンツ開発、ファッション・デザイン業界での属性操作、エンターテイメント産業での視覚的概念合成など、実社会での応用可能性は高い。また、属性の組み合わせによる新しい視覚概念の創出も可能であり、クリエイティブな用途での価値も期待される。

## 5. まとめと所感
### 5.1 論文の意義
この研究は、視覚表現学習における根本的な課題である属性の絡み合いに対して、エンコーダ側からの直接的なアプローチという新しい解決策を提示した点で非常に意義深い。従来の生成側での後処理的な対応ではなく、表現学習段階から属性の分離を実現することで、より根本的で効果的な解決策を提供している。

特に、正負属性による明示的な教示戦略と双目的最適化の組み合わせは、属性分離と生成品質のバランスを巧妙に取った優れた設計であり、今後の関連研究に大きな影響を与えると考えられる。オープンボキャブラリ対応により実用性も高く、産業応用での価値も高い。

技術的には、MLLMの視覚言語理解能力を活用した属性特化型エンコーダという発想が革新的であり、従来の汎用エンコーダの限界を明確に示している。また、大規模な定量的・定性的評価により、提案手法の有効性を多角的に実証している点も評価できる。

### 5.2 今後の展望
論文で指摘された制限事項への対処が今後の重要な課題となる。特に、相関の高い属性（アイデンティティと髪型など）の分離問題は、根本的な概念的課題を含んでいる。ある属性が他の属性に本質的に含まれるのかという哲学的な問題も含む複雑な課題である。

対比学習のハイパーパラメータに対する感度も実用化の観点から重要な改善ポイントである。自動的なハイパーパラメータ最適化や、より堅牢な学習アルゴリズムの開発が期待される。

将来的には、動画への拡張、3D表現への適用、さらに細かい属性分離の実現などが期待される。また、属性間の階層的関係の明示的なモデリングや、因果関係を考慮した属性分離手法の開発も興味深い研究方向となるだろう。制御可能で解釈可能な視覚表現学習の重要なステップとして、この分野の発展に大きく貢献する研究である。
