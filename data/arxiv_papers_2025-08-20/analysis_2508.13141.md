# OptimalThinkingBench: Evaluating Over and Underthinking in LLMs

## 基本情報
- **arXiv ID**: 2508.13141v1 (https://arxiv.org/abs/2508.13141)
- **著者**: Pranjal Aggarwal, Seungone Kim, Jack Lanchantin, Sean Welleck, Jason Weston, Ilia Kulikov, Swarnadeep Saha
- **所属**: FAIR at Meta, Carnegie Mellon University
- **投稿日**: 2025年8月19日
- **カテゴリ**: cs.AI, cs.CL

## 簡単に説明すると

この論文では、大規模言語モデル（LLM）における「過度な思考」と「不十分な思考」を同時に評価する統一ベンチマーク「OptimalThinkingBench」を提案しています。思考型LLMは複雑なタスクで優れた性能を示しますが、簡単なクエリで不必要に大量の思考トークンを生成し、コストと遅延を増加させます。一方、非思考型LLMは高速で安価ですが、困難な推論問題で性能が劣ります。本ベンチマークは、OverthinkingBench（72ドメインの簡単なクエリ）とUnderthinkingBench（11の困難な推論タスク）の2つのサブベンチマークから構成され、思考調整済み精度メトリクスを用いて33の異なるモデルを評価しました。結果として、どのモデルも最適な思考バランスを達成できておらず、o3が最高スコア72.7%、オープンウェイトモデルではGPT-OSS-120Bが62.5%を記録しました。GitHub: https://github.com/facebookresearch/RAM/tree/main/projects/otb

## 1. 研究概要
### 1.1 背景と動機

大規模言語モデルは、事実的質問への回答からコード作成、困難な数学問題の解決まで、多様なタスクでユーザーに利用されています。長い間、LLMは「不十分な思考」問題に悩まされていました。これは、流暢なテキスト生成や簡単なクエリへの回答は可能だが、段階的思考を必要とする困難な推論問題では性能が劣るという問題でした。この状況は過去1年間で劇的に改善され、「思考型」モデルの新たなクラスが複雑なタスクで顕著な性能を示すようになりました。

しかし、思考の増加は数学やコードなどのドメインを一般的に改善する一方で、簡単なクエリに対する利益は限定的であり、時には性能劣化を引き起こすこともあります。利益の減少を超えて、簡単なタスクでの「過度な思考」現象は大幅な遅延をもたらし、APIベースモデルの総コストを増加させ、ユーザー体験に影響を与えます。

現在、多くの最先端LLMには思考型と非思考型の別々のバリアント（例：GPT-4oとo3）が存在し、特定のクエリに適切なモデルを選択する負担がユーザーに委ねられています。これは理想的ではありません。多くのユーザーは各クエリに対してそのような最適な選択を行う技術的知識を欠いており、選択を誤ると精度や効率性を犠牲にすることになります。

研究の動機は、簡単なクエリを効率的に回答（過度に思考しない）しながら、複雑なクエリにより多くの時間を費やす（不十分に思考しない）モデルの開発が望ましいという認識にあります。コストとパフォーマンスのバランスを取る「最適思考モデル」の開発を促進するため、著者らは「OptimalThinkingBench」という新しいベンチマークを導入しました。

### 1.2 主要な貢献

本研究の主要な貢献は以下の通りです。

- **OptimalThinkingBenchの開発**: 性能と効率性の両方について、最適思考LLMの進歩を同時に追跡する単一の統一ベンチマークを開発しました。これは、OverthinkingBenchとUnderthinkingBenchという2つの補完的なサブベンチマークから構成されています。

- **包括的評価の実施**: 33の異なる思考型および非思考型LLMの包括的評価を通じて、最先端モデルが精度と効率性を最適にバランスするのに苦労しており、将来の研究に大きな改善余地があることを示しました。

- **最適思考促進手法の探索**: 最適思考を促進するいくつかの手法を探索し比較しました。結果として、一部のアプローチは有望である一方、効率的で高性能なLLMの間には依然として大きなトレードオフが存在することを明らかにしました。

## 2. 提案手法
### 2.1 手法の概要

OptimalThinkingBenchは、LLMの思考行動の全スペクトラムを評価するように設計された2つの補完的ベンチマークから構成されています。OverthinkingBenchは簡単なクエリでの過度な計算を測定し、UnderthinkingBenchは複雑なタスクでの不十分な推論を定量化します。これらを組み合わせることで、モデルが精度を維持しながら計算コストをタスクの複雑さに適応的に調整できるかを評価する統一フレームワークを提供します。

**OverthinkingBench**は、思考型モデルでの過度な思考、すなわち簡単なクエリで対応する性能向上なしに過度な思考トークンを生成する現象を定量化する原理で設計されています。多様で高品質なベンチマークを構築するため、制約付きデータセット生成とその後の厳密なデータセットフィルタリングの2段階パイプラインを採用しています。

制約付きデータセット生成では、実世界のクエリ分布に沿った幅広いクエリセットをカバーするベンチマークの作成が必要です。制約のペア C = {D, T}（Dは特定のドメイン、Tは回答タイプ）が与えられた時、LLM Lに対して指定された制約を満たすn個の質問-回答ペアの生成を促します。

データセットフィルタリングでは、合成的に生成されたベンチマークには回答の正確性検証と、質問の明瞭性と適切な難易度の確保が必要です。各生成された質問に対して、別のLLMから8個の応答をサンプリングし、すべてのサンプリングされた回答が元の回答と一致する場合のみ質問-回答ペアを保持します。

**UnderthinkingBench**は、どれほど大きな非思考モデルであっても、複雑な推論タスクでの性能は、はるかに小さな思考モデルよりも低くなるという核心原理に基づいて構築されています。Reasoning Gymの100の異なる推論タスクから開始し、小さな思考モデルと大きな非思考モデルの性能を評価し、性能差が閾値を超えるタスクのみを保持します。

### 2.2 技術的詳細

**OverthinkingBenchの評価メトリクス**では、まず正確性について、データセットフィルタリングで使用したのと同じLLM-as-a-Judgeを用いてモデル回答の正確性を判定します。次に、この正確性基準を使用して、Overthinking-Adjusted Accuracy（OAA_t）を提案します。これは、t未満の思考トークンを使用する際のモデルの精度を追跡する統一メトリクスです。

```
OAA_t = (1/n) Σ[i=1 to n] (Correctness_i × I(ThinkTokens_i < t))
```

しかし、閾値tの選択は課題となります。小さな閾値では多くの思考モデルが0点を取り、大きな閾値では過度な思考を罰しません。そこで、集約メトリクスとして、OAA_t曲線下の面積を報告します。

```
AUC_OAA = ∫[0 to t_max] (OAA_t / t_max) dt ≈ Σ[t=0 to t_max] (OAA_t / t_max)
```

このメトリクスには以下の重要な特性があります：(1) 最大値と最小値が精度と比較可能で、解釈しやすく進歩測定が容易、(2) モデルは最小限のトークン（理想的には0）を使用しながら正しく回答することで高スコアを達成、(3) 思考しないが不正確な回答を生成する場合と、正しい回答を生成するが大量に思考する場合の両方の失敗ケースで低スコアを獲得、(4) 積分形式にもかかわらず、各応答のトークン数が固定されているため計算が容易。

**UnderthinkingBenchの評価**では、モデルの複雑な推論タスクへの正しい回答生成能力を思考トークンを制約せずにテストします。Reasoning Gymが提供するタスク固有のプログラム的検証器を使用し、各サンプルについて出力の最後の\boxed{}からモデルの最終回答を抽出し、タスクの検証器に渡してコード実行を通じて正確性をチェックします。

**OptimalThinkingBenchの統一メトリクス**では、過度な思考と不十分な思考が同じ問題の両面を表すため、単一の統一メトリクスを通じて進歩を追跡することが目標です。OverthinkingBenchからのAUC_OAAとUnderthinkingBenchからの精度Acc_utを単一のF1スコアに結合します：

```
F1^otb = 2 × (AUC_OAA × Acc_ut) / (AUC_OAA + Acc_ut)
```

### 2.3 新規性

本研究の新規性は以下の観点で評価できます。

**統一評価フレームワークの確立**: 従来の研究では、思考型モデルの能力や効率性を個別に評価することが多く、両者を統合的に評価するフレームワークは存在しませんでした。本研究は初めて過度な思考と不十分な思考を同時に評価する統一ベンチマークを提案し、モデルの総合的な思考能力を定量化しました。

**思考調整済み精度メトリクスの開発**: 従来の精度メトリクスでは計算コストが考慮されていませんでした。Overthinking-Adjusted Accuracy（OAA）とその曲線下面積（AUC_OAA）は、精度と効率性を統合した新しい評価指標として、思考型モデルの実用的価値をより正確に反映します。

**合成データセット生成手法の革新**: 制約付きデータセット生成と厳密なフィルタリングプロセスにより、多様性と品質を両立したベンチマーク構築手法を確立しました。この手法により、ベンチマークの汚染防止と難易度調整が可能になり、モデル能力の向上に追従できる動的ベンチマークを実現しました。

**大規模比較評価の実施**: 33の異なるモデルを用いた包括的評価により、現在のLLMの思考能力の限界を体系的に明らかにしました。特に、最高性能モデルでも72.7%のスコアに留まるという結果は、この分野での大きな改善余地を定量的に示しています。

**最適思考促進手法の体系的検証**: 長さベース報酬シェーピング、モデルマージング、ルーティング、明示的プロンプティングなど、複数の最適思考促進手法を体系的に評価し、それぞれの有効性と限界を明らかにしました。これらの結果は、今後の研究方向を示す重要な指針となります。

**実世界適用への示唆**: ベンチマークの設計により、実際のユーザークエリ分布に近い評価が可能になり、LLMの実用的価値をより正確に評価できるようになりました。これは、産業応用における最適なモデル選択に直接的な示唆を与えます。

## 3. 実験結果
### 3.1 実験設定

**OptimalThinkingBench作成**では、質問生成、フィルタリング、評価にLlama-4-Maverickを使用し、異なるプロンプトを適用しました。OverthinkingBenchでは72の異なるドメイン、4つの異なる回答タイプを使用し、各（ドメイン、回答タイプ）ペアに対して5つの質問を生成しました。フィルタリングでは各質問に対して8つの応答をサンプリングし、temperature = 0.6、top_p = 1.0を使用しました。評価では最大思考トークン数t_max = 1000を設定しました。UnderthinkingBench作成では閾値λ = 0.3を設定し、思考モデルとしてQwen3-1.7B、非思考モデルとしてQwen3-235B-A22Bを使用しました。

**モデル評価**では、様々なモデルサイズと異なるファミリーの33の異なるオープンソースおよびプロプライエタリモデルをOptimalThinkingBenchで評価しました。ハイブリッドモデルについては、思考モードと非思考モードの両方で評価しました。全ての評価は8つのシードで実行し、一貫したtemperature sampling 0.6を使用しました。

### 3.2 主要な結果

**モデルは精度と効率性の最適バランス達成に失敗**: 主要なF1^otbメトリクスの比較では、o3がOptimalThinkingBenchで最高性能72.7%を達成しました。オープンウェイトモデルでは、GPT-OSS-120Bが62.5%で最良結果を得て、最良クローズドウェイトモデルとの間に10ポイントの差を示しました。GPT-OSS-120B以外のオープンウェイトモデルは全て50%未満のスコアでした。全体として、現在のモデルは効率性と推論能力を効果的にバランスできていません。これは、タスクの複雑さに基づいて計算努力を適応的に調整できる統一モデル（特にオープンレシピとウェイト）の開発に大きな改善余地があることを示しています。

**多くの思考型モデルは簡単なクエリで深刻な過度思考を示す**: OverthinkingBenchでは、全てのモデルが簡単なクエリに対して最低100の思考トークンを生成し、多くのモデルは700トークン以上を生成しました。これはAUC_OAAスコアに反映され、対応する生の精度数値よりもはるかに低くなりました。最高性能のオープンおよびクローズド思考モデルはGPT-OSS-120Bとo3で、それぞれ平均110と135トークンを生成しました。このベンチマークの多くのクエリは「鋼鉄の棒が1メートルの長さの場合、センチメートルでの長さは？」のような簡単な質問であるため、思考型モデルによる不必要な計算は、類似の精度にもかかわらずAUC_OAAスコアを大幅に罰し、より高いコストとユーザーの利便性低下を強調しています。

**しかし、思考型モデルは複雑な推論で大幅な向上を示す**: 簡単なクエリでの過度思考にもかかわらず、思考型モデルはUnderthinkingBenchで非思考型モデルよりもはるかに優れています。o3がUnderthinkingBenchで最高精度64.5%を獲得し、GPT-OSS-120Bが49.7%で続きました。ハイブリッドモードで動作するモデルを分析すると、全てのQwen3モデルが非思考モードで20%以下の精度を示し、Qwen3-32Bは15%のみを達成しました。しかし、これらの同じモデルが思考モードで動作すると、性能が大幅に向上しました。例えば、Qwen3-14Bの精度は思考モードで11%から45%に増加し、34%の改善を示しました。

**蒸留思考型モデルの精度は蒸留後に簡単なクエリで劣化**: R1トレースから蒸留されたQwenモデルなど、思考型モデルのトレースを使用して蒸留された非思考型モデルは、蒸留時にOverthinkingBenchの生の精度数値でも劣化を示しました。例えば、R1-Distill-Qwen-7BはQwen2.5-7Bの94.1%に対して85.1%、R1-Distill-Llama3-8BはLlama3-8Bの95.3%に対して89.8%の性能でした。これは、非思考型モデルを思考実行に特化させることが、思考を必要としないクエリでの性能低下を伴うことを示しています。

### 3.3 既存手法との比較

**効率的推論手法は過度思考を減少させるが性能にも影響**: 思考型モデルでの過度思考を緩和するため、最近提案された効率的推論手法をテストしました。これらの手法は最大68%のトークン使用量を削減しましたが、UnderthinkingBenchでは6つのモデル-手法組み合わせのうち4つでベースバージョンと比較して明確な精度低下がありました。特に、これらの6つの構成のうち2つが全体のF1^otbスコアでアンダーパフォームし、効率向上が推論能力のコストを伴うことを示しました。

**質問難易度ベースルーティングは最適性に寄与するが、オラクルルーターとの大きなギャップが残る**: 簡単な質問（OverthinkingBenchから）に非思考モード、複雑な質問（UnderthinkingBenchから）に思考モードを使用するルーターモデルを活用しました。オープンソースルーターをQwen3モデルで評価し、最良個別モード性能とオラクルルーターの両方と比較しました。ルーターは2%から11%の性能改善で最良個別モードを上回りましたが、全てのルーティング結果はオラクルルーターが達成した結果を大幅に下回り、約12ポイントのギャップがありました。

**明示的プロンプティング**: モデルが明示的に最適に思考するようプロンプトできるかを探索しました。OverthinkingBenchで「過度に思考しない」ようプロンプトすると、精度に影響を与えることなく使用トークンが最大29%一貫して減少しました。対照的に、「段階的に思考しよう」とプロンプトすると、全てのQwenモデルで精度の小さな低下がありましたが、重要なことに思考長が約19-27%増加しました。これは、この口語的プロンプト接尾辞が思考型モデルでの簡単なクエリの過度思考をさらに悪化させることを示唆しています。

## 4. 実用性評価
### 4.1 実装の容易性

OptimalThinkingBenchの実装は比較的容易で、研究コミュニティでの採用に適しています。

**ベンチマーク構築の容易性**では、合成データセット生成レシピにより、人間の介入なしにベンチマークの拡張や難易度調整が可能です。制約付き生成フレームワークは他のドメインや回答タイプへの適用が容易で、ベンチマークの継続的進化を支援します。LLM-as-a-Judgeを用いた自動フィルタリングにより、大規模なデータセットでも効率的な品質管理が可能です。

**評価の実装**では、提案されたメトリクス（OAA、AUC_OAA、F1^otb）は理論的には複雑ですが、実装は比較的単純です。思考トークンのカウントは各モデルのガイドラインに従って行われ、LLM-as-a-Judgeによる正確性判定は標準的な手法です。UnderthinkingBenchでのプログラム的検証器の使用により、客観的で再現可能な評価が可能です。

**拡張性と適応性**では、Reasoning Gymとの統合により、新しい推論タスクの追加が容易です。閾値パラメータ（λ、t_max）の調整により、モデル能力の向上に応じたベンチマークの難易度調整が可能です。72のドメインと4つの回答タイプの組み合わせにより、特定の応用分野に特化したサブベンチマークの構築も可能です。

### 4.2 計算効率

ベンチマークの計算効率は実用的展開において重要な考慮事項です。

**評価コスト**では、33モデルの包括的評価には相当な計算資源が必要です。特に、思考型モデルでの長い推論トレースの生成は、評価時間とコストを大幅に増加させます。OverthinkingBenchの1440問題とUnderthinkingBenchの600問題は、モデルあたり数時間から数日の評価時間を要する可能性があります。

**スケーラビリティの課題**では、8シードでの評価は統計的信頼性を提供しますが、計算コストを8倍に増加させます。temperature sampling（0.6）の使用により、再現性と多様性のバランスを取っていますが、決定論的評価と比較してコストが高くなります。大規模モデルファミリーでの評価では、GPU時間とAPI呼び出しコストが急速に蓄積します。

**効率化の機会**では、バッチ処理と並列評価により、評価時間の短縮が可能です。サブセット評価（例：特定のドメインや回答タイプのみ）により、迅速なプロトタイピングが可能です。キャッシングメカニズムにより、重複するクエリでの計算の削減が可能です。

### 4.3 応用可能性

OptimalThinkingBenchは広範囲な応用領域での活用可能性を持ちます。

**産業応用での価値**では、APIベースLLMサービスプロバイダーにとって、コストと性能のバランスを評価する標準ツールとして価値があります。企業のLLM選択において、特定の使用ケースに最適なモデルの選択を支援します。カスタマーサポート、コンテンツ生成、技術文書作成など、多様なタスクでの最適化指標として活用可能です。

**研究開発への貢献**では、最適思考アルゴリズムの開発において、統一的な評価フレームワークを提供します。新しいアーキテクチャや訓練手法の効果を、単一の包括的メトリクスで評価可能です。効率性と性能のトレードオフに関する研究の標準ベンチマークとして機能します。

**教育と人材育成**では、LLMの効率性と性能の概念を教育する際の具体的な評価ツールとして有用です。学生や研究者が最適化の概念を理解し、実践的なスキルを開発する際の指標となります。産学連携プロジェクトでの共通評価基準として活用可能です。

**将来的な応用拡張**では、マルチモーダルモデルへの拡張により、視覚的推論や音声処理での最適思考評価が可能です。専門分野（医療、法律、金融）特有のタスクへの適応により、分野特化型の最適化評価が可能です。リアルタイムシステムでの応用により、動的な計算資源配分の最適化指標として機能します。

**標準化への貢献**では、業界標準の効率性評価メトリクスの確立に寄与します。規制機関や標準化団体での採用により、LLMの責任ある開発と展開の指針となります。国際的な研究協力での共通評価基準として機能する可能性があります。

## 5. まとめと所感
### 5.1 論文の意義

本研究はLLM評価の分野において極めて重要で革新的な貢献をしました。

**評価パラダイムの革新**として、従来のLLM評価は精度中心であり、計算効率性は副次的な考慮事項でした。本研究は初めて精度と効率性を統合した評価フレームワークを確立し、LLMの実用的価値をより正確に反映する新しいパラダイムを提示しました。特に、過度思考と不十分思考という相反する問題を統一的に扱うアプローチは、従来の二分法的評価を超越した画期的な視点です。

**実世界への適用可能性**では、APIベースLLMサービスの普及により、コストと性能のバランスは実用的に極めて重要になっています。本ベンチマークは、理論的な性能指標と実際のビジネス価値を橋渡しする実用的なツールとして、産業界に直接的な価値を提供します。ユーザー体験の観点から、応答時間と品質のバランスを定量化する手法は、UXデザインやプロダクト開発に重要な指針を与えます。

**方法論的革新**として、思考調整済み精度メトリクス（OAA）の開発は、計算言語学における新しい評価指標の確立として意義深いものです。合成データセット生成とフィルタリングの手法は、ベンチマーク汚染問題に対する実用的解決策を提供し、今後のベンチマーク設計の模範となります。F1スコアによる統一メトリクスは、複数の競合する目標を単一指標で評価する手法として、他の分野への応用可能性も高いです。

**現状の限界の明確化**では、33モデルの包括的評価により、現在のLLM技術の到達点と限界を定量的に明らかにしました。最高性能モデル（o3）でも72.7%のスコアに留まることは、この分野での大きな改善余地を具体的に示しています。オープンウェイトモデルと商用モデルの性能ギャップ（10ポイント）の定量化は、オープンサイエンス推進の重要性を浮き彫りにしています。

**学術的インパクト**として、思考型LLMの理論的理解を深化させ、計算効率性と推論能力の関係性に関する新しい研究領域を開拓しました。効率的推論手法の体系的評価により、この新興分野での研究方向性を明確化しました。ルーティング手法やプロンプティング戦略の効果の定量化は、実用的な最適化技術の発展に重要な知見を提供しています。

### 5.2 今後の展望

本研究の成果を基盤として、複数の発展方向が期待されます。

**技術的発展**として、より洗練された適応的思考制御アルゴリズムの開発が期待されます。現在の手法では、事前に定義されたルールや閾値に基づく制御が主流ですが、動的にタスクの複雑さを認識し、リアルタイムで思考深度を調整するアルゴリズムの開発が重要です。強化学習やメタ学習を用いた自己適応型思考モデルの研究も有望な方向性です。

**ベンチマークの拡張**では、現在の72ドメインと4つの回答タイプをさらに拡張し、より多様な実世界タスクをカバーするベンチマークの開発が必要です。マルチモーダルタスク（視覚推論、音声処理、ロボティクス）への拡張により、より包括的な最適思考評価が可能になります。専門分野（医療診断、法律相談、金融分析）特化型のサブベンチマークの開発も重要です。

**理論的深化**として、思考の質と量の関係性に関する理論的理解の深化が求められます。何が「最適な」思考量を決定するのか、タスクの複雑さをどう定量化するかなど、根本的な問題への取り組みが必要です。認知科学や心理学の知見を統合した、人間の思考プロセスとの比較研究も価値があります。

**実用化の推進**では、リアルタイムシステムでの動的な計算資源配分アルゴリズムの開発が重要です。クラウドコンピューティング環境での自動スケーリングやロードバランシングへの応用により、実際のサービス運用での価値を実現できます。エッジコンピューティング環境での軽量化実装も、IoTや組み込みシステムでの応用に重要です。

**産業標準化**では、業界標準の効率性評価メトリクスとして、本ベンチマークの普及と標準化が期待されます。規制機関や認証団体での採用により、LLMの責任ある開発と展開の指針となる可能性があります。国際標準化機構（ISO）などでの標準化プロセスへの参画も重要です。

**社会的インパクト**として、教育分野での個別化学習システムにおいて、学習者の理解度に応じた適応的説明生成への応用が期待されます。医療分野では、緊急度に応じた診断支援システムでの活用により、限られた計算資源の効果的配分が可能になります。環境配慮の観点から、AI計算の省エネルギー化に貢献し、持続可能なAI開発の推進にも寄与します。

**研究コミュニティの発展**では、最適思考研究の専門コミュニティの形成により、知見の共有と協力的研究の推進が期待されます。年次会議やワークショップの開催、専門論文誌の創設などにより、分野の体系的発展を支援できます。産学連携の促進により、理論研究と実用化の橋渡しも重要です。

**長期的ビジョン**として、真に適応的で効率的なAIシステムの実現に向けて、本研究は重要な基盤を提供しています。将来的には、人間のように直感的に問題の複雑さを判断し、適切な認知資源を配分できるAIシステムの開発が期待されます。これは、AGI（汎用人工知能）の実現における重要な要素の一つとなる可能性があります。