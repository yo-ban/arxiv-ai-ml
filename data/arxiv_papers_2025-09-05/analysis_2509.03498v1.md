# OneCAT - Decoder-Only Auto-Regressive Model for Unified Understanding and Generation

## 基本情報
- **arXiv ID**: 2509.03498v1 (https://arxiv.org/abs/2509.03498)
- **著者**: Han Li他9名 (Meituan Inc, Shanghai Jiao Tong University)
- **所属**: Meituan Inc, Shanghai Jiao Tong University
- **投稿日**: 2025年09月05日
- **カテゴリ**: cs.CV, cs.LG, cs.AI

## 簡単に説明すると

この論文は、統一されたマルチモーダル理解、生成、編集を実現する「OneCAT」という純粋なデコーダオンリーのトランスフォーマーモデルを提案しています。従来のマルチモーダルモデルが外部コンポーネント（Vision Transformerや視覚トークナイザー）を必要とする複雑な多段階パイプラインを採用しているのに対し、OneCATはそれらを推論時に完全に排除し、シンプルで効率的なアーキテクチャを実現しています。

OneCATの核心は、モダリティ固有のMixture-of-Experts（MoE）構造にあります。テキスト処理用、視覚理解用、視覚生成用の3つの専門的なFFNエキスパートを持ち、単一の自己回帰目標で訓練されます。また、マルチスケール視覚自己回帰メカニズムを採用し、拡散ベース手法と比較してデコーディングステップを大幅に削減しながら最先端の性能を維持しています。プロジェクトページは https://onecat-ai.github.io/ で公開されています。

## 1. 研究概要

### 1.1 背景と動機

この研究の動機は、現在のマルチモーダルフレームワークの根本的な問題に対処することから生まれています。従来のアプローチは、理解、生成、編集タスクに対して別々のアーキテクチャを利用するモジュラー設計を採用しており、複雑な多段階パイプラインを作成していました。

このような設計は性能面では有効なシステムを生み出しますが、本質的に複雑さをもたらし、アーキテクチャのボトルネックを生じさせます。特に、クロスモーダル情報の深い早期段階での融合を制限し、重大な推論レイテンシを導入することで、効率性と性能の両方に対する主要な障壁となっています。

統一マルチモーダルモデリングへの収束が進む中、多くの現在のモデルは依然としてモジュラーパラダイムに束縛されています。著者らは、統一システムの完全な潜在能力を解放するために必要な真のパラダイムシフトには、外部コンポーネントを排除するより根本的で第一原理に基づくアーキテクチャへの移行が必要であると主張しています。

### 1.2 主要な貢献

この研究の主要な貢献は以下のように整理できます。

- アーキテクチャの革新として、推論時に外部のVision TransformerやVAEトークナイザーを完全に排除する純粋なデコーダオンリーアーキテクチャを提案
- モダリティ固有のMixture-of-Experts（MoE）構造を導入し、テキスト、視覚理解、視覚生成の各タスクに特化した3つのFFNエキスパートを実装
- Scale-Aware Adapter（SAA）を開発し、マルチスケール視覚生成のための階層的表現学習を実現
- 3段階訓練パイプラインによる統一訓練手法の開発と理解蒸留戦略の導入
- 既存のオープンソース統一モデルを上回る最先端性能の達成

## 2. 提案手法

### 2.1 手法の概要

OneCATは純粋なデコーダオンリーアーキテクチャを採用し、推論時に追加の視覚エンコーダーやトークナイザーを必要としません。この流線型設計により、モデル構造が大幅に簡素化され、計算オーバーヘッドが削減されています。

従来のマルチモーダルモデルが理解タスクにViTのようなセマンティックエンコーダーに依存するのとは異なり、OneCATは軽量なパッチ埋め込み層を利用します。この層は生画像を連続的な視覚トークンに無損失変換し、効率的なマルチモーダル理解を可能にします。

OneCATの中核には、3つの専門的なフィードフォワードネットワーク（FFN）エキスパートから構成されるMixture-of-Experts（MoE）アーキテクチャが統合されています。1つは言語理解のためのテキストトークン処理専用、もう1つはマルチモーダル理解を促進するための連続視覚トークン用、3つ目は画像合成中の離散視覚トークン生成に最適化されています。

### 2.2 技術的詳細

OneCATモデルは、事前訓練されたQwen2.5 LLMから初期化され、その強力な言語モデリング能力を基盤として活用します。MoEアーキテクチャを構築するため、各Qwen2.5トランスフォーマーブロックの元のFFN層を複製して3つの異なるエキスパートを形成します：Text FFN、Visual Understanding FFN、Visual Generation FFNです。

マルチモーダル理解では、生画像を連続視覚トークンのシーケンスに変換するパッチ埋め込み層を使用します。この層は、画像パッチ化のための14×14畳み込み、視覚トークン圧縮のための2×2ピクセルアンシャッフル操作、LLMの隠れ状態次元に一致するよう視覚特徴を投影する2層MLPから構成されます。

テキスト画像生成では、InfinityからのマルチスケールVAEモデルを活用し、ピクセル空間と潜在空間の間で画像をマッピングします。このVAEは16のダウンサンプリング比と32の潜在チャンネルサイズで動作し、語彙を拡大するためのビット単位量子化器を組み込んでいます。

Scale-Aware Adapter（SAA）は、マルチスケールVAEトークンの階層的性質に対処するために導入されました。各スケールの離散視覚トークンに対して専用の低ランク適応モジュールを提供し、異なるスケールが画像生成の異なる側面を制御するという原理に基づいた処理を可能にします。

### 2.3 新規性

この研究の新規性は複数の側面で示されています。まず、純粋なデコーダオンリー設計により、推論時の外部コンポーネント（ViT、VAE）を完全に排除し、特に高解像度入力に対する大幅な効率向上を実現しています。

Scale-Aware Adapter（SAA）の導入により、マルチスケールVAEトークンの階層的性質を活用し、各スケールの離散視覚トークンに対して専用の低ランク適応モジュールを提供します。これにより、異なるスケールが画像生成の異なる側面を制御するという原理に基づいた処理が可能になります。

マルチモーダル多様な注意メカニズムでは、テキストトークンには因果的注意、連続視覚トークンには完全注意、マルチスケール離散視覚トークンにはブロック因果的注意を適用する柔軟な注意パターンを実現しています。

理解蒸留戦略により、カスタムMLLM教師モデルから深い特徴レベルマッチングを通じて視覚知識を効率的に転移させる手法を開発しました。

## 3. 実験結果

### 3.1 実験設定

実験は2つのモデルバリアント、OneCAT-1.5BとOneCAT-3Bで実施されました。OneCAT-1.5Bモデルは、Qwen2.5-1.5B-instructに基づき、1.5Bのアクティブパラメータ（総計4.5B）を含みます。OneCAT-3Bモデルは、Qwen2.5-3B-instructに基づいて構築され、3Bのアクティブパラメータ（総計9B）を利用します。

3段階訓練パイプラインが採用されました。ステージ1では、理解蒸留用に4億3600万のマルチモーダル理解サンプル、生成事前訓練用に5200万のテキスト画像生成サンプルを使用しました。理解サンプルと視覚生成サンプルの訓練トークン比は約8:1でした。

ステージ2の統一中期訓練では、7000万の視覚指示サンプル、6000万の視覚生成サンプル、4000万のテキストオンリー指示サンプルを使用し、最終的な訓練トークン比は約1:2:6でした。ステージ3のSFTでは、1300万の高品質サンプルと300万の視覚生成サンプルを使用しました。

### 3.2 主要な結果

定量的結果において、OneCATは既存のオープンソース統一マルチモーダルモデルを複数のベンチマークで上回る性能を示しました。マルチモーダル理解タスクでは、従来の複雑なViTベース設計と比較して競争力のある結果を達成しながら、推論効率を大幅に改善しています。

テキスト画像生成では、拡散ベース手法と比較してデコーディングステップを大幅に削減しながら、高品質な出力を生成することが実証されました。マルチスケール自己回帰メカニズムにより、粗い解像度から細かい解像度への階層的生成プロセスが実現され、速度と出力品質の両方が向上しています。

画像編集タスクでは、参照画像の近無損失表現を提供するパッチ埋め込み層により、LLMの浅い層が低レベル特徴にアクセスしてピクセルレベルの一貫性を保ち、深い層が高レベル特徴を抽出してセマンティック理解を行うことが可能になっています。

効率性の観点では、外部コンポーネントの排除により、特に高解像度入力に対して大幅な推論速度向上を実現しています。

### 3.3 既存手法との比較

統一マルチモーダルモデリングにおいて、Chameleon、Show-o、Janusなどの既存手法と比較して、OneCATは推論時の外部コンポーネント完全排除という点で差別化されています。これらの手法の多くは依然として別個の視覚エンコーダーや生成モジュールに依存しています。

アーキテクチャ設計の観点から、従来の統合モデルが中央の言語モデルに別個の視覚エンコーダーや生成モジュールを移植する複雑な多コンポーネントパイプラインに依存しているのに対し、OneCATは第一原理に基づいたより根本的なアプローチを採用しています。

効率性では、モダリティ固有MoE構造により、深い早期段階でのクロスモーダル情報融合を促進し、推論時に効率的な方法で実現しています。これは、多くの既存手法が抱えるアーキテクチャボトルネックを回避しています。

## 4. 実用性評価

### 4.1 実装の容易性

実装は中程度の複雑さを持ちます。純粋なデコーダオンリー設計により、従来のモジュラーアプローチと比較してアーキテクチャが大幅に簡素化されています。外部コンポーネントの排除により、システム統合の複雑さが削減され、デプロイメントが容易になっています。

3段階訓練パイプラインは体系的ですが、各段階で異なるデータセットとハイパーパラメータ設定を要求します。理解蒸留戦略の実装には、カスタム教師モデルの構築と深い特徴レベルマッチングの実装が必要です。

モデルの利点として、Qwen2.5 LLMからの初期化により、強力な言語モデリング能力を継承できる点があります。MoE構造により、各モダリティ/タスクに対する専門的処理が可能になり、パフォーマンスと効率性のバランスが取れています。

### 4.2 計算効率

訓練効率の観点では、3段階パイプラインにより段階的に能力を獲得することで、計算リソースの効率的利用が可能になっています。理解蒸留戦略により、大規模視覚データセットでのゼロから学習と比較して大幅な計算削減を実現しています。

推論効率では、外部コンポーネントの排除により、特に高解像度入力に対して大幅な速度向上を達成しています。純粋なデコーダオンリー設計は、メモリ使用量とレイテンシを大幅に削減します。

MoE構造により、各時点でアクティブなパラメータが限定され（例：OneCAT-1.5Bでは4.5B総パラメータのうち1.5Bがアクティブ）、効率的な計算が実現されています。マルチスケール自己回帰生成は、拡散ベース手法と比較してデコーディングステップを大幅に削減しています。

### 4.3 応用可能性

実世界での適用可能性は非常に高い潜在力を持っています。マルチモーダルAIアシスタントでは、理解、生成、編集を統合した包括的な視覚対話システムへの応用が期待されます。コンテンツ制作では、自動画像生成と編集のためのクリエイティブツール、高品質な画像コンテンツのリアルタイム生成が可能になります。

教育分野では、視覚的質問応答システム、マルチモーダル学習教材の自動生成、インタラクティブな教育コンテンツの作成への応用が考えられます。Eコマースでは、商品画像の自動生成と編集、視覚的商品検索と推薦システムへの活用が期待されます。

実装上の利点として、統一アーキテクチャにより、複数のタスクを単一システムで処理可能になり、システム複雑さが削減されます。推論効率の向上により、リアルタイムアプリケーションへの適用が容易になります。

実用的考慮事項として、高解像度画像処理には依然として相当な計算リソースが必要です。マルチスケール生成の最大解像度制限により、一部のアプリケーションでは制約となる可能性があります。

## 5. まとめと所感

### 5.1 論文の意義

この研究は、マルチモーダルAIの分野における重要なパラダイムシフトを提示しており、複雑なモジュラーアプローチから統一されたデコーダオンリーアーキテクチャへの転換を示しています。外部コンポーネントの完全排除により、推論効率の大幅な向上を実現し、実用的なマルチモーダルシステムの新たな可能性を開拓しています。

モダリティ固有MoE構造とScale-Aware Adapterの導入により、統一アーキテクチャ内での専門化と効率性のバランスを巧妙に実現しています。これは、将来のマルチモーダルモデル設計における重要な指針を提供しています。

理解蒸留戦略の開発により、大規模視覚データセットでのゼロからの学習に対する効率的な代替手段を提示し、計算リソース制約下での高性能モデル開発への道筋を示しています。

### 5.2 今後の展望

この研究は多くの将来研究方向を開拓しています。アーキテクチャの進化として、さらなる外部コンポーネント削減と統一化の推進、より効率的なMoE設計とルーティング戦略の開発が期待されます。スケーラビリティとして、より大規模なモデルでの検証、より高解像度画像への対応拡張が重要です。

マルチモーダル能力の拡張として、動画理解と生成への適用、音声モダリティの統合、3Dコンテンツ生成への拡張が考えられます。効率性の向上では、よりアグレッシブなモデル圧縮技術、モバイルデバイス向け軽量化、リアルタイムアプリケーション最適化が必要です。

長期的には、この研究によりマルチモーダルAI分野全体でのパラダイムシフトが加速され、より効率的で統一されたアーキテクチャが主流となる可能性があります。真に汎用的なマルチモーダル知能システムの実現に向けた重要な一歩として、学術界と産業界の両方に大きな影響を与えることが期待されます。
