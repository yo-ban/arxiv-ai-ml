# Strefer - Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data

## 基本情報
- arXiv ID: 2509.03501v1 (https://arxiv.org/abs/2509.03501)
- 著者: Honglu Zhou他7名 (Salesforce AI Research)
- 所属: Salesforce AI Research
- 投稿日: 2025年09月05日
- カテゴリ: cs.CV, cs.CL, cs.AI

## 簡単に説明すると

この論文は、動画理解LLM（Video LLM）の時空間での推論能力を向上させる「Strefer」という合成指示データ生成フレームワークを提案しています。従来のVideo LLMについては大まかな動画理解ができますが、細かい時空間推論が苦手でした。特に時間に基づくイベント参照や空間アンカーリングのためのジェスチャー手がかりを含むユーザクエリに対して困難を示していました。

Strefer は複数の事前訓練済みモデルを組み合わせて、動画に対して時間的に密で、マスク形式の空間情報と時間軸情報を含む構造化メタデータを擬似アノテーションし、現実的な指示応答ペアを生成します。わずか4,253本の動画から947,854個の指示ペアを生成し、オープンソースモデルのみを使用している点が特徴的です。プロジェクトページは https://strefer.github.io/ で公開されています。

## 1. 研究概要

### 1.1 背景と動機

この研究の動機は、現実のAIアシスタントが人間のクエリを空間と時間の両方で理解する必要があるという認識から生まれています。人々は日常的に非言語的手がかり（オブジェクトを指すジェスチャー）や時間ベースの参照（「今日の午前11時に誰と話したか」）を使用します。

現在のVideo LLMは粗いレベルで動作し、時空間での参照能力について必要な細かさに欠けています。既存のVideo LLMについては大まかな動画理解が可能ですが、細かい空間的・時間的推論で苦労しています。特に以下のような場面で困難を示します。

複数の同じカテゴリのエンティティが存在する場合、エンティティが最初のフレームに存在しない場合、一時的な退出と再入場がある場合などです。また、ユーザが「あの人」「さっきの車」といった参照表現を使用する際の曖昧さ解消や、「2分30秒の時点で何が起こったか」といった時間的な問い合わせへの対応も課題となっています。

### 1.2 主要な貢献

この研究の主要な貢献は以下のように整理できます。まずスケーラブルな方法論として、時間的に密で、オブジェクト中心の構造化された時空間メタデータで動画を擬似ラベル付けし、合成指示応答ペアを生成するスケーラブルなアプローチを提案しています。

未探索の能力として、動画ベースのクエリにおける細かい、時空間的、オブジェクト中心の参照のためのデータ合成を運用化しています。実証的検証として、Strefer データで訓練されたVideo LLMが空間的・時間的曖昧さ解消を必要とするタスクでベースラインを上回ることを実証しています。

技術的な観点では8つのデータグループ（G1-G8）を体系的に分類し、伝統的、タイムスタンプ参照、マスク参照タスクをカバーしています。また、プロプライエタリモデルに依存せず、エラー伝播の可能性があるものの、複数モデルコンポーネントの依存関係を管理しています。

## 2. 提案手法

### 2.1 手法の概要

Strefer は複数の事前訓練済みエージェントを組み合わせたモジュラーフレームワークを採用し、動画メタデータを擬似アノテーションして指示データを生成します。システムは最大3分の動画を処理し、同じカテゴリの複数エンティティ、最初のフレームに存在しないエンティティ、一時的な退出・再入場など複雑なシナリオを処理できます。

アーキテクチャは6つの主要コンポーネントから構成されています。具体的にはEntity Recognizer、Referring Parser、Referring Masklet Generatorです。また、Video Clipper、Video Transcriber、Instruction Data Generatorも含まれます。これらのコンポーネントが協調し、包括的な動画理解と指示データ生成を実現しています。

### 2.2 技術的詳細

Entity Recognizerは Tarsier-34B Video LLM を使用して、動画全体で動的行動を示すすべての「アクティブエンティティ」を特定し、明確な区別記述を提供します。Referring Parserは Qwen2.5-32B-Instruct LLM を使用してエンティティ記述から構造化リストを抽出します。

Referring Masklet Generatorが最も重要な技術的貢献です。4段階のプロセスで構成されています。第1段階はフレームサンプリングと並び替えです（重要なコンテンツが動画中央付近にあると仮定）。第2段階では汎化されたエンティティ名詞とGroundingDINOを使用して初期追跡フレーム選択します。第3段階は選択されたフレームからのSAM2による双方向追跡です。第4段階でRexSeekによるマスクレットへの参照表現の割り当てをします。

Video Clipperは PySceneDetect（HSVベース、閾値=20）と SigLIP 埋め込みと階層クラスタリングを組み合わせてセマンティックセグメンテーションを行います。Video Transcriberは2段階プロンプト（存在チェック + 行動記述）を使用して時間的に密な行動中心の転写を生成します。

### 2.3 新規性

この研究の新規性は複数の側面で示されています。まず包括的なマスクレット生成により、GroundedSAM2の制限を解決し、複数語表現、最初のフレームに存在しないエンティティ、複雑な追跡シナリオを処理します。

スケーラブルな擬似アノテーションにより、プロプライエタリモデルなしで、わずか4,253本の動画から947,854個の指示ペアを生成しています。マルチモーダルプロンプト生成では、空間的（マスク/軌跡）と時間的（タイムスタンプ）参照を含む現実的な人間-AI相互作用パターンを作成します。

数学的・アルゴリズム的定式化として、時間トークン変換では動画持続時間D秒とN個の時間トークンに対して、タイムスタンプTをトークン⌊(T/D) × N⌋に変換します。フレーム並び替えヒューリスティックは追跡初期化のために中央フレームを優先し、3 FPSでのSigLIP埋め込みの階層クラスタリングをセマンティック動画セグメンテーションに使用します。

## 3. 実験結果

### 3.1 実験設定

ベースアーキテクチャとしてBLIP-3をBLIP-3-Video拡張付きで使用し、4Bパラメータです。1動画あたり32フレーム、32時間トークン、3×8 H200 GPUで1日間訓練しました。

ベースレシピはBLIP-3-Video指示データ + VideoRefer-700K（合計1,948,679サンプル）を使用しています。実装にはオープンソースモデルのみを使用しています。具体的にはTarsier-34B、Qwen2.5-32B-Instruct、RexSeek-3B、SAM2、GroundingDINO-tinyです。

データセットとベンチマークは5つの主要評価を含みます。VideoRefer-Bench^Dは、Panda-70Mテストセットから400動画でマスク参照領域記述を評価します。VideoRefer-Bench^Qは、198動画の1,000問の多肢選択問題でマスク参照QAを評価します。QVHighlightsは、バランスの取れた正負サンプルでタイムスタンプベースのYes/No QA用に再利用されます。TempCompass（時間知覚と推論評価）とVideoMME（最大1時間の長動画理解）も含まれます。

### 3.2 主要な結果

定量的結果において、最終レシピ対ベースラインで主要な性能改善が示されています。マスク参照領域記述では3.28から3.39への平均スコア向上を示しています。マスク参照領域QAでは0.665から0.688への精度向上、タイムスタンプベースQAでは0.5288から0.6031への精度向上を実現しています。TempCompassでは60.100から61.675への平均スコア向上、VideoMMEでは37.45から37.70へのスコア向上を達成しています。

重要な発見として、わずか28.73%のサンプル追加（545動画）でも、すべてのベンチマークで一貫した改善が得られています。アブレーション研究では、G7データ（マスク+タイムスタンプクエリ、27Kサンプル）だけでベースレシピの1.39%にもかかわらず大幅な改善を提供することが判明しました。

### 3.3 既存手法との比較

動画空間参照において、ボックスレベル表現を使用するArtemis、Elysium、Merlinとは異なり、Strefer はマスクベースの理解に焦点を当てています。DAM、PAM、Omni-RGPT、SAMAなどの同時期の研究と比較して、Strefer はプロプライエタリモデルに依存せず、VideoReferよりも複雑なシナリオを扱います。

動画タイムスタンプ参照では、タイムスタンプベースの時間アノテーションと指示に特化した初の研究です。動画指示データにおいて、ShareGPT4Video、LLaVA-Video-178Kとは異なる特徴があります。プロプライエタリモデルや手動アノテーションなしで空間時間的に根拠付けられたデータを生成する点で区別されています。

質的結果では、エンティティ曖昧さ解消シナリオで優れた性能を示しています。また、ベースラインと比較した前景バイアス処理の改善、細かい空間と時間での動作理解の向上、タイムスタンプベースの動画理解の向上も実現しています。

## 4. 実用性評価

### 4.1 実装の容易性

実装は中高レベルの複雑さを持ちます。6つ以上の異なるモデル/コンポーネントのオーケストレーションが必要です。訓練には1日間3×8 H200 GPUを必要とします。一方、推論では標準ハードウェアを使用します。データエンジンの複雑さとして、エラー伝播の可能性のある多段階パイプラインがあります。

モジュラー設計により個別コンポーネントを独立してアップグレードできる利点があります。しかし複数のモデル依存関係により正確な再現が複雑になる可能性があります。リソース要件として訓練には3×8 H200 GPUで1日間必要ですが、推論は標準的なハードウェアを使用できます。

開発上の考慮事項として、オープンソースのみの依存関係により再現可能性が高く、4Bパラメータモデルサイズは展開に管理可能です。ただし、複数モデル擬似アノテーションパイプラインは計算集約的で、コンポーネント間のエラー伝播の可能性があります。

### 4.2 計算効率

訓練効率の観点では、ベースラインと比較して最小限の追加動画データ（+545動画）で大幅な利得を達成することが肯定的です。しかし複数モデル擬似アノテーションパイプラインは計算集約的という懸念もあります。最適化として、4Bパラメータモデルサイズは展開に管理可能です。

推論効率では、アーキテクチャの柔軟性により、アーキテクチャ拡張と視覚的プロンプトアプローチの両方をサポートします。トークン効率として、1動画あたり32フレームと32時間トークンを使用し、スケーラビリティとして、モジュラー設計によりリソース制約に応じてコンポーネント置換が可能です。

評価では最大3分の動画を処理できます。ただし、より長い動画への拡張性は不明確です。また、単純なベースラインと比較した推論時間/メモリ要件の詳細分析を欠いています。現在の32フレーム制限により長尺動画での有効性に制約をもたらす可能性があります。

### 4.3 応用可能性

実世界での適用可能性は非常に高い潜在力を持っています。ナビゲーションシステムでは、ジェスチャーや時間的参照を理解するAIコンパニオン（「5分前に通った建物」）への応用が期待されます。サーベイランスでは細かい空間時間イベントの検出と分析に使用できます。インタラクティブロボティクスでは人間の空間的・時間的手がかりに応答する時空間認識ロボットへの応用があります。アクセシビリティでは詳細な動画コンテンツ理解のための支援技術への活用が考えられます。

実装上の利点として、以下の4つのポイントがあります。第1に、プロプライエタリ依存関係がない完全オープンソースモデルパイプラインです。第2に、約4K動画からほぼ1M指示ペアを生成するスケーラビリティです。第3に、モジュラー設計により個別コンポーネントを独立してアップグレード可能な点です。第4に、高価な人手アノテーションや大規模な動画収集を回避できるコスト効率です。

実用的な考慮事項として、時間制約（現在最大3分）、マスクのみのサポート（点、ボックス、スクリブルは未対応）、依然としてLLMバックボーン能力に大きく依存することが挙げられます。現在のPhi-3-mini-4k-instructの能力により性能が制限される面もあります。

## 5. まとめと所感

### 5.1 論文の意義

この研究は次世代AIコンパニオンに向けた基礎的進歩を表しており、洗練された時空間推論が可能なシステムの実現に貢献しています。指示データにおける「量より質」という重要な洞察を実証し、わずか28.73%のサンプル追加で改善を達成しています。これによりVideo LLM訓練における計算コストと時間の削減に関する重要な示唆を提供しています。

完全オープンソースアプローチにより研究がアクセス可能で再現可能になり、研究コミュニティ全体において時空間での動画理解の進歩を加速する可能性があります。マスクレット生成における技術的革新と合成指示データ生成への体系的アプローチは、細かい時空間タスクでのVideo LLM訓練の新基準を確立しています。

ロボティクス、サーベイランス、インタラクティブAIシステムでの明確な応用により、実用的価値の高い研究となっています。従来の大まかな動画理解から、人間の自然な参照パターン（ジェスチャー、時間参照）に対応できる細かい理解への転換は、AI分野における重要な進歩です。

### 5.2 今後の展望

この研究は多くの将来研究方向を開拓しています。データ品質の向上として、フィルタリング戦略とフィードバック検証メカニズムの開発が必要です。出力レベル根拠付けとして、入力レベル参照から出力レベル時空間根拠付けへの移行が期待されます。

空間参照の多様性として、マスクを超えて点、ボックス、スクリブルへの拡張が重要です。データ混合最適化として、最適な訓練データ構成の体系的探索が必要です。モデルスケーリングでは、より大きく強力なLLMバックボーンでの訓練が期待されます。

長期的には、この研究により時空間での動画理解研究コミュニティ全体において進歩を加速し、実世界でのマルチモーダルAI応用の実現可能性を高める効果があります。技術的制約（3分制限、マスクのみサポート）の解決により、より包括的な時空間理解システムの開発につながることが期待されます。

特に重要なのは、人間の自然な参照パターン（「あそこの」「さっきの」といった指示語と時間参照）をAIが理解できるようになることで、人間-AI相互作用がより自然で直感的になる可能性があることです。これは、真に実用的なAIアシスタントの実現に向けた大きな一歩と言えるでしょう。