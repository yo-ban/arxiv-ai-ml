# EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow

## 基本情報
- arXiv ID: 2507.06224v1 (https://arxiv.org/abs/2507.06224)
- 著者: Yixiang Chen・Peiyan Li・Yan Huang・
  Jiabing Yang・Kehan Chen・Liang Wang
- 所属: (1) NLPR・Institute of Automation・
  Chinese Academy of Sciences。
  (2) School of Artificial Intelligence・
  University of Chinese Academy of Sciences。
  (3) FiveAges
- 投稿日: 2025年07月09日
- カテゴリ: cs.CV, cs.RO

## 簡単に説明すると
EC-Flowは、アクションラベルなしのビデオからロボット操作を学習する新しいフレームワークです。
従来の手法は物体中心のフロー予測に依存していましたが、変形可能な物体やオクルージョン、非変位操作などで問題がありました。
EC-Flowは「実施体中心フロー」を予測することで、これらの制限を克服します。

具体的には、ロボットアームの関節の動きを追跡します。
そして、URDFファイルを使って実行可能なアクションに変換します。
ゴール画像予測を組み合わせることで、言語指示と物体相互作用を適切に扱えるようになりました。
シミュレーションと実世界の両方で評価し、従来手法を大きく上回る性能を達成しています。

GitHubリポジトリ: https://ec-flow1.github.io

## 1. 研究概要
### 1.1 背景と動機
言語ガイド付きロボット操作の分野では、多くの最先端手法が模倣学習フレームワークを採用しています。
これらは、視覚観察と言語指示を低レベルのロボットアクションに直接マッピングするVision-Language-Actionモデルを訓練します。
しかし、これらのアプローチには重要な制限があります。
大規模な低レベルアクションラベル付きデータセットへの依存が強く、そのようなデータは収集が困難でノイズも多いという問題があります。

アクションラベルなしの操作ビデオは豊富に存在し、有用な動作の事前知識を提供できる可能性があります。
最近の研究では、物体中心のフロー予測によってアクション監督を回避しようとしています。
しかし、これらの手法には3つの主要な制限があります。
1. 剛性仮定：物体全体で一様な変換を前提とし、変形可能な物体を扱えない
2. オクルージョンへの脆弱性：物体の状態変化のみからアクションを導出するため、部分観測性のシナリオで効果的でない
3. 非物体変位の操作：物体が非並進的な状態変化（例：丸いスイッチの回転）や静的な状態（例：マウスボタンの押下）の場合に失敗する

### 1.2 主要な貢献
本研究は、物体中心フローから実施体中心フローへと予測を移行することで、これらの制限を根本的に回避できることを示しています。
主な貢献について、以下のような点があります。

- EC-Flow手法の提案により、アクションラベルなしのビデオデモンストレーションのみから操作ポリシーの学習を可能にした
- ゴール画像条件付きフローアライメントメカニズムを設計し、実施体中心フローを言語指示と物体相互作用に整合させた
- URDF対応のアクション計算パラダイムを提案し、視覚的フロー予測から実行可能なアクションへの物理的に根拠のある変換を実現した
- 変形可能な物体、オクルージョン環境、非物体変位の操作タスクでの実験により、従来手法と比較して62%、45%、80%の成功率向上を達成した。

## 2. 提案手法
### 2.1 手法の概要
EC-Flowフレームワークは、ビデオのみのデータセットを使用して言語条件付き操作タスクに対応します。
各デモンストレーションには、固定カメラから撮影されたRGB視覚観察と言語指示が含まれますが、低レベルアクションデータは明示的に除外されています。

提案されたEC-Flowは、2つのモジュールを通じて従来研究の主要な制限を克服します。
実施体中心フロー予測では、物体を追跡する代わりに、実施体のランダムにサンプリングされた点の画素単位のフロー（将来の位置）を予測します。
運動学対応アクション計算では、実施体のURDFモデルと連続する予測されたEC-Flowのみを活用して、アクションラベルの監督なしに実行可能なアクションを計算します。

### 2.2 技術的詳細
**実施体中心フロー予測**
フロー予測データセットの構築から始めます。
まず、Grounded SAMを使用して実施体の画素単位マスクを抽出し、マスク内でランダムに点をサンプリングします。
CoTrackerモデルを使用して、初期クエリ点の将来のuv座標と可視性を予測し、グラウンドトゥルースの点追跡結果を取得します。

拡散ベースのフロー予測では、条件付き拡散プロセスを実装してフローのノイズを除去します。
マルチモーダル条件信号には、ResNet-50でエンコードされた視覚的コンテキスト、CLIPテキストエンコーダからの言語ガイダンス、初期点座標が含まれます。
推論時には、DDIMサンプリングを使用して迅速に軌跡を生成します。

**ゴール画像予測**
予測された実施体中心フローは動作ダイナミクスを捉えますが、物体相互作用や言語指示との意味的整合を本質的に保証するものではありません。
このギャップを埋めるために、視覚的グラウンディングメカニズムとして機能する補助的なゴール画像予測タスクを導入します。

**運動学対応アクション計算**
予測されたフローを実行可能なアクションに変換するには、実施体の関節構造を考慮します。
まず、一貫した可視性、最小動作しきい値を超える有意な変位、有効な深度情報という3つの基準に従ってサンプリング点をフィルタリングします。
次に、URDFファイルの幾何学的属性を使用して点を関節に割り当て、エンドエフェクタの変換を計算します。

### 2.3 新規性
既存のロボット操作学習手法と比較して、EC-Flowには以下の新規性があります。

1. **実施体中心アプローチ**：物体の特性や状態に依存せず、変形可能な物体と剛体の両方で一般化でき、オクルージョンと非物体変位の操作も処理できる。
2. **ゴール条件付きアライメント**：予測されたフローが運動学的制約に従いながら、指示に関連する物体との相互作用を保証する
3. **物理ベースのアクション計算**：URDFファイルを活用して、視覚的予測と実行可能なアクションの間のギャップを埋める
4. **アクションラベルなし学習**：低レベルアクションデータを必要とせず、ビデオデモンストレーションのみから学習可能

## 3. 実験結果
### 3.1 実験設定
評価は2つの環境で実施されました。

**シミュレーション評価**
Meta-Worldベンチマークを使用し、Sawyerロボットアームによるテーブルトップ操作タスクを評価しました。
9つのタスクについて、各タスクで5つのビデオデモンストレーションを使用し、合計45本のビデオで訓練しました。
各タスクは、ランダムに初期化された物体位置で25回評価されました。

**実世界評価**
Franka Research 3ロボットアームとIntel Realsense D435i RGB-Dカメラを使用しました。
7つの操作タスクを3つのカテゴリで構築しました。
以下のようなタスクが含まれています。
- 剛体物体（3タスク）
- 変形可能な物体（2タスク）
- 非物体変位の操作（2タスク）

### 3.2 主要な結果
**シミュレーション結果**
Meta-Worldベンチマークでの結果について、EC-Flowは従来の物体中心フロー手法と比較して平均で16.4%の改善を達成しました。
特に、btn-top-pressとhammer-strikeタスクでは平均65%の成功率向上を示しました。
これらのタスクでは物体のオクルージョンが課題となっており、従来手法は正確な物体追跡に苦戦していました。

**実世界結果**
実世界評価では以下の改善を達成しました。
- 変形可能な物体操作：45%の改善
- 非物体変位の操作：80%の改善
- 行動クローニング手法も上回り、各タスク5本のビデオのみで優れた性能を達成

### 3.3 既存手法との比較
EC-Flowは以下の6つの最先端手法と比較されました。

1. BC-Scratch：標準的な行動クローニングベースライン
2. BC-R3M：R3Mの事前学習重みを使用
3. UniPi：ビデオ予測と逆ダイナミクスモデルの2段階アプローチ
4. Diffusion Policy：拡散ベースのアクション予測器
5. AVDC：物体中心フローパラダイム
6. Track2Act：物体中心フロー予測を直接使用

EC-Flowは、アクションラベル付きデータを使用する手法（1-4）と、ビデオのみを使用する手法（5-6）の両方を上回りました。

## 4. 実用性評価
### 4.1 実装の容易性
EC-Flowの実装は比較的直接的です。
必要なのは標準的なURDFファイルのみで、これはほとんどのロボットシステムで利用可能です。
既存のオフザシェルフモデル（Grounded SAM、CoTracker）を活用することで、実装の複雑さを軽減しています。

### 4.2 計算効率
8つのNvidia 4090 GPUを使用して1日で訓練が完了します。
推論時には、250回のDDIMサンプリング反復を実行しますが、リアルタイム制御には十分な速度です。
サンプリング点数Npを400、予測ホライズンTを8に設定することで、性能とコストのバランスを取っています。

### 4.3 応用可能性
EC-Flowの応用範囲は広く、以下のような分野での活用が期待されます。
- **製造業**：変形可能な材料の操作、組立作業。
- **医療**：柔軟な組織の操作、手術支援。
- **家庭用ロボット**：洗濯物の折りたたみ、調理作業。
- **倉庫自動化**：様々な形状の物体の取り扱い。

## 5. まとめと所感
### 5.1 論文の意義
EC-Flowは、アクションラベルなしのビデオからロボット操作を学習する新しいパラダイムを提示しています。
実施体中心のアプローチを採用することで、物体中心手法の根本的な制限を克服し、より広範な実世界シナリオへの適用を可能にしました。

特に注目すべきは、変形可能な物体やオクルージョン下での操作など、従来手法では対処が困難だったシナリオでの大幅な性能向上です。
これにより、より実用的で汎用的なロボット操作システムの実現に向けた重要な一歩となっています。

### 5.2 今後の展望
論文では以下の将来の方向性が考えられます。

1. **マルチビューへの拡張**：複数のカメラ視点を使用することで、より堅牢な3D理解を実現
2. **動的環境への対応**：移動する障害物や協働する人間がいる環境での動作
3. **長期タスクへの拡張**：より複雑な多段階タスクへの適用
4. **転移学習の改善**：異なるロボット間での知識転送メカニズムの開発。

一方で、現在のシステムには以下のような制限があります。
- 深度センサーの精度への依存。
- URDFファイルの正確性が必要。
- 訓練データの多様性による性能の影響。
- エンドエフェクタの向きが固定という仮定
