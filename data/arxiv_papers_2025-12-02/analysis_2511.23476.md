# Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction

## 基本情報
- arXiv ID: 2511.23476v1 (https://arxiv.org/abs/2511.23476)
- 著者: Bao Shu, Yan Cai, Jianjian Sun, Chunrui Han, En Yu, Liang Zhao, Jingcheng Hu, Yinmin Zhang, Haoran Lv, Yuang Peng, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Xiangyu Yue
- 所属: CUHK MMLab, Peking University, StepFun, Tsinghua University
- 投稿日: 2024年11月29日
- カテゴリ: cs.AI, cs.LG

## 簡単に説明すると
WMAct（World-Model internalization through efficient interaction and ACTive reasoning）は、
LLMエージェントが複雑な環境で効率的に推論する世界モデルを構築する手法です。
従来の一括推論（モノリシック推論）では、モデルが内部的に全ての状態遷移をシミュレートする必要があり、
認知負荷が高く間違った環境知識を強化するリスクがありました。
WMActは「考えながら行動する」アプローチを採用し、マルチターン相互作用を通じて環境から直接フィードバックを受けながら学習します。
報酬リスケーリング機能と相互作用頻度アニーリング戦略の2つの主要メカニズムにより、
効率的で冗長性の少ない相互作用を促進し、最終的にマルチターンが必要だったタスクを単一ターンで解決できる世界モデル推論を実現しています。

## 1. 研究概要
### 1.1 背景と動機
近年の大規模言語モデル（LLM）の発展により、OpenAI-o1やDeepseek-R1などの高度な推論能力を持つモデルが登場しています。
これらのモデルは強化学習により長時間の推論プロセスを通じて最終答えを導出する能力を獲得し、
数学、コーディング、論理、マルチモーダルタスクなど様々な分野で大きな進歩を遂げています。

この高度な推論能力をベースとして、研究の焦点はLLMを自律エージェントとして機能させることに移っています。
これにより、エージェンティックタスクという新たな挑戦が生まれました。
これらのタスクでは、モデルは解を導出するだけでなく、逐次計画の策定、実行可能なアクションの生成、
動的な相互作用を行う必要があり、環境特性や状態遷移といった世界知識の理解が求められます。

従来の研究では、タスクプランニング、環境知覚、状態変化予測などの明示的に構造化された世界モデル推論プロセスを設計し、
強化学習によってこれらのパターンを強化することで、モデルの環境動態理解を向上させようと試みてきました。
しかし、これらの事前定義された人間設計の認知パターンは短期的には効果的かもしれませんが、
最終的にはモデルが環境との相互作用から自発的に戦略を学習する能力を制限するボトルネックとなります。

### 1.2 主要な貢献
本研究の主要な貢献として、以下の点が挙げられます。

第1に、モノリシック推論とマルチターン相互作用という2つのRL学習パラダイムの比較分析を行いました。
モノリシック推論は与えられたタスクに対して完全な計画を生成する必要がある一方、
マルチターン相互作用では環境との中間フィードバックを通じて段階的に対応できることを示しています。

第2に、WMAct（World-Model internalization through efficient interaction and ACTive reasoning）フレームワークを提案しました。
このフレームワークは構造化された推論からモデルを解放し、「行動を通じた思考」を可能にします。

第3に、効率的な世界モデル推論を実現する2つの主要メカニズムを開発しました。
報酬リスケーリングメカニズムはアクション効果に基づいて結果報酬を調整し、冗長性削減と目的のある相互作用を促進します。
相互作用頻度アニーリング戦略は許可される相互作用ターン数を段階的に減少させ、
モデルが環境手がかりに過度に依存するのではなく学習を内在化するよう促します。

第4に、Sokoban、Maze、Taxiの3つの環境での包括的な実験評価を実施しました。
実験結果は、WMActが従来マルチターン相互作用を必要としていたタスクを単一ターンで解決でき、
複雑な環境への強い転移可能性を持つことを示しています。

## 2. 提案手法
### 2.1 手法の概要
WMActは従来のモノリシック推論の制約から脱却し、マルチターン相互作用を通じて効率的な世界モデル推論を構築します。
従来のアプローチでは、モデルが与えられたプロンプトxに対して全体的な計画と最終答えを含む完全な応答yを生成する必要がありました。
このプロセスでは外部フィードバックなしに広大な解空間の内部シミュレーションが必要で、
継続的な列挙、仮説生成、検証という計算コストの高いプロセスが強いられていました。

WMActでは、このアプローチを根本的に変更しています。
1回で完全な計画yを生成するのではなく、複数ターンにわたって思考ステップTtとアクションセットAtのシーケンスを生成します。
各アクションセットAtは複数の個別アクションを含むことができ、認知負荷を軽減します。
各アクションセットAtの後には環境観測otが続き、最大相互作用ターン数はLmaxによって制約されます。

各ターンtにおいて、モデルは思考ステップTtとアクションセットAt ∼ πθ(·|x, H<t)を生成します。
ここで、H<t = (o1, T1, A1, o2, ..., ot-1, Tt-1, At-1)は相互作用履歴を表します。
最終応答は全ての思考ステップ、アクションセット、観測の連結であり、
完全な軌跡y = (o1, T1, A1, o2 ... oL, TL, AL)を形成します。

### 2.2 技術的詳細
WMActの核心は2つの主要メカニズムにあります。

**報酬リスケーリング機能**は、結果ベースの報酬を効果的なアクションの割合によってスケーリングします。
アクションが前の状態と異なる状態に導く場合、そのアクションは効果的と見なされます。
エピソードがN個のアクションを含み、そのうちNeff個が効果的である場合、
スケールされた報酬Rscaledは次のように計算されます：

Rscaled = Routcome × (Neff / N)

このスケーリング係数は、エージェントが冗長または非効果的なアクションを最小化し、
有意な進歩をもたらすアクションを優先するよう促します。
効果性の基準は環境によって提供される状態変化から直接導出されるため、
エージェントがシステムを悪用することは本質的に困難であり、
報酬信号が真のタスク進歩に根ざしていることを保証します。

**相互作用頻度アニーリング戦略**は、環境フィードバックの即座の利益と
内部推論の長期的な必要性のバランスを取ります。
τ訓練反復ごとに、最近のエピソードから2つの統計を計算します：
プロンプトあたりの平均相互作用ターン数L̄と、観測された最大ターン数L'max。
その後、次の訓練フェーズの最大許可相互作用ターン数を以下のように設定します：

Lmax = (L̄ + L'max) / 2

この動的に調整された制限は2つの目的を果たします。
第1に、ポリシーがより効率的になるにつれてより少ない相互作用を可能にし、モデルの能力に適応します。
第2に、暗黙的にカリキュラム学習の形式を実装します：
訓練初期では、エージェントはより多くのターンで探索することが許可され、
後期段階ではより厳しい制約内でより効率的な問題解決が奨励されます。

### 2.3 新規性
既存手法と比較したWMActの主要な違いは以下の通りです。

第1に、構造化された推論パターンからの解放です。
従来手法は明示的に設計された認知パターン（タスクプランニング、環境知覚など）に依存していましたが、
WMActはモデルが相互作用を通じて自然に推論パターンを形成することを可能にします。

第2に、効率性重視の学習メカニズムです。
単純な結果報酬ではなく、アクション効率性を考慮した報酬リスケーリングにより、
モデルは冗長なアクションを避け、目的のある相互作用を学習します。

第3に、段階的な自律性向上戦略です。
相互作用頻度アニーリングにより、モデルは初期は環境探索を十分に行い、
段階的に環境動態を内在化して自律的な推論能力を獲得します。

第4に、マルチターンから単一ターンへの能力転移です。
従来手法とは異なり、WMActは最終的にマルチターン相互作用で学習した知識を単一ターンで実行できる能力に昇華させます。

## 3. 実験結果
### 3.1 実験設定
実験は3つの異なる環境で実施されました。
Taxi環境では、ナビゲーションと乗客のピックアップ/ドロップオフタスクを通じて、
複雑な指示に従い逐次依存関係を処理するエージェントの能力をテストします。
Maze環境では、グリッド世界での空間ナビゲーションを通じて、空間推論と経路探索能力を評価します。
Sokoban環境では、不可逆的なシンボリック計画と先見性を要求するパズル解決タスクを提示し、
長期的なアクション結果について推論するモデルの能力に挑戦します。

訓練設定では、厳密なオンポリシーPPOアルゴリズムを採用し、
KL散逸損失とエントロピーボーナスを省略しています。
主な実験はQwen3-8B-Own（独自の一般・推論データセットで監督付き細調整を受けたモデル）を使用し、
ベースモデル能力の影響を分離するためQwen2.5-instruct-7Bとの比較も含んでいます。
訓練時のグローバルバッチサイズは256、デコーディング温度1.0で8つの応答をサンプリングし、
最大完了長は16kトークン、初期最大制限は30相互作用ターンに設定されています。

評価プロトコルは2つのモードで実施されました。
標準評価では、訓練時と同じ難易度レベルのドメイン内タスクで評価し、
困難評価では、堅牢性と汎化をテストするため難易度を上げたタスクで評価しています。

### 3.2 主要な結果
**標準タスクでの性能**において、WMActは全てのベースライン手法と比較して優れた結果を達成しました。
具体的には、SokobanでWMActは78.57の成功率、Mazeで88.14、Taxiで62.16を達成しています。
これらのスコアは、PPO-EntirePlanと標準マルチターンPPOアプローチの両方を大幅に上回っています。
注目すべきは、WMActがGPT-4oやClaude 4.5 Sonnetなどの大型商用モデルの性能さえも上回っていることです。

**困難タスクでの汎化**において、WMActは複雑性が増したより困難なタスクバリアントでも強い性能を維持しています。
Sokoban Hard-1で52.68、Sokoban Hard-2で49.90、Maze Hardで50.59を達成し、
これらの困難なタスクで大幅な性能低下を示すPPO-EntirePlanとPPO-Interactiveを大きく上回っています。
結果は、WMActがマルチターン相互作用を通じて環境動態を効果的に内在化し、
訓練分布に過適合することなく未見の複雑なシナリオに汎化できることを示しています。

**効率的な世界モデル推論**の証拠として、訓練精度カーブは重要な傾向を示しています。
WMActの単一ターン性能は、PPO-EntirePlanに匹敵するだけでなく、
全ての環境において自身のマルチターン訓練性能に段階的に近づいています。
この収束は、相互作用を通じて獲得された知識が効果的に圧縮・内在化され、
環境フィードバックなしでも堅牢な性能を可能にしていることの強い証拠です。

**一般ベンチマークでの性能**では、Sokoban環境で訓練されたWMActモデルを
幅広い一般ベンチマークで評価しました。
WMAct-SokobanモデルはQwen3-8B-Ownモデルに対して包括的な改善を示し、
複雑な推論において顕著な向上を示しています。
HMMT25で5.05ポイント、GPQA-Diamondで2.24ポイントの大幅な増加が見られました。
また、LiveBenchでの1.67ポイント向上に示されるように、汎用タスクでも改善が見られています。

### 3.3 既存手法との比較
アブレーション研究により、各コンポーネントの効果が詳細に分析されました。

**各コンポーネントの効果**では、相互作用訓練PPO-Interactiveの導入が
非相互作用ベースラインに対して大幅な性能向上をもたらし、特に困難なタスクバリアントで顕著でした。
これはマルチターン相互作用の基本的な利益を実証しています。
提案された効果的アクション報酬スケーリングの追加により結果がさらに大幅に向上し、
Hard-2タスクで最も顕著な向上（41.26から48.05）が観察されました。
これは報酬メカニズムが冗長アクションにペナルティを課すことで、より効率的な探索を成功裏に促進していることを示しています。

**ステップペナルティと頻度アニーリングの比較**では、頻度アニーリングが標準レベルで78.57の成功率を達成し、
ステップペナルティを6.14上回り、より困難なレベルでも汎化を維持しています。
一定の負の報酬を課すことで経路効率を促進するよう設計されたステップペナルティは、
エージェントを近視眼的戦略に向かわせ、グローバル性能を阻害します。
対照的に、エージェントの環境との相互作用頻度を調整する頻度アニーリングは、
訓練初期により徹底的な探索を促進し、環境動態の内在化を促進することで、
効率的な世界モデル推論を発達させるより効果的な経路を提供します。

**モデル事前知識の効果**について、Qwen2.5-instruct-7Bの訓練動態は重要な観察を提供しています。
Qwen2.5-instruct-7Bモデルは、より高度なモデルで見られるような
マルチターン相互作用が改善された単一ターン性能を駆動するという同じ傾向を示していません。
これは、反射、自己修正、戦略的先見性などの高度な推論パターンの重要性を示唆しています。
これらのパターンは、特定のフィードバックから一般的な原則を抽象化し、
内部状態を有意味に更新し、環境動態の圧縮された表現を形成する能力をモデルに提供するため、
内在化にとって重要です。

## 4. 実用性評価
### 4.1 実装の容易性
WMActは標準的な強化学習フレームワーク（オンポリシーPPO）をベースとしており、
既存のRL訓練パイプラインとの統合が比較的容易です。
報酬リスケーリングメカニズムは環境によって提供される状態変化に基づいているため、
追加的な複雑な報酬エンジニアリングを必要とせず実装できます。

相互作用頻度アニーリング戦略も比較的シンプルな統計的手法（平均ターン数と最大ターン数の計算）に基づいており、
実装の複雑性は低く抑えられています。
ただし、環境固有のパラメータ調整（τの値など）が必要であり、
新しい環境に適用する際には慎重な調整が求められます。

### 4.2 計算効率
WMActは訓練初期により多くの環境相互作用を必要とするものの、
アニーリング戦略により段階的に相互作用数を削減していくため、長期的には効率的です。
最終的に単一ターンで解決できる能力を獲得することで、推論時の計算コストは大幅に削減されます。

実験結果によると、最終的にマルチターン相互作用が必要だったタスクを単一ターンで解決できるため、
実運用時の応答時間とリソース消費量は従来のマルチターンアプローチと比較して大幅に改善されます。
ただし、訓練フェーズでは環境シミュレータとの頻繁な相互作用が必要であり、
ある程度の計算リソースが必要となります。

### 4.3 応用可能性
WMActの手法は、逐次意思決定と環境相互作用を必要とする様々な分野に応用可能です。

**自律ロボティクス分野**では、ロボットが複雑な環境でのナビゲーションや操作タスクを学習する際に、
WMActのアプローチを適用できます。特に、環境の不確実性が高い状況での適応的行動学習に有効です。

**ゲームAI分野**では、リアルタイム戦略ゲームや複雑なパズルゲームにおいて、
エージェントが効率的な戦略を学習するために活用できます。
実験で示されたSokobanの成功は、パズル系ゲームへの直接的な応用可能性を示しています。

**自動運転分野**では、動的な交通環境での意思決定において、
安全性を確保しながら効率的な経路計画と障害物回避を学習するために応用できます。

**金融取引分野**では、市場の動的変化に対応した取引戦略の学習において、
リスク管理を考慮した効率的な意思決定プロセスの構築に活用できます。

**対話システム分野**では、ユーザとの複数回にわたる対話を通じて、
より効果的なコミュニケーション戦略を学習し、最終的に簡潔で効果的な応答を生成できるシステムの開発に応用できます。

## 5. まとめと所感
### 5.1 論文の意義
本研究は、LLMエージェントの世界モデル推論における根本的な課題に対して、
革新的で実践的なアプローチを提示した点で大きな意義があります。
特に、「考えながら行動する」という自然な学習パラダイムを形式化し、
効率的な実装方法を示したことは、この分野の発展に重要な貢献となります。

従来の構造化された推論パターンの制約から脱却し、
モデルが相互作用を通じて自然に推論能力を獲得できるフレームワークを提供したことは、
AIエージェントの自律性向上において画期的な進歩です。
また、マルチターンで学習した知識を最終的に単一ターンで実行できるという能力転移の実現は、
実用的な観点からも非常に価値の高い成果です。

報酬リスケーリングと相互作用頻度アニーリングという2つのメカニズムは、
シンプルでありながら効果的であり、他の研究者が容易に再現・応用できる手法となっています。
包括的な実験評価と詳細なアブレーション研究により、手法の有効性が十分に検証されている点も評価できます。

### 5.2 今後の展望
本研究で指摘された「認知行動の重要性」は、今後の研究方向性を示す重要な洞察です。
反射や自己修正などの高度な推論パターンを持たないモデルでは、
相互作用から効果的に学習できないという発見は、
ベースモデルの能力とエージェント学習の関係について重要な示唆を提供しています。

今後の研究では、より多様で複雑な環境での評価や、
異なるドメイン間での転移学習能力の検証が期待されます。
また、実世界環境での応用において、安全性や信頼性を確保するための追加的なメカニズムの開発も重要な課題となるでしょう。

相互作用頻度アニーリングの最適化についても、より適応的で環境固有の調整方法の開発が望まれます。
現在のτパラメータのような固定的な設定ではなく、
モデルの学習進捗や環境の複雑性に応じて動的に調整するより高度なアニーリング戦略の研究が有望です。

また、本手法のスケーラビリティについても継続的な検証が必要です。
より大規模で複雑な環境や、より長期的なタスクに対する適用可能性について、
さらなる研究が期待されます。

マルチエージェント環境での協調や競争関係の学習、
部分観測環境での適用など、より実世界に近い条件での研究も重要な発展方向となるでしょう。
これらの拡張により、WMActのアプローチがより広範囲の実用的アプリケーションに適用可能となることが期待されます。