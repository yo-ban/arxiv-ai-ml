# Escaping the Verifier: Learning to Reason via Demonstrations

## 基本情報
- arXiv ID: 2511.21667v1 (https://arxiv.org/abs/2511.21667)
- 著者: Locke Cai (Together AI, Massachusetts Institute of Technology), Ivan Provilkov (Together AI)
- 所属: Together AI, Massachusetts Institute of Technology
- 投稿日: 2024年12月01日
- カテゴリ: cs.AI

## 簡単に説明すると

この論文は、Large Language Models（LLM）の推論能力を向上させるための新しい手法を提案しています。手法の名称は「RARO（Relativistic Adversarial Reasoning Optimization）」です。従来の強化学習ベースの推論訓練では、タスクの正解を検証する「検証器（verifier）」が必要でした。しかし、現実の多くのタスクでは検証器が利用できません。この問題を解決するため、RAROは専門家のデモンストレーションデータのみから推論能力を学習します。

具体的には、方策（policy）と相対的批評者（relativistic critic）の間に敵対的な相互作用を設定します。方策は専門家の答えを模倣しようとします。批評者は方策と専門家の答えを比較・区別しようとします。この手法により、検証器なしでも強力な推論能力を獲得できることを実証しています。数学問題（Countdown、DeepMath）や詩作（Poetry Writing）などの多様なタスクで効果を示しています。

## 1. 研究概要

### 1.1 背景と動機

近年のLLMの進歩は推論能力の向上と密接な関係があります。推論により、LLMはユーザークエリへ答える前の中間的な思考ステップを実行できます。候補解の提案や自己修正を通じてより高品質な回答を生成できます。この分野では、数学や競技プログラミングなどの検証可能なタスクでの強化学習が大きな成功を収めています。最近の推論LLMは専門家レベルの性能を達成しています。

しかし、現実世界の多くの重要で困難なタスクでは、検証器が利用できないという根本的な問題があります。分析的な文章作成、オープンエンド研究、財務分析などのタスクでは問題が生じます。明確に定義できない評価基準、許容可能な答えの幅広いバリエーション、その他の実用的制約により、LLMの出力を直接検証できません。

このような環境では、人間のフィードバックからの強化学習（RLHF）が一般的なアプローチです。ただし、デモンストレーションデータを超えた人間の嗜好データの収集には時間とコストがかかります。デモンストレーションデータのみを用いた場合、通常の教師付き学習（SFT）では次トークン予測目標による訓練をします。これは大規模な強化学習訓練が促すような推論行動を十分に誘発しません。

### 1.2 主要な貢献

本研究の主要な貢献として、次のような点があります。

- 逆強化学習の新しい視点から推論モデルを訓練する原理的手法RAROを開発しました
- デモンストレーションデータのみを用いて推論モデルの訓練を可能にしました
- 制御された推論タスクであるCountdownタスクでRAROを評価しました
- 検証なしのベースラインを20ポイント上回り、RLVRの性能に90%まで近づくことを示しました
- DeepMathデータセットを用いた数学問題の一般領域でRAROの推論誘発能力をテストしました
- 検証なしのベースラインを一貫して上回り、RLVRと同様のスケーリング傾向を示しました
- Poetry Writingタスクでの評価により、RAROの優れた性能が検証不可能な領域でも汎化することを示しました

## 2. 提案手法

### 2.1 手法の概要

RAROは、専門家の質問-回答（QA）データセットが与えられた設定で動作します。LLM方策を明示的推論を通じて専門家レベルの答えを生成するように訓練することを目標とします。この設定は、検証可能なタスクが比較的稀である現実を反映しています。一方、検証不可能な多くの領域で専門家デモンストレーションデータが豊富に存在します。

アプローチの核心は、方策と相対的批評者の間に敵対的な相互作用を設定する新しい逆強化学習フレームワークです。方策は専門家の答えを模倣することを学びます。批評者はペアワイズ比較を通じて方策と専門家の答えを区別することを学びます。両者を強化学習で共同訓練することにより、検証器なしでもデモンストレーションのみから強力な推論能力の創発を可能にします。

### 2.2 技術的詳細

手法では最尤推定から報酬勾配への変換から始まります。LLM方策を π_θ(a,z|q) としてパラメータ化します。これは質問 q に条件づけられた答え a と Chain-of-Thought（CoT）推論 z の分布を表現します。専門家デモンストレーションでの標準的な最尤目標には問題があります。CoT推論の組み合わせ的に大きな（実質的に無界の）集合における周辺尤度の計算を要求するためです。計算上実用的ではありません。

この困難を解決するため、逆強化学習の視点を採用します。周辺尤度を直接最大化する代わりに、QAペアに対するパラメータ化された報酬 r_φ(a,q) を学習します。r_φ に対する方策 π_θ(a|q) の最適化が最尤目標をほぼ最大化する「準最適」方策をもたらすようにします。

KL正則化された報酬を最大化する目標の下では、最適方策が以下の閉形式解を持つことが示されます。

π_θ*⁽φ⁾(a|q) = 1/Z_θ*⁽φ⁾(q) · π_ref(a|q) · exp{1/β · r_φ(a,q)}

ここで Z_θ*⁽φ⁾(q) は分配関数です。π_ref は固定参照方策です。β>0 はKL正則化の強度を制御します。

報酬モデルに対する対応する勾配は次のようになります。

∇_φ L(φ) = 1/β(E_(q,a)~p̂_D[∇_φ r_φ(a,q)] - E_q~p̂_q E_a'~π_θ*⁽φ⁾(·|q)[∇_φ r_φ(a',q)])

この勾配は、専門家の答えで r_φ(a,q) を増加させます。方策の答えで減少させることにより、方策を専門家分布に向けて押し進めます。

### 2.3 新規性

本手法の主要な新規性は相対的批評者（Relativistic Critic）の導入にあります。初期の2項分類設定では問題がありました。方策が専門家分布に近づくにつれて分類タスクが困難になります。最適方策では批評者が本質的にランダム推測に退化します。これにより高分散で無情報な勾配を方策に提供するという問題がありました。

相対的な定式化では、批評者は3つ組 (q, a, a*) を取ります。これは1つの方策答えと1つの専門家答えから構成されます。どちらが良いかまたは品質が等しい場合は「tie」を出力します。これにより、方策が最適な場合でも批評者が区別を強制されない退化を解決します。

具体的には、相対的批評者 c_φ は質問 q と2つの候補答え (a⁽¹⁾, a⁽²⁾) を取ります。ラベル ℓ ∈ {1, 2, tie} を返します。1つの専門家答えと1つの方策答えがあると仮定して、以下のように定義します。

批評者の報酬は次のように定義されます。

R_critic(q, a⁽¹⁾, a⁽²⁾) = I[ℓ が専門家] + τ_crit · I[ℓ = tie]。

方策の報酬は次のようになります。

R_policy(q, a⁽¹⁾, a⁽²⁾) = I[ℓ が方策] + τ_pol · I[ℓ = tie]。

ここで τ_crit と τ_pol は「tie」ラベルを処理するための tie 報酬です。非退化性と安定した学習を保証する新しいハイパーパラメータです。

## 3. 実験結果

### 3.1 実験設定

RAROを推論の相補的な側面を探る3つの多様な推論タスクで評価しています。

**Countdown**：答えの検証が答えの生成よりもはるかに簡単な制御された推論タスクです。4つの整数を組み合わせて24を得る24スタイルのバリアントを使用しています。

**DeepMath**：DeepMathデータセット（103k高品質の数学問題）を用いた一般的な数学の推論問題領域です。Countdownと比較して、一般数学領域での答え検証は非常に困難です。しばしば導出の再現を要求します。

**Poetry Writing**：客観的な検証器を許可しない検証不可能なオープンエンド推論タスクのカスタムデータセットです。評価にはGPT-5を判定者として使用しています。分離評価と専門家詩との比較評価の両方を実施しています。

ベースラインには以下が含まれます。教師付き微調整（SFT）、合理化（Rationalization）、反復的な直接の嗜好最適化（Iterative DPO）があります。RL logitベース報酬（RL-Logit）、検証可能なタスクでのRLVRも含みます。

### 3.2 主要な結果

**Countdownタスク**では、RAROはすべての検証器なしベースラインを20ポイント上回りました。特に、RAROはSFTベースラインよりも約15-20ポイント高い性能を示しています。RLVR上限に90%まで近づく結果を達成しました。これは制御された環境でのRAROの推論誘発能力の有効性を示しています。

**DeepMathタスク**では、RAROは検証なしベースラインを一貫して上回りました。RLVRと同様のスケーリング傾向を示しています。特に、より大きなモデルサイズ（1.5B、3B、7B）全体でRAROの性能向上が観察されました。手法のスケーラビリティが実証されました。これは困難な一般推論環境での検証と同程度に困難な生成においてもRAROが有効であることを示しています。

**Poetry Writingタスク**では、RAROはすべてのベースラインを15ポイント以上上回りました。検証不可能な領域での手法の優れた性能を実証しました。GPT-5による判定では、RAROの生成した詩は専門家の詩により近い品質であると評価されています。オープンエンドタスクでの明示的推論の大幅な改善を示しています。

### 3.3 既存手法との比較

RAROと既存手法との比較では、以下の重要な発見がありました。

**SFTとの比較**：標準的な教師付き微調整と比較して、RAROは全タスクで一貫して15-25ポイントの性能向上を示しました。これは、次トークン予測目標だけでは誘発できない推論行動をRAROが効果的に促進することを示しています。

**RL-Logitとの比較**：最近提案されたlogitベース報酬を用いた強化学習手法と比較して、RAROはより安定した訓練を示しました。より良い最終性能も達成しています。これは相対的な批評者設定の優位性を示唆しています。

**RLVRとの比較**：検証可能なタスクで検証器を持つRLVRとの比較では興味深い結果が得られました。RAROはCountdownでRLVRの性能の約90%を達成しました。DeepMathで約85%の性能を達成しています。これは検証器なしでも非常に競争力のある推論能力を獲得できることを示しています。

## 4. 実用性評価

### 4.1 実装の容易性

RAROの実装は比較的容易です。手法は既存の強化学習フレームワーク（特にGRPO）の上に構築されています。追加の複雑な外部コンポーネントを必要としません。共有LLMを方策と批評者の両方に使用することで、メモリ使用量を削減しています。汎化を促進します。データ混合により、方策と批評者のロールアウトを単一バッチで結合できます。訓練ループが簡素化されます。

実装の安定化には、いくつかの実用的改善が含まれています。リプレイバッファによる過去の方策ロールアウトと現在のものの混合があります。過度な長さのフィルタリング、アドバンテージ/長さ正規化の除去などです。これらの最適化はすべて標準的な技術であり、特別な専門知識を必要としません。

### 4.2 計算効率

計算効率の観点から、RAROは方策と批評者の両方を訓練します。そのため、標準的なSFTよりもやや計算コストが高いです。しかし、共有LLMアーキテクチャにより、このオーバーヘッドは大きく軽減されます。また、外部検証器やより複雑な人間のフィードバック収集システムが不要です。そのため、全体的なシステム複雑性は管理可能です。

実験では、Qwen2.5の1.5B、3B、7Bモデルでのスケーリング研究をしました。RAROがより大きなモデルサイズでも効率的にスケールすることが示されました。訓練時間は標準的なRLVR手法と同程度であり、実用的な展開に適しています。

### 4.3 応用可能性

RAROの応用可能性は非常に広いです。手法は検証器が利用できない任意の領域に適用できます。専門家のデモンストレーションデータのみを必要とします。これにより、以下のような多様な領域での応用が期待されます。

**創作的タスク**：詩作、小説執筆、音楽作曲などの芸術的創作において効果が期待されます。明確な正解が存在しないが専門家の作品が豊富にある領域です。

**分析的タスク**：財務分析、市場調査、戦略立案などの業務領域での活用が考えられます。複雑な推論を要求するが客観的評価が困難な領域です。

**研究支援**：文献調査、仮説生成、実験設計などの研究活動での応用が可能です。オープンエンドで探索的な性質を持つ活動です。

手法の柔軟性により、タスク固有の調整を最小限に抑えながら、多様な領域に適応できることが期待されます。

## 5. まとめと所感

### 5.1 論文の意義

この研究は、LLMの推論能力向上において重要な進展を示しています。従来のRLVR手法が検証可能なタスクに限定されていた制約を突破しています。検証器なしでも強力な推論能力を獲得できる原理的手法を提供しています。逆強化学習と相対的批評者の組み合わせは理論的に堅牢です。実験的に効果的であることが実証されています。

特に注目すべきは、手法が多様なタスク（数学からクリエイティブライティングまで）で一貫して優れた性能を示していることです。これは、RAROが推論の根本的な側面を捉えていることを示しています。タスク固有の調整に依存しない汎用的なアプローチです。

理論的貢献も重要です。最尤推定から報酬勾配への変換、および相対的批評者による安定化は、今後の推論学習研究の基礎となり得ます。特に、tie オプションの導入による非退化性の保証は、敵対的訓練における重要な洞察です。

### 5.2 今後の展望

今後の展望として、著者らは以下の方向性を提案しています。

**汎用的な敵対設定**：より多様な領域で訓練を安定化するフレームワークへの拡張が必要です。現在の手法は特定のタスクで優れた性能を示しています。異なる領域間での転移学習や統合的な訓練への拡張は重要な研究課題です。

**サンプル効率の改善**：現在の手法は効果的ですが、データ効率の観点での改善余地があります。特に、限られた専門家デモンストレーションから効率的に学習する手法の開発は実用的に重要です。

**大規模モデルでのスケーリング**：大規模なモデルサイズで手法をスケーリングすることは、実用的価値を高めるために不可欠です。現在の実験は7Bモデルまでですが、より大規模なモデルでの検証が必要です。

**報酬解釈性の向上**：より良い報酬解釈性を可能にする代替的な批評者設定の開発が重要です。手法の透明性と信頼性向上のために必要です。

この研究は、推論LLMの訓練パラダイムを拡張しています。検証不可能な実世界タスクでの推論能力向上への実用的道筋を提供しています。理論的堅牢性と実験的効果の組み合わせにより、今後の推論学習研究の重要な基盤となることが期待されます。