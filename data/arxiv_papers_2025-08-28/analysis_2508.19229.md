# StepWiser: Stepwise Generative Judges for Wiser Reasoning

## 基本情報
- arXiv ID は 2508.19229v1 である (https://arxiv.org/abs/2508.19229)
- 著者は Wei Xiong らである。Wenting Zhao、Weizhe Yuan、Olga Golovneva、Tong Zhang、Jason Weston、Sainbayar Sukhbaatar が共著者として参加している
- 所属は FAIR at Meta, University of Illinois Urbana-Champaign, NYU である
- 投稿日は2025年8月28日である
- カテゴリは cs.AI, cs.CL である

大規模言語モデルが複雑な問題を解く際に、中間推論ステップの論理的妥当性を評価することは重要な課題です。

従来のProcess Reward Models（PRMs）は分類器として機能し説明を提供せず、静的データセットでの学習に限界がありました。本研究では、推論ステップを評価する作業自体を推論タスクとして捉え直し、判定前にthinking tokenを出力するGenerative Judgeを提案しています。

StepWiserは強化学習により訓練され、中間ステップの判定精度向上、ポリシーモデルの訓練時改善、推論時探索の改善を実現します。

## 1. 研究概要
### 1.1 背景と動機
大規模言語モデルが複雑な問題に取り組む際、Chain-of-Thought（CoT）やReActのような多段階の推論戦略に依存しています。しかし、これらの中間推論ステップの論理的妥当性を確保することが重要な研究課題となっています。

Process Reward Models（PRMs）はこのニーズに応えるツールとして登場し、ステップバイステップのフィードバックを提供しています。しかし、現在のアプローチには2つの主要な欠点があります。まずブラックボックスの分類器として機能し、なぜステップが正しいか欠陥があるかを説明しません。また静的データセットでの教師あり学習に依存するため、新しい推論パターンへの汎化が制限されます。

一方で推論モデル自体は優れた性能を得るために強化学習でCoTを生成するよう訓練されています。

### 1.2 主要な貢献
本研究では、中間推論ステップを報酬するために、まずその推論ステップについて推論し、判断を下す前にメタ推論プロセスを実行することを提案しています。このメタ推論プロセス自体もRLで訓練されます。

ステップワイズ・ジェネレーティブ・ジャッジを構築するための全体的な手法には3つのコンポーネントが含まれます。

ベースポリシーモデルに連結した情報的推論チャンクを作成する新しいセルフセグメンテーション技術を開発しました。モンテカルロロールアウトの相対的結果を通じてチャンクにターゲット報酬を割り当てる手法を開発しています。判断推論チェーンと最終報酬判断のRLでのオンライン訓練を実現しています。

## 2. 提案手法
### 2.1 手法の概要
StepWiserは3つの主要コンポーネントから構成されます。まずベースポリシーモデルに連結した情報的な推論チャンクを作成するセルフセグメンテーション技術です。

次にポリシーモデルが生成したチャンクに対して、結果の相対的なロールアウトを比較してバイナリターゲットラベルでアノテーションします。最後にジェネレーティブ・ステップワイズ・ジャッジをGRPOでのオンラインRLで訓練します。

### 2.2 技術的詳細
Chunks-of-Thoughtと呼ばれるセルフセグメンテーション技術では、ステップの定義が重要な課題でした。従来の手法は事前定義されたトークンやダブル改行を区切り文字として使用していましたが、これらは論理的に完結しておらず、独立した単位ではありませんでした。

本研究では、統一した目的、論理的結束、明確な移行という原則に基づいてセグメンテーションを定義しています。モンテカルロ方式でQ値を推定し、チャンク前後で成功率が向上（または低下）した場合にそのチャンクを良い（または悪い）とラベル付けします。

### 2.3 新規性
従来のPRMとの主要な違いは、判断タスクを推論タスク自体として扱うことです。従来のアプローチでは、PRMはブラックボックスの分類器として機能し、スコアやラベルを説明なしに提供していました。

本研究のジェネレーティブジャッジは、最終判定を下す前にその根拠を説明する明示的なCoTを生成します。このアプローチはLLMの固有の推論能力を活用し、推論モデルの標準的な訓練パイプラインを利用できます。

## 3. 実験結果
### 3.1 実験設定
包括的な評価を実施するため、3つの主要な次元で評価しています。まず中間ステップのジャッジの分類精度をProcessBenchで評価しました。

次にジャッジが推論履歴を整理して再サンプリングする新しい推論時探索パラダイムで性能を評価しました。これは元の生成長を維持しながら連続計算を効率的にスケールする方法です。最後に下流モデル訓練のデータ選択での有用性を評価しています。

### 3.2 主要な結果
実験結果によると、RLで訓練されたジェネレーティブステップワイズジャッジは、従来のSFTベースのベースラインや他の既存手法をすべての評価軸で上回っています。メタ推論する能力をRLで訓練したことの効果が重要な要因として明らかになっています。

ProcessBenchにおいて、StepWiserは既存のPRM手法と比較して優れた性能を示しています。特に推論時探索パラダイムでの改善が顕著でした。

### 3.3 既存手法との比較
従来の分別型PRMはクロスエントロピー损失でバイナリラベルを予測するように学習されています。より最近の手法では、事前定義されたトークンを生成するようにLLMをプロンプトしています。

一方、ジェネレーティブCoT推論ジャッジの最近のパラダイムでは、評価自体が推論タスクとして枠組みされています。ジャッジはまず最終判定を出力する前に、根拠を説明する明示的なCoTを生成します。StepWiserはこのアプローチを採用していますが、ステップワイズな監視に焼点を当てていることが特徴です。

## 4. 実用性評価
### 4.1 実装の容易性
既存のLLMアーキテクチャと強化学習フレームワークを基盤とするため、実装は比較的容易です。ただし、論理的一貫性を満たすチャンクセグメンテーション、モンテカルロロールアウト、オンライン強化学習の複雑な組み合わせには専門知識が必要です。

### 4.2 計算効率
モンテカルロロールアウトによる評価とオンライン強化学習により計算コストは高くなりますが、生成される判定の品質向上により全体的な効率は改善されます。推論時の探索パラダイムでは元の生成長を維持しながら計算をスケールできます。

### 4.3 応用可能性
数学領域以外の複雑な推論タスクへの応用が期待されます。法的推論、医学的診断、科学的発見など、段階的な検証が重要な分野で活用できる可能性があります。

## 5. まとめと所感
### 5.1 論文の意義
Process Reward Modelingの分野に重要な貢献をもたらしています。従来のブラックボックス分類器から説明可能なジェネレーティブジャッジへの技術的転換は、LLMの推論能力向上に新たな道筋を示しています。

メタ推論という概念の導入により、推論について推論するという高次の認知プロセスを機械学習に組み込んだ点が画期的です。強化学習による訓練により、静的なデータセットの限界を超えて継続的に改善できる点も重要です。

### 5.2 今後の展望
モンテカルロロールアウトの計算コストを削減するセグメンテーション技術の開発が期待されます。現在の手法はLLMによる後処理に依存していますが、エンドツーエンドで学習可能なセグメンテーション機構の開発により、さらなる性能向上が見込まれます。

他の推論タスクドメインへの拡張も重要な課題です。数学以外の科学的推論、創造的な問題解決、倫理的判断など、より広範囲な推論タスクでの検証により、手法の汎用性を確認できるでしょう。
