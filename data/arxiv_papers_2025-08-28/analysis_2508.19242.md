# Autoregressive Universal Video Segmentation Model

## 基本情報
- arXiv ID: 2508.19242 (https://arxiv.org/abs/2508.19242)
- 著者: Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma
- 所属: NVIDIA, Yonsei University, Carnegie Mellon University, National Taiwan University
- 投稿日: 2025年8月28日
- カテゴリ: cs.CV, cs.AI

## 簡単に説明すると
この論文は動画セグメンテーションを言語モデリングのように扱う新しい手法「AUSM」を提案します。従来は用途別に分かれていた2つの手法を単一アーキテクチャで統合します。言語モデルの自己回帰手法を動画に適用し、次フレームのマスクを予測します。長時間動画でも一定メモリで処理可能です。

## 1. 研究概要
### 1.1 背景と動機
動画セグメンテーションの分野では、現在2つの主要なパラダイムが存在します。1つはプロンプト付き動画セグメンテーション（VOS）で、初期のマスクやボックスを与えて特定のオブジェクトを追跡するものです。もう1つはプロンプトなし動画セグメンテーション（VIS/VPS）で、予め定義されたカテゴリの全オブジェクトを自動的に検出・追跡します。

従来の手法はタスク固有のアーキテクチャに依存しており、統合的な解決策が不足していました。一方、言語処理分野では、大規模言語モデル（LLM）が単一の自己回帰アーキテクチャで多様なタスクを統合的に処理できることが示されています。本研究は、この言語モデリングの成功を動画セグメンテーションに適用し、「次の単語予測」を「次フレームのマスク予測」として捉える新しい視点を提案します。

### 1.2 主要な貢献
本研究の主要な貢献として次の点が挙げられます。
- プロンプト付きとプロンプトなしの動画セグメンテーションを統合する自己回帰的な数式化の提案
- 任意長の動画を一定メモリで処理できるメモリ効率性の高いアーキテクチャの設計
- 言語モデルのような並列学習を可能にする学習フレームワークの開発
- 7つのベンチマークで既存の統合的手法を上回る性能の実現

## 2. 提案手法
### 2.1 手法の概要
AUSMは動画セグメンテーションを自己回帰的な系列モデリング問題として定式化します。言語モデルが P(y_{1:T}) = ∏_{t=1}^T P(y_t | y_{<t}) として各トークンを前のトークンに条件付けして生成するように、AUSMは P(y_{1:T} | I_{1:T}) = ∏_{t=1}^T P(y_t | y_0, y_{<t}, I_{≤t}) として各フレームのセグメンテーションを生成します。

主要な構成要素は以下の通りです：
1. **History Marker**: 過去のマスク情報を空間的特徴マップに変換し、詳細な情報を保持
2. **History Compressor**: Mamba（状態空間モデル）を使用して時系列情報を一定サイズの状態に圧縮
3. **History Decoder**: 現在フレームの特徴と圧縮された過去情報を統合
4. **Pixel Decoder**: Transformerベースのデコーダでマスク予測を出力

### 2.2 技術的詳細
AUSMはオブジェクトクエリプール V ∈ R^{N^det × D} と、追跡済みインスタンスを管理するバッファIDベクトル B ∈ R^{N^id × D} を維持します。推論時には、割り当て済みIDベクトル集合 A と過去のマスク予測集合 M を管理し、一対一の対応関係を保持します。

History Markerは、過去のマスク情報を空間特徴に変換する際に、Token Markを活用してインスタンスマスクを空間特徴マップに融合：
S_t[h,w,:] = (∑_{i=1}^{|A_{t-1}|} M_{t-1}^i[h,w] · A_{t-1}^i) / (ε + ∑_{i=1}^{|A_{t-1}|} M_{t-1}^i[h,w])

History CompressorはMamba、Self-Attention、FFNの3層構造で、時間次元をMambaで、空間次元をSelf-Attentionで処理します。この設計により、フレーム数に関係なく一定メモリで長時間動画を処理できます。

### 2.3 新規性
従来手法との主な違いは以下の点です：
1. **統合的定式化**: 異なるタスクを単一の自己回帰フレームワークで統一
2. **一定メモリ処理**: 明示的なFIFOメモリバッファを使わず、状態空間モデルで効率的に長期記憶を管理
3. **並列学習**: Teacher Forcingを活用し、従来のフレームバイフレーム学習と異なり、時間軸での並列学習を実現
4. **詳細情報保持**: インスタンスをベクトルに圧縮せず、Token Markにより空間的詳細を維持

## 3. 実験結果
### 3.1 実験設定
**学習データセット**: COCO、DAVIS 2017、MOSE、SA-V、YouTube-VIS 2019・2021、OVISを混合して使用

**評価ベンチマーク**: 
- プロンプト付きタスク: DAVIS 2017、YouTube-VOS 2018・2019、MOSEでJ&F（領域類似度と輪郭精度の平均）、YouTube-VOSではG指標
- プロンプトなしタスク: YouTube-VIS 2019・2021、OVISでAverage Precision（AP）

**学習戦略**: 3段階の学習を採用
- Stage 1: COCO疑似動画（3フレーム）での事前学習
- Stage 2: 混合データセットでの5フレーム学習
- Stage 3: 16フレームでの長期適応（backbone凍結、時間モジュールのみ更新）

### 3.2 主要な結果
**プロンプト付き動画セグメンテーション**:
- DAVIS 2017: J&F = 84.9（Swin-B）
- YouTube-VOS 2018: G = 83.7
- YouTube-VOS 2019: G = 82.4  
- MOSE: J&F = 67.8

**プロンプトなし動画セグメンテーション**:
- YouTube-VIS 2019: AP = 59.3
- YouTube-VIS 2021: AP = 51.8
- OVIS: AP = 46.5（統合手法中最高性能）

**学習効率**: 16フレーム系列で従来の逐次学習に対して2.5倍の高速化を実現

### 3.3 既存手法との比較
統合手法として、UniVS（Swin-L）に対してSwin-Bバックボーンにも関わらずYouTube-VOS 2018で+8.7ポイントの改善を達成しました。特に重いオクルージョンと長期インタラクションが特徴的なOVISデータセットで統合手法中最高性能を達成し、自己回帰定式化の有効性を実証しています。

専用手法と比較すると、プロンプト付きタスクではSAM2には及ばないものの、統合性を保ちながら競争力のある性能を示しています。プロンプトなしタスクでは専用手法に匹敵する性能を実現し、アーキテクチャの汎用性の高さを証明しています。

## 4. 実用性評価
### 4.1 実装の容易性
**高**: TransformerとMambaという既存の確立された構成要素を組み合わせており、実装は比較的容易です。PyTorchエコシステムの標準的なモジュールを活用でき、Teacher Forcingによる並列学習も標準的な技術です。ただし、History MarkerのToken Mark実装やMambaの状態管理には専門知識が必要です。

### 4.2 計算効率
**優秀**: 主要な利点の一つが計算効率の向上です。従来の逐次学習と比較して16フレーム系列で2.5倍の学習高速化を実現し、より長い系列では更なる改善が期待されます。推論時も明示的なメモリバッファを使用せず一定メモリで動作するため、長時間動画での効率性が高く、任意長の動画ストリームに対応できます。

### 4.3 応用可能性
**幅広い**: 単一モデルでプロンプト付きとプロンプトなしの両方のタスクに対応できるため、実用的な価値が高いです。論文では以下の応用可能性も言及されています：
- オブジェクト追跡（バウンディングボックスをマスクに変換）
- スクリブルや点などの他のプロンプト形式への対応
- 参照動画オブジェクトセグメンテーション（テキストプロンプト）

自動運転、映像編集、監視システム、医療画像解析など多様な分野での活用が期待できます。

## 5. まとめと所感
### 5.1 論文の意義
この論文は動画セグメンテーション分野に言語モデリングの成功パラダイムを適用した画期的な研究です。分野を統合する理論的フレームワークを提供し、実用的な性能向上も同時に実現しています。

特に重要な点は以下です：
1. **パラダイムシフト**: 従来のタスク別アーキテクチャから統合的自己回帰アプローチへの転換
2. **効率性の向上**: 並列学習により学習効率を大幅に改善
3. **スケーラビリティ**: 一定メモリでの長時間動画処理を実現
4. **実証的成功**: 理論だけでなく7つのベンチマークで実際の性能向上を実証

言語処理と視覚処理の融合という近年のトレンドを動画理解分野に適用した先駆的な研究として、学術的価値が非常に高いと評価できます。

### 5.2 今後の展望
**短期的改善点**:
- より細かい特徴（stride 4）の活用によるVOS性能向上
- 動画専用バックボーンの開発
- より長い学習系列（32フレーム以上）での性能評価

**長期的発展性**:
- 他の動画理解タスク（動画キャプション、行動認識など）への統合
- マルチモーダル（音声、テキスト）情報の組み込み
- リアルタイム処理への最適化
- 大規模動画データでの基盤モデル構築

特に言語モデルの急速な発展（長文脈処理、推論時計算スケーリングなど）を動画分野に適用する研究の基盤として、今後の影響は非常に大きいと予想されます。
