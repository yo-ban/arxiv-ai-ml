# Learning to Reason for Factuality

## 基本情報
- arXiv ID: 2508.05618v1 (https://arxiv.org/abs/2508.05618)
- 著者: Xilun Chen他7名
- 所属: FAIR at Meta, University of Washington
- 投稿日: 2025年8月11日
- カテゴリ: cs.CL, cs.AI

## 簡単に説明すると

この論文は、推論型大規模言語モデル（R-LLM）の事実性を向上させるための新しい学習手法を提案しています。

推論型LLM（OpenAI-o1やDeepSeek-R1など）は、複雑な推論タスクで優れた性能を示しますが、事実性の面では通常のLLMよりも多くの幻覚（ハルシネーション）を生成してしまう問題があります。

著者らは、事実の精度、回答の詳細度、関連性を同時に考慮する新しい報酬関数を設計し、オンライン強化学習（RL）を用いて事実的推論を学習する手法を開発しました。その結果、ハルシネーション率を平均23.1ポイント削減し、回答の詳細度を23%向上させることに成功しています。

## 1. 研究概要
### 1.1 背景と動機

推論型大規模言語モデル（R-LLM）は、Long Chain-of-Thought（長い思考連鎖）プロセスを用いることで、数学やコーディングなどの複雑な推論タスクにおいて大幅な性能向上を達成しています。しかし、事実性の面では大きな課題があります。

著者らがDeepSeek-R1とQwQ-32Bという2つの人気のR-LLMを6つの長文事実性データセットでベンチマークしたところ、通常のモデル（DeepSeek-V3とQwen-2.5-32B）と比較して、ハルシネーション率がそれぞれ平均10ポイントと13ポイント高いことが判明しました。

この原因として、既存のR-LLMの強化学習訓練が主に数学やコーディングなどの論理的推論タスクを対象としており、事実性などの他の重要な特性を見落としていることが挙げられます。そこで著者らは「推論戦略を学習してLLMの事実性を向上させることは可能か？」という研究課題を設定しました。

### 1.2 主要な貢献

本研究の主要な貢献は以下のような点があります。

- **長文事実性のための初のオンラインRL手法**: 事実の精度、回答の詳細度、関連性を同時に考慮する新しい報酬関数を提案
- **スケーラブルなVeriScore実装**: 最大30倍の高速化を実現し、オンラインRLでのリアルタイム報酬計算を可能に
- **包括的な評価**: 6つの長文事実性ベンチマークで評価し、ハルシネーション率を平均23.1ポイント削減
- **詳細度の向上**: 事実的記述の数を23%増加させながら、全体的な有用性を維持

## 2. 提案手法
### 2.1 手法の概要

提案手法は、オフライン訓練とオンライン訓練の2段階から構成されています。

**オフライン訓練**:
1. 教師あり微調整（SFT）: Long CoT形式で推論を行うようモデルを訓練
2. Direct Preference Optimization（DPO）: 事実性に基づく選好ペアで学習

**オンライン訓練**:
Group Relative Policy Optimization（GRPO）を用いて、新しい報酬関数を最適化します。

訓練プロンプトは、WildChatとLongFactから抽出した実世界の多様なプロンプトを基に、Llama 4を使用して7,000個の合成プロンプトを生成しました。

### 2.2 技術的詳細

**報酬関数の設計**
提案する報酬関数は3つの要素から構成されています。

$$\mathcal{R}(y|x) = \mathcal{R}_{fact} + \lambda \cdot \mathcal{R}_{dtl} + \mu \cdot \mathcal{R}_{rel}$$

ここで、
- $\mathcal{R}_{fact} = F/(T+1)$: 事実の精度（Fは事実的主張の数、Tは総主張数）
- $\mathcal{R}_{dtl} = \log(1+F)$: 回答の詳細度（対数で割引）
- $\mathcal{R}_{rel} = \mathbb{1}(y_{ans} \succ y_{ref})$: LLM-as-Judgeによる関連性評価

この設計により、以下の問題を回避します。
1. 短い回答による報酬ハッキング（精度を上げるために詳細を省く）
2. 関連性の低い事実の羅列による報酬ハッキング
3. 極端な例：すべての質問に同じWikipedia記事を暗唱する

**スケーラブルなVeriScore実装**
オリジナルのVeriScoreは1つの応答の検証に数分かかるため、オンラインRLには不適切でした。著者らは以下の最適化を実施しました。
- 文ごとの逐次処理をバッチ処理に変更
- 非同期API呼び出しの活用
- Matrixライブラリを用いた並列LLM推論

結果として、平均5秒で応答を検証可能になり（元は2分）、約30倍の高速化を達成しました。

### 2.3 新規性

既存手法との主な違いは以下の通りです。

1. **オンラインRL vs オフラインRL**: 従来の事実性向上研究はDPOなどのオフラインRLに限定されていたが、本研究は初めてオンラインRLを適用
2. **包括的な報酬設計**: 事実性だけでなく、詳細度と関連性も考慮することで、実用的な応答を生成
3. **推論型LLMへの焦点**: 通常のLLMではなく、Long CoTを用いる推論型LLMの事実性改善に特化
4. **スケーラブルな実装**: 実用的なオンラインRL訓練を可能にする高速な事実性評価

## 3. 実験結果
### 3.1 実験設定

**評価データセット**:
- LongFact: 複数段落の回答を必要とする250の質問
- FAVA: 200の情報探索クエリ（141問を使用）
- AlpacaFact: 241の事実探索指示
- Biography: 183の人物伝記に関する質問
- FactBench-Hard: 532の挑戦的な質問
- FACTORY: 最先端LLMでも約40%の精度しか達成できない新しいベンチマーク

**評価指標**:
- 事実の精度（Precision）: 支持される主張数 / 総主張数
- 詳細度（Detail Level）: 支持される事実の数
- 有用性（Win Rate）: GPT-4oを審査員として基準モデルとの比較

**基準モデル**: Llama-3.1-8B-Instruct

### 3.2 主要な結果

**既存推論モデルの評価**:
- DeepSeek-R1: DeepSeek-V3と比較して平均10ポイント高いハルシネーション率
- QwQ-32B: Qwen-2.5-32Bと比較して平均13ポイント高いハルシネーション率

**提案手法の結果**:
- **SFT**: 精度を平均10.9ポイント向上させたが、詳細度を25%以上削減
- **SFT + DPO**: 精度を平均22.8ポイント向上させたが、勝率が37.8%に低下（品質劣化）
- **SFT + GRPO（提案手法）**: 
  - 精度を平均23.1ポイント向上（45.0% → 68.1%）
  - 詳細度を23%向上（19.4 → 23.9）
  - 勝率54.4%を維持（基準モデルと同等以上の品質）

### 3.3 既存手法との比較

**オフラインRL（DPO）との比較**:
- DPOは高い精度を達成するが、応答の品質が大幅に低下（勝率37.8%）
- 提案手法は精度と品質の両方を向上（勝率54.4%）

**非推論モデルとの比較**:
提案手法により訓練されたLlama-3.1-8Bは、より大きな非推論モデルに匹敵する事実性を達成しました。
- 平均精度68.1%は、DeepSeek-V3（55.1%）やQwen-2.5-32B（51.4%）を上回る

## 4. 実用性評価
### 4.1 実装の容易性

実装は比較的複雑ですが、著者らは以下のツールを提供しています。
- fairseq2ライブラリを用いた訓練フレームワーク
- スケーラブルなVeriScore実装
- Matrix推論ライブラリとの統合

ただし、以下の点で実装に課題があります。
- 複数のGPUクラスタの設定が必要（訓練用と推論用）
- Google Search APIの設定とコスト
- LLM-as-Judge用の追加モデル

### 4.2 計算効率

計算リソースの要求は高いです。
- SFT: 8 H100 GPU、1エポック
- DPO: 16 H100 GPU、1エポック
- GRPO: 32 H100（訓練）+ 8 H100（推論）+ 32 H100（VeriScore用）

VeriScoreの高速化により、オンラインRLが現実的になりましたが、依然として大規模な計算リソースが必要です。

### 4.3 応用可能性

応用可能性は広範囲に及びます。
- **情報検索システム**: 事実に基づく正確な回答生成
- **教育アプリケーション**: 信頼できる学習コンテンツの生成
- **医療・法律分野**: 高い事実性が要求される専門分野での活用
- **企業向けチャットボット**: 正確な製品情報やサポート情報の提供

## 5. まとめと所感
### 5.1 論文の意義

この研究は、推論型LLMの事実性向上という重要な課題に取り組んだ先駆的な研究です。

特に印象的なのは、単純に事実の精度を最適化するだけでは実用的でないという洞察です。短く簡潔な回答や、関連性の低い事実の羅列といった「報酬ハッキング」の問題を、包括的な報酬設計により解決している点は実践的に重要です。

また、VeriScoreの30倍高速化により、これまで不可能だったオンラインRLでの事実性学習を実現した技術的貢献も大きいです。評価結果も説得力があり、ハルシネーション率を大幅に削減しながら、回答の詳細度と有用性を維持または向上させています。

### 5.2 今後の展望

著者らは論文で以下の制限事項を挙げています。
- 現在の手法は英語のみに対応
- 計算コストが依然として高い
- 事実性評価の完全な自動化にはまだ課題がある

今後の発展として期待される方向性は以下の通りです。
1. **多言語対応**: 英語以外の言語での事実性向上
2. **効率化**: より少ない計算リソースで同等の性能を達成
3. **他のタスクへの拡張**: 事実性以外の属性（安全性、創造性など）への適用
4. **より大規模なモデルへの適用**: 70B以上のモデルでの検証
5. **実世界での展開**: 商用システムへの統合と評価

Learning to Reason for Factualityは、推論型LLMの実用化に向けた重要な一歩であり、今後のLLM研究における事実性の重要性を示す画期的な研究といえるでしょう。