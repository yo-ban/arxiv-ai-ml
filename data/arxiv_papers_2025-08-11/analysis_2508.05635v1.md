# Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation

## 基本情報
- arXiv ID: 2508.05635v1 (https://arxiv.org/abs/2508.05635)
- 著者: Yue Liao他14名
- 所属: AgiBot Genie Team, NUS LV-Lab, BUAA
- 投稿日: 2025年8月11日
- カテゴリ: cs.CV, cs.LG

## 簡単に説明すると

Genie Envisionerは、ロボット操作のための統一ワールド基盤プラットフォームです。このシステムは、ビデオ生成技術を中核として、ロボットの認識、ポリシー学習、評価を単一のフレームワークに統合しています。

主要なコンポーネントは以下の3つです。
1. GE-Base: 言語指示に基づいてロボット操作の将来のビデオを生成する大規模ビデオ拡散モデル
2. GE-Act: ビデオの潜在表現から実行可能なアクション軌道に変換する軽量なワールドアクションモデル
3. GE-Sim: アクション条件付きビデオ生成によるクローズドループシミュレーター

プロジェクトのWebサイト: https://genie-envisioner.github.io

このプラットフォームの最も注目すべき特徴は、わずか1時間の遠隔操作データで新しいロボット実施形態に適応できる点です。論文では、双腕ロボットでの実験を通じて、布の折りたたみや箱の組み立てなど、複雑な操作タスクでの優れた性能を実証しています。

## 1. 研究概要
### 1.1 背景と動機

現在のロボット操作システムは、データ収集、トレーニング、評価の各段階が分断されており、それぞれ専用のインフラストラクチャ、手動キュレーション、タスク固有の調整が必要となっています。この断片化により、イテレーションが遅くなり、失敗モードが不明瞭になり、大規模での再現性が阻害されています。

著者らはAlan Kayの言葉を引用し、新しいアプローチを提案しています。ロボットの認識、ポリシー学習、評価を単一のビデオ生成モデルに統合します。

この研究の中核的な洞察は以下の通りです。従来のVLAモデルは言語中心の表現空間を使用します。一方、ビデオ生成を通じた視覚中心の空間構築により、環境の正確なモデリングが可能です。

### 1.2 主要な貢献

本研究の主要な貢献は以下のような点があります。

- 統一プラットフォームの構築: ロボット操作のためのポリシー学習、評価、シミュレーションを単一のビデオ生成フレームワークに統合。
- GE-Base: 約3,000時間のビデオデータと100万エピソードのロボット操作データで訓練された、指示条件付きマルチビュービデオ拡散モデル。
- GE-Act: 視覚潜在特徴を細粒度かつ低遅延のモーターコマンドに変換する軽量並列フローマッチングアクションモデル。
- GE-Sim: ビデオベースのワールドシミュレーターとして機能し、分散クラスター並列化により1時間あたり数千エピソードの評価を可能に。
- EWMBench: 視覚的忠実度、物理的一貫性、指示-アクションアラインメントを測定する標準化されたベンチマークスイート。

## 2. 提案手法
### 2.1 手法の概要

Genie Envisionerは、ビデオ生成を中核とした統一プラットフォームとして設計されています。システムの中心にはGE-Baseがあります。これは空間的、時間的、意味的なロボット相互作用のダイナミクスを構造化された潜在空間で捉える大規模な指示条件付きビデオ拡散モデルです。

このプラットフォームは、ロボットの視覚観察に基づいて動作します。高レベルの指示に従った操作行動の時間的進化を捉えるビデオチャンクを自己回帰的に生成します。
ロボットドメイン適応事前学習を活用することで、GE-Baseは言語指示から具体化された視覚空間へのマッピングを確立します。
これにより、実世界の相互作用の空間的、時間的、意味的規則性をモデル化し、ロボット操作の本質を捉えます。

### 2.2 技術的詳細

**GE-Baseアーキテクチャ**

GE-Baseは、ロボット操作データの逐次的性質に合わせて、出力を離散的なビデオチャンクに分割する自己回帰ビデオ生成フレームワークを採用しています。
各ステップで、ワールド基盤モデルは次のチャンクを生成します。
- 初期視覚観察
- 言語指示
- 長期履歴フレームからスパースサンプリングされたスパースメモリ

モデルは、頭部搭載ビュー1つと手首搭載ビュー2つの3つのオンボードカメラからの時間的に同期された入力を活用するマルチビュー、言語および画像条件付き生成フレームワークに拡張されています。

**GE-Actアーキテクチャ**

GE-Actは160Mパラメータの自己回帰アクションデコーダーで、マルチモーダル潜在表現を時間的に構造化されたアクションポリシーに変換します。
このアーキテクチャは以下のような特徴を持ちます。
- GE-Baseの視覚バックボーンと並列に動作
- 視覚DiTブロックの深さを反映しながら、計算効率のために隠れ次元を削減
- クロスアテンション機構を通じて視覚潜在特徴を統合

**非同期推論**：
視覚処理とモーター制御の時間的ギャップを橋渡しするため、Slow-Fast非同期推論モードを導入。ビデオDiTは5Hzで動作し、アクションモデルは30Hzで実行され、1:6の時間解像度比を実現しています。

### 2.3 新規性

既存手法との主な違いは以下の通りです。

1. **視覚中心のアプローチ**: 従来のVLAモデルが言語中心の表現空間を使用するのに対し、GEは生成ビデオモデリングを通じて視覚中心の空間を構築
2. **統合プラットフォーム**: 学習、シミュレーション、評価を単一のコヒーレントなプラットフォーム内で実現
3. **少量データでの適応**: わずか1時間の遠隔操作データで新しいロボット実施形態への効果的な転移を実現
4. **マルチビュー合成と自己回帰デコーディング**: メモリメカニズムを備え、時間と空間の一貫性とタスク関連性を向上

## 3. 実験結果
### 3.1 実験設定

実験は、AgiBot G1プラットフォームで実施されました。また、クロスエンボディメント評価のためのFrankaアームとAgilex Cobot Magicシステムでも実施されました。評価タスクは以下の5つです。

1. サンドイッチを作る: 複数オブジェクトの調整と手続き的タスク実行をテスト
2. お茶を注ぐ: 流体操作における細粒度モーション制御と器用さを評価
3. テーブルを拭く: 軌道の安定性とコンプライアント力の適用を評価
4. 電子レンジで食品を温める: 関節オブジェクトと多段階インターフェース操作を評価
5. 洗濯洗剤を梱包する: 動的知覚とモーショントラッキングを評価

評価指標として、ステップワイズ成功率（SR）とエンドツーエンド成功率（E2E）の2つを使用しています。

### 3.2 主要な結果

**AgiBot G1での性能**:
GE-ActはUniVLAおよびGR00T N1と比較して、すべてのタスクで一貫して優れた性能を示しました。特に、高速モードでは標準モードと同等以上の性能を達成しました。「コンベアから洗剤を梱包する」などの遅延に敏感なタスクでは、性能差が顕著でした。

**クロスエンボディメント性能**:
たとえば、以下のような結果が得られました。
- Agilex Cobot Magicでの布折りたたみと箱折りたたみタスクで、GE-ActはGR00T N1、π0、UniVLAを明確に上回る性能を示しました
- Dual Frankaプラットフォームでも同様の傾向が観察され、わずか1時間の適応データで優れた性能を達成

**推論速度**:
GE-Actは、オンボードNVIDIA RTX 4090 GPUで54ステップのトルク軌道を200ms以内に生成しました。これによりリアルタイム推論能力を実証しました。

### 3.3 既存手法との比較

EWMBenchベンチマークによる評価では、GE-Baseは複数の評価次元でベースラインを一貫して上回りました。
- 時間的アラインメントと動的一貫性で特に優れた性能
- Klingは一般的なビデオ生成タスクで強い性能を示すも、細粒度制御に必要な専門的理解が不足
- Hailuoはゼロショット具体化シナリオで優れているが、カートゥーン調の出力が視覚的リアリズムを損なう
- COSMOSとLTX-Videoは人間の手中心のタスクでは効果的だが、ロボットコンテキストへの適応に苦戦

## 4. 実用性評価
### 4.1 実装の容易性

Genie Envisionerは実装の容易性において以下の利点があります。
- プラグアンドプレイ設計により、既存のロボットシステムへの統合が容易
- 少量のデータ（約1時間の遠隔操作）で新しいロボットへの適応が可能
- 事前学習モデルとベンチマークツールがオープンソース化される予定

### 4.2 計算効率

システムの計算効率は実用的なレベルに達しています。
- リアルタイム推論: 200ms以内で54ステップのアクション生成
- 省コストな訓練: 32個のNVIDIA A100 GPUで約7日間の事前学習
- 非同期推論により、視覚処理(5Hz)とアクション生成(30Hz)を効率的に分離

### 4.3 応用可能性

Genie Envisionerの応用可能性は広範囲に及びます。
- 家庭用タスク: 料理、掃除、整理整頓など
- 産業用途: コンベアベースの動的オブジェクト操作
- 研究開発: 新しいロボット操作アルゴリズムの評価とベンチマーキング
- シミュレーション: 物理シミュレータの代替として高忠実度の視覚的予測を提供

## 5. まとめと所感
### 5.1 論文の意義

この研究は、ロボット操作分野において重要な変革を提示しています。従来の断片化されたアプローチから、ビデオ生成を中核とした統一プラットフォームへの移行は、以下の点で革新的です。

1. **統合的アプローチの実現**: 学習、評価、シミュレーションを単一フレームワークで実現することで、開発サイクルの高速化と一貫性の向上を達成
2. **少量データでの汎化**: わずか1時間のデータで新しいロボットに適応できる能力は、実用化への大きな前進
3. **視覚中心の表現学習**: 言語中心ではなく視覚中心のアプローチにより、より忠実な環境ダイナミクスのモデリングを実現

特に印象的なのは、メモリベースの意思決定能力です。論文で示された例では、ロボットが箱に物を入れて蓋を閉じた後、中身が見えない状態でも正しいスタンプを選択できることが実証されています。これは、タスク関連のメモリを維持する能力を示しており、より複雑な長期的タスクへの応用可能性を示唆しています。

### 5.2 今後の展望

著者らは論文で以下の制限事項を挙げています。
- データソースの多様性: 現在はAgiBot-World-Betaデータセットのみに依存
- 実施形態の範囲: 上半身の卓上操作に限定され、巧緻な手の動きや全身動作は未対応
- 評価方法: プロキシメトリクスと部分的な人間検証に依存

今後の発展として期待される方向性は以下の通りです。
1. **大規模シミュレーションデータの統合**: インターネット規模やシミュレーションベースのデータソースを組み込み
2. **より複雑な実施形態への拡張**: 巧緻な手の協調や全身移動を含む能力を統合
3. **評価プロトコルの改善**: 人間の判断とより密接に整合したスケーラブルな評価手法を開発
4. **実世界展開**: 安全性と信頼性を確保した実環境での大規模な展開

Genie Envisionerは、AGIレベルの操作能力を持つ具体化AIシステムへの意味のある一歩を表しており、今後のロボティクス研究の基盤となる可能性を秘めています。