# On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification

## 基本情報
- arXiv ID: 2508.05629v1 (https://arxiv.org/abs/2508.05629)
- 著者: Yongliang Wu他9名
- 所属: Southeast University, Independent Researcher, UCLA, Shanghai Jiao Tong University, NTU, UC Berkeley, Wuhan University, UC Merced
- 投稿日: 2025年8月11日
- カテゴリ: cs.LG, cs.AI

## 簡単に説明すると

本論文は、大規模言語モデル（LLM）の教師あり微調整（SFT）における汎化性能の限界を強化学習（RL）の観点から理論的に分析し、Dynamic Fine-Tuning（DFT）という新しい手法を提案しています。

SFTの勾配更新を数学的に分析すると、暗黙的に問題のある報酬構造が含まれていることが判明しました。具体的には、エキスパートアクションの確率に反比例する重み付けが行われているため、モデルが低確率のアクションに過度に注目し、汎化性能が制限されています。

DFTは、各トークンの目的関数をそのトークンの確率で動的にリスケーリングするという、わずか1行のコード変更で実装可能な手法です。この単純な修正により、複数のベンチマークにおいて標準的なSFTを大幅に上回る性能を達成しています。

プロジェクトのコード: https://github.com/yongliang-wu/DFT

## 1. 研究概要
### 1.1 背景と動機

教師あり微調整（SFT）は、エキスパートのデモンストレーションデータを用いてモデルを訓練する手法として、LLMのポストトレーニングにおける標準的なアプローチとなっています。実装の容易さとエキスパートのような振る舞いを迅速に獲得できる利点があります。

しかし、SFTは強化学習（RL）手法と比較して汎化性能が限定的であることが知られています。RLは明示的な報酬や検証信号を活用し、多様な戦略を探索することで、より強い汎化性能を達成できます。ただし、RLは計算リソースが大量に必要で、ハイパーパラメータに敏感であり、報酬信号が必要という制約があります。

著者らは「SFT自体を根本的に改善できないか」という問いを提起しています。これは、ネガティブサンプルがなく、報酬モデルや検証モデルが利用できない場合、SFTが唯一の実行可能なアプローチであるため重要です。

### 1.2 主要な貢献

本研究の主要な貢献は以下のような点があります。

- SFTとRLの根本的な違いを数学的に解明し、SFTの勾配更新が特定の暗黙的報酬構造を持つポリシー勾配法の特殊ケースであることを証明
- SFTの暗黙的報酬が極めてスパースで、エキスパートアクションの確率に反比例することを発見
- Dynamic Fine-Tuning（DFT）という理論的に動機付けられた解決策を提案
- 複数のモデルとベンチマークで実験を行い、わずか1行のコード変更で大幅な性能向上を実証
- オフラインRLセッティングでも既存手法を上回る性能を達成

## 2. 提案手法
### 2.1 手法の概要

提案手法のDynamic Fine-Tuning（DFT）は、SFTの暗黙的な報酬構造の問題を解決するために設計されています。

標準的なSFTの損失関数は以下のように表されます。
$$\mathcal{L}_{\text{SFT}}(\theta) = \mathbb{E}_{(x,y^*)\sim\mathcal{D}}[-\log\pi_\theta(y^*|x)]$$

ここで、$(x,y^*)$はエキスパートデモンストレーションのペアです。

著者らは、この勾配を重要度サンプリングを用いてポリシー勾配の形式に変換し、SFTが以下のような暗黙的な報酬構造を持つことを明らかにしました。
- 報酬: $r(x,y) = \mathbf{1}[y=y^*]$（エキスパート軌道と一致する場合のみ1）
- 重要度重み: $w(y|x) = 1/\pi_\theta(y|x)$

この重要度重みがモデルがエキスパート応答に低い確率を割り当てる場合に大きくなり、勾配の分散が無限大になる可能性があることが問題の根源です。

### 2.2 技術的詳細

**Dynamic Fine-Tuningの核心**

DFTは、この歪んだ報酬構造を修正するために、動的に報酬を再重み付けします。具体的には、ポリシー確率の逆数（1/w）を掛けることで補正します。

結果として得られるDFTの損失関数は以下のようになります。
$$\mathcal{L}_{\text{DFT}}(\theta) = \mathbb{E}_{(x,y^*)\sim\mathcal{D}}[\text{sg}(\pi_\theta(y^*|x))\log\pi_\theta(y^*|x)]$$

ここで、sg(・)は勾配停止演算子です。

実際の実装では、トークンレベルで重要度サンプリングを適用します。
$$\mathcal{L}_{\text{DFT}}(\theta) = \mathbb{E}_{(x,y^*)\sim\mathcal{D}}[-\sum_{t=1}^{|y^*|}\text{sg}(\pi_\theta(y^*_t|y^*_{<t},x))\log\pi_\theta(y^*_t|y^*_{<t},x)]$$

この修正により、すべてのエキスパート軌道に対して一様な報酬（1）が割り当てられることになります。これは、正しいサンプルすべてに一様な報酬を割り当てる検証ベースの報酬アプローチ（RLVR）と類似しています。

### 2.3 新規性

既存手法との主な違いは以下の通りです。

1. **理論的基盤**: SFTの限界を強化学習の観点から数学的に解明し、根本的な原因を特定
2. **実装の簡潔性**: わずか1行のコード変更で実装可能
3. **計算効率**: 追加のサンプリングや報酬モデルを必要としない
4. **汎用性**: さまざまなモデルサイズ、タスク、データセットで一貫した改善を実現

## 3. 実験結果
### 3.1 実験設定

実験は主に数学推論タスクで実施されました。

**データセットとモデル**:
- 訓練データ: NuminaMath CoTデータセット（約86万問から10万問をランダムサンプリング）
- 評価モデル: Qwen2.5-Math-1.5B/7B、LLaMA-3.2-3B、LLaMA-3.1-8B、DeepSeekMath-7B-Base
- 評価ベンチマーク: Math500、Minerva Math、Olympiad Bench、AIME 2024、AMC 2023

**訓練設定**:
- オプティマイザ: AdamW（学習率5×10^-5、LLaMA-3.1-8Bのみ2×10^-5）
- バッチサイズ: 256
- 最大入力長: 2048トークン
- 学習率スケジュール: コサイン減衰（ウォームアップ比0.1）

### 3.2 主要な結果

**標準SFTとの比較**:
DFTは評価したすべてのLLMにおいて、標準SFTと比較して一貫して大幅な性能向上を達成しました。

主な結果は以下の通りです。
- Qwen2.5-Math-1.5B: DFTは平均+15.66ポイントの向上（SFTの+2.09に対して5.9倍）
- LLaMA-3.2-3B: DFTは+3.46ポイント（SFTの+2.05に対して1.4倍）
- LLaMA-3.1-8B: DFTは+10.02ポイント（SFTの+5.33に対して1.88倍）
- DeepSeekMath-7B: DFTは+15.51ポイント（SFTの+7.18に対して1.58倍）
- Qwen2.5-Math-7B: DFTは+15.90ポイント（SFTの+2.37に対して3.8倍）

**困難なベンチマークでの性能**:
標準SFTが性能低下を示す困難なベンチマークにおいて、DFTは特に優れた汎化性能を示しました。

たとえば、以下のような結果が得られました。
- Olympiad Bench（Qwen2.5-Math-1.5B）: SFTは15.88→12.63に低下、DFTは27.08に向上
- AIME24（Qwen2.5-Math-7B）: SFTは6.68→2.48に低下、DFTは8.56に向上
- AMC23（Qwen2.5-Math-1.5B）: SFTは19.38→18.75に低下、DFTは38.13に向上

**学習効率**:
DFTは標準SFTと比較して以下の利点を示しました。
- より速い収束（ほとんどのベンチマークで120ステップ以内にピーク性能を達成）
- 初期段階での優れた性能（10-20ステップでSFTの最終精度を上回る）
- 高いサンプル効率（最適な結果に到達するために必要な更新回数が少ない）

### 3.3 既存手法との比較

**Importance-Weighted SFT（iw-SFT）との比較**:
並行して提案されたiw-SFT手法と比較して、DFTはほとんどの設定で優れた性能を示しました。
- LLaMA-3.2-3B: DFTが+2.39ポイント優位
- LLaMA-3.1-8B: DFTが+4.15ポイント優位
- DeepSeekMath-7B: DFTが+3.34ポイント優位

また、iw-SFTは参照モデルを必要とするため計算オーバーヘッドが大きいのに対し、DFTはモデル自身のトークン確率から直接重み付けを導出するため効率的です。

**オフラインRL設定での性能**:
DFTはオフラインRL設定でも優れた性能を示しました。
- 平均スコア35.43を達成（最良のオフライン手法RFTの23.97を+11.46ポイント上回る）
- オンラインRL手法GRPOの32.00も+3.43ポイント上回る
- DPO、PPOなどの既存RL手法をすべて上回る性能

## 4. 実用性評価
### 4.1 実装の容易性

DFTの実装は極めて簡単です。標準的なSFTのコードに対して、損失計算の部分に確率による重み付けを追加するだけで実装できます。具体的には、以下のような1行の変更で実現可能です。

```python
# 標準SFT
loss = -log_probs.mean()

# DFT
loss = -(log_probs * log_probs.exp().detach()).mean()
```

### 4.2 計算効率

DFTは計算効率の面でも優れています。
- 追加の参照モデルが不要（DPOやiw-SFTと異なり）
- 追加のサンプリングが不要（RLメソッドと異なり）
- 標準SFTとほぼ同じ計算コスト
- より速い収束により、実際の訓練時間は短縮される可能性

### 4.3 応用可能性

DFTの応用可能性は広範囲に及びます。
- 様々なモデルサイズ（1.5B〜8B）で効果を確認
- 複数のモデルファミリー（Qwen、LLaMA、DeepSeek）で動作
- 数学推論以外のタスクへの拡張も期待される
- ネガティブサンプルや報酬モデルが利用できない状況で特に有用

## 5. まとめと所感
### 5.1 論文の意義

この研究は、LLMの微調整における重要な理論的洞察と実践的解決策を提供しています。

特に印象的なのは、SFTの限界を数学的に解明し、その根本原因を特定した点です。SFTの勾配更新が暗黙的に問題のある報酬構造を含んでいるという発見は、なぜSFTがRLと比較して汎化性能が劣るのかを明確に説明しています。

また、わずか1行のコード変更で大幅な性能向上を実現できるという実用性の高さも注目に値します。理論的な洞察が直接的に実用的な改善につながる好例といえるでしょう。

トークン確率分布の分析結果も興味深く、標準SFTがすべてのトークンを一様にフィッティングしようとするのに対し、DFTは重要なトークンに焦点を当て、文法的機能を持つトークンの確率を抑制することが示されています。これは人間の学習プロセスにも類似しており、本質的な概念に集中することの重要性を示唆しています。

### 5.2 今後の展望

著者らは論文で以下の制限事項を挙げています。
- 現在の実験は主に数学推論タスクに限定されている
- より大規模なモデル（70B以上）での検証が必要
- 他のドメイン（コード生成、創造的ライティングなど）での効果検証が必要

今後の発展として期待される方向性は以下の通りです。
1. **理論的拡張**: DFTの理論的基盤をさらに深化させ、最適な重み付け戦略の探求
2. **応用範囲の拡大**: 数学以外のタスクへの適用と効果検証
3. **大規模モデルへの適用**: より大きなモデルサイズでの性能評価
4. **他の微調整手法との組み合わせ**: DFTと他の手法（LoRA、QLoRAなど）の併用効果の検証

Dynamic Fine-Tuningは、理論と実践を橋渡しする優れた研究成果であり、LLMの微調整における新たな標準となる可能性を秘めています。