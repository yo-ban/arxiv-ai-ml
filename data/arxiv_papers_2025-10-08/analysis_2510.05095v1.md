# From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models

## 基本情報
- arXiv ID: 2510.05095v1 (https://arxiv.org/abs/2510.05095)
- 著者: Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia 
- 所属: The Chinese University of Hong Kong, The University of Hong Kong, The Hong Kong University of Science and Technology
- 投稿日: 2025年10月08日
- カテゴリ: cs.AI, cs.LG

## 簡単に説明すると
この論文は、大規模推論モデル（LRM）を人間の好みに合わせるための新しい手法を提案しています。
従来の好み最適化手法をLRMに適用すると、推論過程の確率的サンプリングによって勾配の分散が大きくなり、学習が不安定になるという問題がありました。
著者らは、この問題を解決するためにBVPO（Bias-Variance Optimized Preference Optimization）という手法を開発しました。
この手法は高分散のトレースベース勾配と低分散の空トレース勾配を最適に組み合わせることで、安定した学習と優れた性能を実現しています。
コードや実験データは論文内で明記されていませんが、DeepSeek R1モデルをベースとした実装について詳細に説明されています。

## 1. 研究概要
### 1.1 背景と動機
近年、DeepSeek R1、Gemini 2.5、GPT-o1などの大規模推論モデル（LRM）は、最終的な回答を生成する前に明示的な推論トレースを生成することで、複雑な多段階問題や数学的問題において大幅な性能向上を実現しています。
これらのモデルは、テスト時計算をスケールアップする「テスト時スケーリング」の概念を導入し、従来の言語モデルとは異なるアプローチを採用しています。

一方で、実世界でのデプロイメントの前提条件である人間の好みとのアライメントについては、LRMの特化した体系的な研究が不足しています。
既存のRLHF（Reinforcement Learning from Human Feedback）やDPO（Direct Preference Optimization）などの手法は、推論トレースを外部化しない従来の言語モデル向けに開発されており、LRMの適用する際に特有の問題が発生します。

特に深刻な問題は、推論トレースの確率的サンプリングで起因する「トレース誘起勾配分散」です。
LRMでは、統計的に正しい好み最適化の目的関数は、全ての可能な推論トレースを考慮した周辺確率を比較します。
しかし、これは指数的に大きな空間での計算となり実用的ではありません。
そのため、実際にはサンプリングされた単一のトレースを用いた近似が行われます。
ただし、長くて可変長のトレースは結合対数確率の大きな変動をもたらし、勾配推定の分散を著しく増大させます。

### 1.2 主要な貢献
本研究は、LRMのアライメントにおける根本的な課題を特定し、理論的に裏付けられた解決策を提案しています。

主要な貢献は次のような点があります。

- 問題の特定と定式化では、LRMにおける確率的推論トレースサンプリングによる高勾配分散の問題を初めて体系的に分析した。この分散がアライメント学習の安定性を阻害する主要なボトルネックである。
- BVPO手法の提案では、高分散のトレースベース勾配推定器と低分散の空トレース勾配推定器を線形結合する新しいアプローチを開発した。バイアス・分散トレードオフを明示的に最適化する手法である。
- 理論的保証の提供では、提案手法が条件付き分散を厳密に削減することを証明した。平均二乗誤差（MSE）を最小化する最適な混合係数の閉形式解を導出し、SGDの収束境界を改善することを理論的に示した。
- 包括的な実験検証では、3つの異なるサイズのLRMでAlpacaEval 2とArena-Hardベンチマークにおいて最大7.8ポイントの改善を達成した。さらに一般的な会話データのみで学習したにも関わらず、6つの数学推論ベンチマークで最大4.0ポイントの性能向上を実現した。

## 2. 提案手法
### 2.1 手法の概要
BVPO（Bias-Variance Optimized Preference Optimization）は、LRMの好み最適化における勾配分散問題を解決するために設計された新しいアプローチです。従来のトレースベースDPO損失が持つ高分散の問題を、低分散の空トレース損失と組み合わせることで緩和します。

手法の核心は、2つの異なる勾配推定器の凸結合を用いることです。
第一の推定器は従来のトレースベース勾配で、サンプリングされた推論トレースと最終回答の結合確率を用いて計算されます。
第2の推定器は新たに導入された空トレース勾配で、推論トレースを空に固定（r = ∅）して確率的サンプリングを回避します。
これら2つの推定器を混合係数αによって線形結合し、最適なα値をMSE最小化の観点から理論的に決定します。

### 2.2 技術的詳細
BVPOの数学的定式化は以下の通りです。LRMの確率分布を π_θ(r,y|x) = π_θ(r|x)π_θ(y|x,r) として因数分解し、理想的な周辺好み損失 L_m と実用的なトレースベース損失 L_t の関係を明確にします。

トレースベース損失 L_t は、サンプリングされたトレース・回答ペア (r±, y±) の結合確率を比較します。
L_t(θ) = E[(x,y±,r±)~D_t][ℓ_t(θ; x, y±, r±)]

一方、空トレース損失 L_e は、空のトレース r = ∅ に条件付けして確率的サンプリングを排除します。
L_e(θ) = E[(x,y'±)~D_e][ℓ_e(θ; x, y'±)]

BVPOの結合損失は以下のようになります：
L_c(θ) = α L_t(θ) + (1-α) L_e(θ)

対応する結合勾配推定器は次のようになります。
g_c = α g_t + (1-α) g_e

最適な混合係数α*は、真の周辺勾配に対するMSEを最小化するように決定され、理論的に導出された閉形式解を持ちます。

### 2.3 新規性
本手法の新規性は、LRMのアライメントにおける根本的な問題（トレース誘起勾配分散）を初めて特定したことにあります。バイアス・分散トレードオフの観点から体系的に解決しています。従来の好み最適化手法がこの問題を認識していなかったのに対し、BVPOは問題の数学的本質を明らかにし、理論的保証を持つ解決策を提供しています。

特に重要な革新は、空トレース推定器の導入です。この推定器は、推論過程を無効化することで確率的サンプリングによる分散を排除しながら、最終回答の好み比較という本質的な情報を保持します。
さらに、2つの推定器の最適結合を MSE最小化という統計的に最適な基準で決定する理論的フレームワークを構築し、これがSGDの収束性能とも直接的に関連することを証明しています。

## 3. 実験結果
### 3.1 実験設定
実験は三つの異なるサイズのLRM（DeepSeek-R1-Distill-Qwen-7B、DeepSeek-R1-Distill-Qwen-1.5B、DeepSeek-R1-0528-Qwen3-8B）で実施されました。これらのモデルは、Qwen 2.5およびQwen 3ベースモデルから思考連鎖推論データを用いてSFTで訓練されており、RLHFは未適用の状態です。

学習データには UltraFeedback データセットのプロンプトを使用し、各モデルに温度0.8で5つの応答を生成させました。生成された応答は ArmoRM モデルを用いてランク付けし、DeepSeek R1の慣例に従って最終回答部分のみを評価対象としました。最高ランクと最低ランクの応答をそれぞれ好みと非好み のサンプルとして選択し、DPO目的関数と組み合わせてBVPOを実装しました。

評価は二つのモードで行われました：標準的な推論モード（Thinking）と、プロンプトに「<think></think>」を付加して推論トレース生成を抑制するモード（NoThinking）です。前者は標準的な推論能力を、後者は即座の応答を好むユーザーのシナリオを反映しています。

### 3.2 主要な結果
アライメント性能の評価では、BVPOは全ての設定で一貫して最良のベースラインを上回る結果を示しました。AlpacaEval 2では、Thinkingモードで最大7.8ポイント、長さ制御勝率で最大5.1ポイントの改善を達成しました。Arena-Hardでは、Thinkingモードで最大5.1ポイント、NoThinkingモードで最大6.8ポイントの勝率向上を実現しています。

特に注目すべきは、モデルサイズにかかわらず改善が観測されたことです。1.5Bの小規模モデルから8Bの大規模モデルまで、全てのサイズでBVPOはDPOやSimPOといった既存手法を上回る性能を示しました。これは、提案手法がモデルサイズに依存しない本質的な改善をもたらすことを示唆しています。

数学推論能力の評価では、一般的な会話データのみで学習したにもかかわらず、6つの数学推論ベンチマーク（AIME 2024/2025、AMC、Minerva、OlympiadBench、MATH-500）で平均最大4.0ポイントの改善を達成しました。これは、好み最適化が推論能力を損なうのではなく、むしろ強化する可能性を示す重要な発見です。

### 3.3 既存手法との比較
BVPOと既存の好み最適化手法（DPO、SimPO）との比較では、全ての評価指標においてBVPOが優位性を示しました。DPOとの比較では、AlpacaEval 2のThinkingモードで平均3-5ポイント、Arena-Hardで平均2-4ポイントの改善が見られました。SimPOとの比較でも同様の傾向が観測され、BVPOの手法的優位性が確認されました。

重要な点は、これらの改善が統計的に有意であり、複数の独立したベンチマークで一貫して観測されたことです。また、思考モードと非思考モードの両方で改善が見られたことから、BVPOがLRMの多様な運用シナリオに対応できることが示されています。

ベースライン手法と比較して、BVPOは学習の安定性も改善していることが観測されました。これは、理論的に予測された分散削減効果が実際の学習過程で確認されたことを意味し、提案手法の理論的基盤の妥当性を裏付けています。

## 4. 実用性評価
### 4.1 実装の容易性
BVPOは既存のDPOパイプラインに容易に統合できる「ドロップイン」手法として設計されています。実装に必要な変更は最小限で、主に二つの損失関数の線形結合と最適混合係数の計算のみです。空トレース損失の計算は、単純にプロンプトに「<think></think>」を付加するだけで実現でき、特別なアーキテクチャ変更や複雑な前処理は不要です。

提案手法は、好み最適化アルゴリズムに依存しない設計となっており、DPO以外のアルゴリズム（SimPO、KTOなど）にも容易に適用可能です。また、混合係数αの決定も、提供された理論的フレームワークに基づいて自動化できるため、ハイパーパラメータチューニングの負担も最小限に抑えられます。

### 4.2 計算効率
計算効率の観点では、BVPOは既存手法と比較して大幅な計算オーバーヘッドを追加しません。空トレース推定器の計算は、推論トレース生成を無効化するため、むしろ計算量を削減する効果があります。全体的な計算コストは、従来のトレースベース学習と比較して約1.5倍程度の増加に留まります。

学習時間の観点では、二つの損失関数の並列計算が可能であり、効率的な実装により学習時間の大幅な増加を避けることができます。また、分散削減により学習の安定性が向上するため、収束に必要なエポック数が減少し、総合的な学習時間は従来手法と同等かそれ以下となる可能性があります。

### 4.3 応用可能性
BVPOの応用可能性は非常に広範囲に及びます。LRMが普及する現在の環境において、本手法は様々な領域での活用が期待されます。教育分野では、段階的説明を要求される場面と即座の回答が必要な場面の両方に対応できるため、適応的な学習支援システムの構築に活用できます。

研究分野では、複雑な科学的問題の解決において、詳細な推論過程と最終結果の両方の品質を同時に最適化することが可能になります。ビジネス応用では、意思決定支援システムにおいて、透明性の高い推論過程と効率的な結論導出を両立させることができます。

さらに、提案手法の理論的基盤は、LRM以外の構造化出力生成タスクにも拡張可能です。例えば、コード生成における中間ステップの最適化や、マルチステップの質問応答システムの改善などにも応用できる可能性があります。

## 5. まとめと所感
### 5.1 論文の意義
本論文は、LRMのアライメントという新しく重要な問題領域において、根本的な技術的課題を特定し、理論的に裏付けられた解決策を提供した画期的な研究です。特に、トレース誘起勾配分散という以前に認識されていなかった問題を数学的に定式化し、バイアス・分散トレードオフの観点から統一的に解決したことは、理論的にも実践的にも大きな貢献です。

提案されたBVPO手法は、単なる経験的改善ではなく、MSE最小化という統計的に最適な基準に基づいて設計されており、SGDの収束理論とも直接的に関連付けられています。この理論的厳密性は、手法の信頼性と一般化可能性を保証する重要な要素となっています。

実験結果においても、複数のモデルサイズとベンチマークで一貫した改善を示し、さらに推論能力の保持・向上も確認されており、実用的価値の高さが実証されています。これらの結果は、LRMの実用展開における重要なマイルストーンとなるでしょう。

### 5.2 今後の展望
本研究は、LRMアライメントの新しい研究方向を切り開いています。今後の発展方向として、混合係数の動的調整メカニズムの開発が考えられます。学習の進行に伴って最適な混合比が変化する可能性があり、適応的なα調整により更なる性能向上が期待できます。

また、空トレース以外の低分散推定器の探索も興味深い研究テーマです。例えば、短縮トレースや構造化トレースを用いた推定器により、バイアス・分散トレードオフのさらなる最適化が可能かもしれません。

さらに、提案手法の理論的フレームワークは、他の生成タスクや最適化問題にも拡張可能であり、より広範な機械学習問題への応用が期待されます。特に、マルチステップ生成やツール使用を伴うAIシステムにおいて、類似の分散問題とその解決策の研究が重要になるでしょう。